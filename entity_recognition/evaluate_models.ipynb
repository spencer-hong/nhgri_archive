{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "spacy.require_gpu()\n",
    "\n",
    "\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    # scorer returns recall, precision, and f-1 score for entity types\n",
    "    example_list = []\n",
    "    \n",
    "    for example in examples:\n",
    "        \n",
    "        input_text, input_annotations = example\n",
    "        pred = ner_model(input_text)\n",
    "\n",
    "        temp = Example.from_dict(pred,input_annotations)\n",
    "        \n",
    "        example_list.append(temp)\n",
    "\n",
    "    scores = scorer.score(example_list)\n",
    "    return scores\n",
    "\n",
    "def prepare_examples(line):\n",
    "    return line['text'], { \"entities\": [(i['start'], i['end'], i['label']) for i in line['spans']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we had 10 training runs for each N sample sets\n",
    "\n",
    "DATA_SAMPLES = [100, 200, 300, 400, 500]\n",
    "TRAIN_RUN = np.arange(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "to_plot = []\n",
    "for sample in tqdm(DATA_SAMPLES, total = len(DATA_SAMPLES)):\n",
    "    for run in tqdm(TRAIN_RUN, total = len(TRAIN_RUN)):\n",
    "        load = f\"data_curve/{sample}_samples/runs_{run}/model-best\"\n",
    "        hold_out = f\"data_curve/holdout_{run}.spacy\"\n",
    "        data = []\n",
    "        nlp = spacy.blank(\"en\")\n",
    "\n",
    "        # evaluate only on the holdout set\n",
    "        doc_bin = DocBin().from_disk(hold_out) \n",
    "\n",
    "        # get gold entities\n",
    "        for doc in doc_bin.get_docs(nlp.vocab):\n",
    "\n",
    "            spans = []\n",
    "\n",
    "            for ent in doc.ents:\n",
    "\n",
    "                spans.append({\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_})\n",
    "            data.append({\"text\": doc.text, \"spans\": spans})\n",
    "\n",
    "        # get predicted entities\n",
    "        nlp = spacy.load(load)\n",
    "\n",
    "        ner = spacy_evaluate(nlp, [prepare_examples(page) for page in data])['ents_per_type']\n",
    "\n",
    "        to_plot.append((ner, f\"{sample}-{run}\"))\n",
    "        \n",
    "\n",
    "        # if comparing against a pretrained model\n",
    "        \n",
    "        # nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "        # ner = spacy_evaluate(nlp, [prepare_examples(page) for page in data])['ents_per_type']\n",
    "\n",
    "        # to_plot.append((ner, f\"{0}-{run}\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for category in to_plot:\n",
    "    for entity in category[0]:\n",
    "        if entity in ['PERSON', 'LOC', 'IDNUM', 'EMAIL', 'ORG']:\n",
    "            temp_row = [(category[0][entity]['p'], \"precision\", entity, category[1]),\\\n",
    "                        (category[0][entity]['r'], \"recall\", entity, category[1]), \\\n",
    "                       (category[0][entity]['f'], \"f1\", entity, category[1])]\n",
    "            rows.extend(temp_row)\n",
    "df = pd.DataFrame(rows, columns = [\"value\", \"score_type\", \"entity\", \"category\"])\n",
    "\n",
    "df['sample'] = df['category'].str.split('-').str[0].astype(int)\n",
    "df['run'] = df['category'].str.split('-').str[-1].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 14,\n",
    "\"xtick.labelsize\": 14,\n",
    "\"ytick.labelsize\": 14,\n",
    "\"legend.fontsize\": 14,\n",
    "\"font.size\":14})\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2 * 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5 * 2\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2 * 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5 * 2\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2 * 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5 * 2\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2 * 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5 * 2\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5), dpi=300)\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "ax.xaxis.label.set_color('black')\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "sns.pointplot(data=df.loc[df.score_type == 'f1'], x=\"sample\", y=\"value\", hue = 'entity', errorbar = 'ci', scale = 1, \\\n",
    "             capsize=.15, errwidth=2)\n",
    "\n",
    "sns.despine()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(\"Samples finetuned\")\n",
    "ax.set_ylabel(\"F-1 score\")\n",
    "ax.legend(frameon = False)\n",
    "\n",
    "# plt.savefig(\"../figures/data_curve_pii_detection.pdf\", bbox_inches = \"tight\", dpi = 300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.category.str.contains(\"500\")) & (df.score_type == 'precision')].groupby('entity').agg({\"value\": 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.category.str.contains(\"500\")) & (df.score_type == 'precision')].groupby('entity').agg({\"value\": 'std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.category.str.contains(\"500\")) & (df.score_type == 'recall')].groupby('entity').agg({\"value\": 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.category.str.contains(\"500\")) & (df.score_type == 'recall')].groupby('entity').agg({\"value\": 'std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.category.str.contains(\"500\")) & (df.score_type == 'f1')].groupby('entity').agg({\"value\": 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.category.str.contains(\"500\")) & (df.score_type == 'f1')].groupby('entity').agg({\"value\": 'std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the mistakes are close to the gold truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "doc_bin = DocBin()\n",
    "\n",
    "nlp = spacy.load(\"data_curve/500_samples/runs_1/model-best/\")\n",
    "doc_bin = DocBin().from_disk(\"data_curve/holdout_1.spacy\")  # your file here\n",
    "# examples = []  # examples in Prodigy's format\n",
    "total = 0\n",
    "for doc in doc_bin.get_docs(nlp.vocab):\n",
    "    pred = nlp(doc.text_with_ws)\n",
    "  \n",
    "    gold = [ent.text for ent in doc.ents if ent.label_ == \"LOC\"]\n",
    "    \n",
    "    predict = [ent.text for ent in pred.ents if ent.label_ == \"LOC\"]\n",
    "    predict_org = [ent.text for ent in pred.ents if ent.label_ == \"ORG\"]\n",
    "    total += len(gold)\n",
    "    if gold != predict:\n",
    "#         print('text', doc.text)\n",
    "        print(\"real\", gold)\n",
    "        print(\"predicted\", predict)\n",
    "        print(\"org\", predict_org)\n",
    "#         print('---------')\n",
    "        #     print(\"predicted_org\", predict_org)\n",
    "        print(\"=========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
