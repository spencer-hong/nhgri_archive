{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1293f2-155f-4865-a6a1-ac4da3225e0d",
   "metadata": {},
   "source": [
    "### this assumes that you already have annotated your archive with entities using LabelStudio\n",
    "### see this section's README for more information\n",
    "### this notebook creates 10 training-validation sets of each sample size (100 to 500, spaced by 100) each with holdout sets for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "# annotated archive from LabelStudio\n",
    "# annotated for person name, mailing address, email address, \n",
    "# ID_num (identifiers that look like credit card numbers or social security numbers), \n",
    "# organization name\n",
    "\n",
    "with open(\"archive.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        corpus.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66641d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8652b-6c33-4530-8de7-edb9cc6821c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23a407-9646-4813-be27-cebdf6280ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert JSONL to spaCy doc format\n",
    "# taken from https://github.com/explosion/spaCy/discussions/10202\n",
    "def jsonl2doc(jsonl_dict, nlp) -> Doc:\n",
    "    doc = nlp(jsonl_dict[\"text\"])\n",
    "    ents = []\n",
    "    for ann in jsonl_dict.get(\"spans\", []):\n",
    "        span = Span(\n",
    "            doc,\n",
    "            ann[\"token_start\"],\n",
    "            ann[\"token_end\"] + 1,\n",
    "            ann[\"label\"],\n",
    "        )\n",
    "        ents.append(span)\n",
    "    doc.set_ents(ents)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets with holdout evaluation set from training size of 100 to 500\n",
    "nlp = spacy.blank(\"en\")\n",
    "n = 100 # size of the evaluation set\n",
    "\n",
    "for k in tqdm(range(0, 10)):\n",
    "    random.shuffle(original)\n",
    "    holdout = original[:n]\n",
    "\n",
    "    train = original[n:]\n",
    "\n",
    "    for i in tqdm(np.arange(100, 550, 100)):\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        to_hold = []\n",
    "        doc_bin = DocBin()\n",
    "    \n",
    "        random_sample = random.sample(train, i)\n",
    "        for j in random_sample :\n",
    "            doc_bin.add(jsonl2doc(j, nlp))\n",
    "        if not Path(f\"data_curve/{i}_samples/\").exists():\n",
    "            Path(f\"data_curve/{i}_samples/\").mkdir(parents=True)\n",
    "        assert (len(doc_bin) == i)\n",
    "        doc_bin.to_disk(f\"data_curve/{i}_samples/train_{k}.spacy\")\n",
    "        \n",
    "    doc_bin = DocBin()\n",
    "    for i in holdout:\n",
    "        doc_bin.add(jsonl2doc(i, nlp))\n",
    "    assert (len(doc_bin) == 100)\n",
    "    doc_bin.to_disk(f\"data_curve/holdout_{k}.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([i['text'] for i in corpus]) - set([i['text'] for i in holdout]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef993a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
