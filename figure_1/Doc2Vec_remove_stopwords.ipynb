{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab772659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../gerlach_et_al_src/')\n",
    "\n",
    "from filter_words import run_stopword_statistics\n",
    "from filter_words import make_stopwords_filter\n",
    "from filter_words import remove_stopwords_from_list_texts\n",
    "\n",
    "from real_corpora import tranfer_real_corpus_toID_and_shuffle\n",
    "from ldavb import ldavb_inference_terminal, obtain_ldavb_cpuTime_memory\n",
    "from evaluation import obtain_nmi_unsup, state_dwz_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from utils_tiramisu import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# this is the same TIRAMISU_PATH as shown in start_here.ipynb\n",
    "TIRAMISU_PATH = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef5ed7-29c3-44bf-9848-63feab691448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbc31d4",
   "metadata": {},
   "source": [
    "Following the code from _Gerlach et al._ (2019), we filter stopwods at the document level as Doc2Vec models are trained on documents as the text records in a corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0150436-3b05-49e4-8612-efa5297fd7bf",
   "metadata": {},
   "source": [
    "`../cache/pdfs_word_excel_powerpoint_010924.parquet` is simply a Pandas DataFrame that contains the combined texts of the scanned/electronic PDFs and MS documents. The columns are `text`, which is the raw text, and `nodeID` which is the nodeIDs of the split single-page PDFs or the MS documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdfs = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:CONVERT_TO] -> (f:File) \n",
    "where e.fileExtension = 'pdf' and f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "all_ms = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['doc', 'docx', 'ppt', 'pptx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "corpus = pd.read_parquet('../cache/pdfs_word_excel_powerpoint_010924.parquet')\n",
    "corpus['processed'] = corpus['text'].apply(lambda x: re.sub(r'\\W+', ' ', x.strip().lower()) )\n",
    "all_excel = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['xls', 'xlsx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "corpus = corpus.loc[~corpus.nodeID.isin(all_excel['nodeID'].to_list())]\n",
    "\n",
    "corpus['processed'] = corpus['text'].apply(lambda x: re.sub(r'\\W+', ' ', x.strip().lower()) )\n",
    "\n",
    "corpus = corpus[['processed', 'nodeID']]\n",
    "\n",
    "folder_structure = pd.concat([all_pdfs, all_ms])\n",
    "\n",
    "map_nodeID_to_docID = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:PART_OF] -> (d:Document) \n",
    "where e.fileExtension = 'pdf' \n",
    "return c.nodeID as nodeID, d.nodeID as documentID \n",
    "\"\"\").set_index('nodeID').to_dict()['documentID']\n",
    "\n",
    "\n",
    "merged = pd.merge(corpus, folder_structure, on = 'nodeID')\n",
    "merged['documentID'] = merged['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else None)\n",
    "\n",
    "pdfs_with_paths = merged.loc[merged.documentID.notna()]\n",
    "\n",
    "together = pd.concat([merged.loc[merged.fileExtension != 'pdf'], pdfs_with_paths])\n",
    "together['documentID'] = together.apply(lambda x: x['nodeID'] if x['documentID'] is None else x['documentID'], axis = 1)\n",
    "\n",
    "together = together[['processed', 'fileExtension', 'path', 'documentID']]\n",
    "\n",
    "together['filePath'] = together['path'] + '---' + together['documentID']\n",
    "\n",
    "to_put_into_gensim = together[['processed', 'filePath']].set_index('filePath').groupby('filePath').apply(lambda x : x.to_numpy().tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5104cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts = []\n",
    "list_nodeIDs = []\n",
    "\n",
    "for document in tqdm(to_put_into_gensim, total = len(to_put_into_gensim)):\n",
    "    temp_corpus = []\n",
    "    for i in to_put_into_gensim[document]:\n",
    "        \n",
    "        for token in i[0].split():\n",
    "            if token == '':\n",
    "                continue\n",
    "            temp_corpus.append(token.strip())\n",
    "        \n",
    "    if len(temp_corpus) == 0:\n",
    "        continue\n",
    "    list_nodeIDs.append(document)\n",
    "    list_texts.append(temp_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16788a5f-314a-4887-8e44-a0983583a52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a5199-ed61-4f28-86dc-0452a3d98482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is provided by Gerlach et al\n",
    "path_stopword_list =  'stopwords_filtering/data/stopword_list_en'\n",
    "\n",
    "## number of realizations for the random null model\n",
    "N_s = 10\n",
    "\n",
    "## get the statistics\n",
    "df = run_stopword_statistics(list_texts,N_s=N_s,path_stopword_list=path_stopword_list)\n",
    "\n",
    "## look at the entries\n",
    "df.sort_values(by='F',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fbafb-8105-446b-8556-217238c13ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fraction in tqdm([0, 0.01, 0.05, 0.1, 0.2, 0.5]):\n",
    "    cutoff_type = 'p'\n",
    "    cutoff_val = fraction\n",
    "    method = 'INFOR'\n",
    "    df_filter = make_stopwords_filter(df,\n",
    "                                  method = method,\n",
    "                                  cutoff_type = cutoff_type, \n",
    "                                  cutoff_val = fraction, )\n",
    "    list_words_filter = list(df_filter.index)\n",
    "    list_texts_filter = remove_stopwords_from_list_texts(list_texts, list_words_filter)\n",
    "    N = sum([ len(doc) for doc in list_texts ])\n",
    "    N_filter = sum([ len(doc) for doc in list_texts_filter ])\n",
    "    print('Remaining fraction of tokens',N_filter/N, fraction)\n",
    "    to_df = [' '.join(i) for i in list_texts_filter]\n",
    "    \n",
    "    to_df = pd.DataFrame(to_df)\n",
    "    \n",
    "    to_df['nodeIDs'] = list_nodeIDs\n",
    "    to_df.columns = ['text', 'nodeID']\n",
    "\n",
    "    to_df.to_parquet(f'../models/stopwords_filtering/filtered_text_240320_{fraction}_removed_22070.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735745d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
