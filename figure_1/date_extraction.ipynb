{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7404c41-0fdd-4f53-abf4-16808e24522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from utils_tiramisu import *\n",
    "\n",
    "import re\n",
    "from dateutil import parser\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2c51e-e8c2-45b1-a875-b6fd4705c8c3",
   "metadata": {},
   "source": [
    "We first extract the dates of MS Office documents. Due to potential corruption during template and saving times, we take the median date in the MS Office metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ed22b-7dcc-4c2a-b74d-7178c9407b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(path):\n",
    "    all_dates = []\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'dcterms:created' in data['metadata']:\n",
    "        if isinstance(data['metadata']['dcterms:created'], list):\n",
    "            try:\n",
    "                all_dates.extend([parser.parse(i).date() for i in data['metadata']['dcterms:created']])\n",
    "            except:\n",
    "                print(i)\n",
    "                print(data['metadata']['dcterms:created'])\n",
    "        else:\n",
    "            all_dates.append(parser.parse(data['metadata']['dcterms:created']).date())\n",
    "    if 'custom:_DCDateCreated' in data['metadata']:\n",
    "        if isinstance(data['metadata']['custom:_DCDateCreated'], list):\n",
    "            all_dates.extend([parser.parse(i).date() for i in data['metadata']['custom:_DCDateCreated']])\n",
    "        else:\n",
    "            all_dates.append(parser.parse(data['metadata']['custom:_DCDateCreated']).date())\n",
    "    \n",
    "    if \"dcterms:modified\" in data['metadata']:\n",
    "        if isinstance(data['metadata']['dcterms:modified'], list):\n",
    "            all_dates.extend([parser.parse(i).date() for i in data['metadata']['dcterms:modified']])\n",
    "        else:\n",
    "            all_dates.append(parser.parse(data['metadata']['dcterms:modified']).date())\n",
    "    return sorted(all_dates)[len(all_dates)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f8343-74d2-48f6-bc11-433a979a1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date extractable documents from corpus\n",
    "\n",
    "all_ms = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['doc', 'docx', 'ppt', 'pptx', 'xlsx', 'xls'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as file_extension\n",
    "\"\"\")\n",
    "\n",
    "all_pdfs = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:CONVERT_TO] -> (f:File) \n",
    "where e.fileExtension = 'pdf' and f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, e.originalPath as path, e.fileExtension as file_extension\n",
    "\"\"\")\n",
    "\n",
    "folder_structure = pd.concat([all_pdfs, all_ms])\n",
    "\n",
    "all_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4d681",
   "metadata": {},
   "source": [
    "The following folders are created by doing the text extraction from the [start_here.ipynb](../start_here.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c44239-389a-4c9c-a5de-c1c2f69e6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = []\n",
    "date_types = []\n",
    "for i, row in all_ms.iterrows():\n",
    "    date = get_date(f\"../text_extraction/ms_tika/{row['nodeID']}.json\")\n",
    "\n",
    "    if date.year <= 2015:\n",
    "        all_dates.append(date)\n",
    "        date_types.append(row['file_extension'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a76ba3-0c33-48a3-9c1b-9f50d7d2337c",
   "metadata": {},
   "source": [
    "We ignored those from 2016 and onwards; some are checked to have corrupted dates in the MS files due to potential saving by staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcd636-3c9b-4c3c-851e-32cb03dbad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.DataFrame({\"date\": all_dates, \"type\": date_types})\n",
    "date_df = date_df.sort_values('date')\n",
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462dffaf-4c07-4f86-9aa5-d216f54fc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df['type'] = date_df['type'].map({\"ppt\": \"powerpoint\", \"doc\": \"word\", \"pptx\": \"powerpoint\", \"docx\": \"word\", \"xls\": \"excel\", \"xlsx\": \"excel\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61f3ae-fad7-42ab-ab53-b13608e95334",
   "metadata": {},
   "source": [
    "We now extract the dates from PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811cb576",
   "metadata": {},
   "source": [
    "`../cache/pdfs_word_excel_powerpoint_010924.parquet` is simply a Pandas DataFrame that contains the combined texts of the scanned/electronic PDFs and MS documents. The columns are `text`, which is the raw text, and `nodeID` which is the nodeIDs of the split single-page PDFs or the MS documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f322808-5165-4933-bfb0-acb1fda5dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load all the PDF text segregated by documents\n",
    "\n",
    "# this is the compilation of all of the extracted text\n",
    "together = pd.read_parquet(\n",
    "    \"../cache/pdfs_word_excel_powerpoint_010924.parquet\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "map_nodeID_to_page = map_nodeID_to_docID.set_index('nodeID').to_dict()['page']\n",
    "# map_nodeID_to_path = map_nodeID_to_docID.set_index(\"nodeID\").to_dict()['path']\n",
    "map_nodeID_to_docID = map_nodeID_to_docID.set_index('nodeID').to_dict()['documentID']\n",
    "\n",
    "together['docID'] = together['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else x)\n",
    "together['page'] = together['nodeID'].apply(lambda x: map_nodeID_to_page[x] if x in map_nodeID_to_page else 0)\n",
    "together = pd.merge(together, folder_structure, left_on = 'nodeID', right_on = 'nodeID')\n",
    "\n",
    "together['text'] = together['text'].apply(lambda x: x + \" \")\n",
    "\n",
    "together = together.sort_values(['docID', 'page']).groupby('docID').agg({\"text\": \"sum\", \"path\": set}).reset_index()\n",
    "\n",
    "together['path'] = together['path'].apply(lambda x: list(x)[0])\n",
    "together['text'] = together['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0687379-4acd-4a1f-8544-45d61d3b9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm/dd/yyyy\n",
    "# mm/dd/yy\n",
    "# month day, year\n",
    "# month abreviation day, year \n",
    "# mm.dd.yyyy\n",
    "## handles st/nd/rd\n",
    "\n",
    "patternone = re.compile(\"(((1[0-2]|0?[1-9])(\\/|-|\\.)(3[01]|[12][0-9]|0?[1-9])(\\/|-|\\.)(?:[0-9]{2})?[0-9]{2})|((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+\\d{1,2}(st|nd|rd)?,?\\s+\\d{4}))\",re.IGNORECASE )\n",
    "\n",
    "\n",
    "\n",
    "# yyyy/mm/dd\n",
    "patterntwo = re.compile(\"((19[7-9][0-9]|20[0-9]{2})/((0?[13578]|1[02])/(0?[1-9]|[12][0-9]|3[01])|(0?[469]|11)/(0?[1-9]|[12][0-9]|30)|0?2/(0?[1-9]|1[0-9]|2[0-8]))|(19([79][26]|8[048])|20([02468][048]|[13579][26]))/0?2/29)\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "\n",
    "## day month, year\n",
    "\n",
    "patternthree = re.compile(\"((\\d{1,2})(st|nd|rd)?\\s+((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?),?(\\s+\\d{4})))\", re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c58dad-7d70-4f6b-abda-f7fed7d27335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_pdf(text):\n",
    "    returned = []\n",
    "    \n",
    "    \n",
    "    for match in patternone.findall(text):\n",
    "        date = dateparser.parse(match[0])\n",
    "\n",
    "        # while we don't anticipate dates beyond 2016, keep the year truncated at 2020\n",
    "        # so that we can review how well our date extraction falls within reasonable time period\n",
    "        if date is not None:\n",
    "            if date.year < 1980 or date.year > 2020:\n",
    "                pass\n",
    "            else:\n",
    "                returned.append(date)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    for match in patterntwo.findall(text):\n",
    "        date = dateparser.parse(match[0])\n",
    "        \n",
    "        if date is not None:\n",
    "            if date.year < 1980 or date.year > 2020:\n",
    "                pass\n",
    "            else:\n",
    "                returned.append(date)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    for match in patternthree.findall(text):\n",
    "        \n",
    "        date = dateparser.parse(match[0])\n",
    "        \n",
    "        if date is not None:\n",
    "            if date.year < 1980 or date.year > 2020:\n",
    "                pass\n",
    "            else:\n",
    "                returned.append(date)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645a41a-e6b0-4c4d-babb-0a1510e0c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "together['dates'] = together['text'].progress_apply(get_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5cc18-64f8-46d3-8268-107782e6fcef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ddf33d1-b672-4db9-8475-a9c6db926e58",
   "metadata": {},
   "source": [
    "We get the dates from PDF by the following rules\n",
    "- only take dates in the first two pages (avoids references, and most of the time, the date of the document should be in the first two pages)\n",
    "- only consider documents whose dates fall within one month of one another. since our granularity if month/year, we only care about the month level detail.\n",
    "- get the median of valid dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e820e-aea8-4857-847a-b69319b1e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_one_month(listofdates):\n",
    "    min_date=  min(listofdates)\n",
    "    max_date= max(listofdates)\n",
    "\n",
    "    if relativedelta(pd.Timestamp(max_date), pd.Timestamp(min_date)).years == 0 and \n",
    "    relativedelta(pd.Timestamp(max_date), pd.Timestamp(min_date)).months <= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_median_date(date_list):\n",
    "    return sorted(date_list)[len(date_list)//2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ceeba-b54f-49d8-a823-cd4c0653b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(together, folder_structure, on = 'nodeID')\n",
    "\n",
    "pdfs_with_paths = merged.loc[merged.documentID.notna()]\n",
    "\n",
    "together = pd.concat([merged.loc[merged.fileExtension != 'pdf'], pdfs_with_paths])\n",
    "together['documentID'] = together.apply(lambda x: x['nodeID'] if x['documentID'] is None else x['documentID'], axis = 1)\n",
    "\n",
    "together[\"text\"] = together['text'].apply(lambda x: x + \" \")\n",
    "together = together.sort_values(['documentID', 'page'])\n",
    "\n",
    "\n",
    "# take the dates in the first two pages\n",
    "per_document = together.loc[together.fileExtension == 'pdf'].groupby('documentID').head(2)\n",
    "\n",
    "per_document = per_document.reset_index().groupby('documentID').agg({\"fileExtension\": \"first\", \"text\": sum,  \"dates\" : list})\n",
    "\n",
    "per_document['dates'] = per_document['dates'].apply(lambda x: list(chain(*x)))\n",
    "\n",
    "valid_pdf = per_document.loc[(per_document.fileExtension == \"pdf\") & (per_document.dates.str.len() > 0)]\n",
    "\n",
    "valid_pdf['filtered_month'] = valid_pdf['dates'].apply(within_one_month)\n",
    "valid_pdf['median_date'] = valid_pdf['dates'].apply(get_median_date)\n",
    "\n",
    "valid_pdf_by_month = valid_pdf.loc[valid_pdf.filtered_month].reset_index()\n",
    "\n",
    "valid_ms = pd.merge(date_df, nhgri_with_dates, on = 'nodeID', how = 'left')[['date', 'type', 'nodeID', 'text']]\n",
    "valid_ms['date'] = valid_ms['date'].apply(lambda x: pd.Timestamp(x))\n",
    "\n",
    "valid_pdf_by_month['date'] = valid_pdf_by_month['median_date'].apply(lambda x: \\\n",
    "                                        datetime.datetime(pd.Timestamp(x).year, pd.Timestamp(x).month, 1))\n",
    "\n",
    "valid_pdf_by_month.columns = ['nodeID', 'type', 'text', 'dates', 'filtered_month', 'median_date', 'date']\n",
    "\n",
    "all_documents_with_dates = pd.concat([valid_pdf_by_month[['type', 'text', 'date', 'nodeID']], valid_ms[['date', 'type', 'text', 'nodeID']]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372b50-d187-4df6-a03e-1e94d980246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of documents with a valid date\n",
    "\n",
    "all_documents_with_dates.shape[0] / 22843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af1c57-edd0-4b08-9ba8-914efa282cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad8ef4-ae67-4bac-81ef-96f468985c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator, PercentFormatter\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "\n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize= SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=SMALL_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize = (4, 4), dpi = 300, gridspec_kw={'height_ratios': [2, 1]} )\n",
    "\n",
    "ax[0].grid(True, which = 'major', axis = 'x')\n",
    "ax[0].set_yticks([])\n",
    "ax[0].spines['right'].set_visible(False)\n",
    "ax[0].spines['top'].set_visible(False)\n",
    "ax[0].spines['bottom'].set_color('black')\n",
    "ax[0].spines['left'].set_color('black')\n",
    "ax[0].xaxis.label.set_color('black')\n",
    "ax[0].tick_params(axis='x', colors='black')\n",
    "ax[0].yaxis.label.set_color('black')\n",
    "ax[0].tick_params(axis='y', colors='black')\n",
    "ax[0].spines['bottom'].set_linewidth(0.5)\n",
    "ax[0].spines['left'].set_linewidth(0.5)\n",
    "\n",
    "ax[0].set_xlim([datetime.date(1988, 1, 1), datetime.date(2020, 1, 1)])\n",
    "ax[0].yaxis.set_tick_params(labelleft=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[1].spines['right'].set_visible(False)\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "ax[1].spines['bottom'].set_color('black')\n",
    "ax[1].spines['left'].set_color('black')\n",
    "ax[1].xaxis.label.set_color('black')\n",
    "ax[1].tick_params(axis='x', colors='black')\n",
    "ax[1].yaxis.label.set_color('black')\n",
    "ax[1].tick_params(axis='y', colors='black')\n",
    "ax[1].spines['bottom'].set_linewidth(0.5)\n",
    "ax[1].spines['left'].set_linewidth(0.5)\n",
    "\n",
    "bins = pd.date_range(start='1988-1-1',\n",
    "                  end='2020-1-1',\n",
    "                  periods=35)\n",
    "ax[1].set_xlim([datetime.date(1988, 1, 1), datetime.date(2020, 1, 1)])\n",
    "ax[1].set_ylabel(\"Documents\")\n",
    "ax[1].yaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "\n",
    "ax[1].locator_params(axis='y', nbins=5)\n",
    "\n",
    "pdf_hist = np.array(all_documents_with_dates.loc[all_documents_with_dates.type == 'pdf']['date'].to_list()).flatten()\n",
    "word_hist = np.array(all_documents_with_dates.loc[all_documents_with_dates.type == 'word']['date'].to_list()).flatten()\n",
    "ppt_hist = np.array(all_documents_with_dates.loc[all_documents_with_dates.type == 'powerpoint']['date'].to_list()).flatten()\n",
    "excel_hist = np.array(all_documents_with_dates.loc[all_documents_with_dates.type == 'excel']['date'].to_list()).flatten()\n",
    "ax[1].hist([pdf_hist,word_hist,ppt_hist, excel_hist], bins, stacked=True,  color = [cmap_key('pdf'), \\\n",
    "                cmap_key('word'), cmap_key('powerpoint'), cmap_key('excel')], linewidth = 0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.savefig('../cache/pdf-powerpoint-word-time-period-with-parta_240313.pdf', transparent = True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753ff03-14b1-484d-b341-b0fc5afc3420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c047c38-cea9-4a79-bd62-c6fa0063acfc",
   "metadata": {},
   "source": [
    "As a temporal control, we track easy terms that have clear beginning and ending timelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34fe1f8-13eb-4605-a551-c03767f0fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = {\"nhgri\": [\n",
    "    \"nhgri\", \n",
    "    \"national human genome research institute\"\n",
    "],\n",
    "         \"nchgr\": [\n",
    "             \"nchgr\",\n",
    "             \"national center for human genome research\"\n",
    "         ],\n",
    "         \"ohgr\": [\n",
    "             \"ohgr\",\n",
    "             \"office of human genome research\"\n",
    "         ],\n",
    "         # current director of NHGRI\n",
    "         \"green\": [\n",
    "             \"eric green\",\n",
    "             \"e green\",\n",
    "             \"e. green\",\n",
    "             \"green, eric\",\n",
    "             \"green, e\",\n",
    "         ],\n",
    "         # former director of NIH\n",
    "         \"varmus\": [\n",
    "             \"varmus\",\n",
    "             \"harold varmus\",\n",
    "             \"h. varmus\",\n",
    "             \"h varmus\",\n",
    "             \"varmus, harold\",\n",
    "             \"varmus, h\",\n",
    "             \"varmus, h.\"\n",
    "         ],\n",
    "         # former director of NHGRI, HGP\n",
    "         \"collins\": [\n",
    "             \"francis collins\",\n",
    "             \"f. collins\",\n",
    "             \"f collins\",\n",
    "             \"collins, francis\",\n",
    "             \"collins, f\",\n",
    "             \"collins, f.\",\n",
    "             \"fc\",\n",
    "         ],\n",
    "         # former director of NIH\n",
    "         \"watson\": [\n",
    "             \"watson\",\n",
    "             \"j. watson\",\n",
    "             \"j watson\",\n",
    "             \"james watson\",\n",
    "             \"watson, james\",\n",
    "             \"watson, j\",\n",
    "             \"watson, j.\"\n",
    "         ],\n",
    "         # private effort to sequence a human genome\n",
    "         \"celera\": [\n",
    "             \"celera\"\n",
    "         ],\n",
    "         # the five main sequencing centers\n",
    "         \"g5\": [\n",
    "             \"g5\",\n",
    "              \"baylor college of medicine\",\n",
    "             \"bcm\",\n",
    "             \"broad institute\",\n",
    "             \"broad/mit\",\n",
    "              \"whitehead\",\n",
    "             \"whitehead/mit\",\n",
    "              \"joint genome institute\",\n",
    "             \"jgi\",\n",
    "               \"washu\",\n",
    "             'wustl',\n",
    "             \"washington university in st. louis\",\n",
    "             \"washington university in st louis\",\n",
    "             \"washington university at st louis\",\n",
    "              \"washington university at st. louis\",\n",
    "              \"sanger institute\",\n",
    "             \"wellcome sanger\"\n",
    "         ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70dd637-204f-4288-8b38-6bb921bca8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents_with_dates = all_documents_with_dates.loc[all_documents_with_dates.text.notna()]\n",
    "all_documents_with_dates = all_documents_with_dates.loc[(all_documents_with_dates.date.dt.year >= 1988) & \\\n",
    "                                                       (all_documents_with_dates.date.dt.year <= 2012)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8b596-5893-4212-b2cb-4af694d8f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_entities = []\n",
    "\n",
    "\n",
    "for i, row in tqdm(all_documents_with_dates.iterrows(), total = all_documents_with_dates.shape[0]):\n",
    "    temp = []\n",
    "    for group, term in enumerate(terms):\n",
    "        \n",
    "        matches = re.findall(r\"\\b(\" + r\"|\".join([alias for alias in terms[term]]) + r\")\\b\", row['text'].lower())\n",
    "        if len(matches) > 0:\n",
    "            list_of_entities.append((row['date'], True, term))\n",
    "        else:\n",
    "            list_of_entities.append((row['date'], False, term))\n",
    "entities_df = pd.DataFrame(list_of_entities, columns = [\"date\", \"entity\", \"text\"])\n",
    "entities_df['year'] = entities_df.date.dt.year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facdd9c-ba3c-4b93-8a83-f00b2faceb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize = (60 * (1/2.54/10), 120 * (1/2.54/10)), dpi = 300 )\n",
    "\n",
    "\n",
    "\n",
    "linewidth = 0.5\n",
    "num = entities_df.loc[(entities_df.text == 'ohgr')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[0], label = \"OHGR\" , color = \"#FF8811\",\\\n",
    "            estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth )\n",
    "\n",
    "ax[0].axvline(x = 1989, ymin =0, ymax = 1, color = 'black', alpha = 0.6, linestyle = \"dashed\", linewidth = linewidth )\n",
    "ax[0].axvline(x = 1997, ymin =0, ymax = 1, color = 'black', alpha = 0.6, linestyle = \"dashed\", linewidth = linewidth)\n",
    "\n",
    "num = entities_df.loc[(entities_df.text == 'nchgr')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[0], label = \"NCHGR\" , color = \"#C65B7C\", \\\n",
    "             estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "\n",
    "\n",
    "num = entities_df.loc[(entities_df.text == 'nhgri')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[0], label = \"NHGRI\", color = \"#4B2142\", \\\n",
    "             estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num = entities_df.loc[(entities_df.text == 'watson')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[1], label = \"Watson\",  color = \"#FF8811\",\\\n",
    "              estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "\n",
    "num = entities_df.loc[(entities_df.text == 'collins')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[1], label = \"Collins\" , color =  \"#C65B7C\",\\\n",
    "              estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "num = entities_df.loc[(entities_df.text == 'green')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[1], label = \"Green\" , color =  \"#4B2142\",\\\n",
    "              estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "\n",
    "ax[1].axvline(x = 1993.33, ymin =0, ymax = 1, color = 'black', alpha = 0.6, linestyle = \"dashed\", linewidth = linewidth)\n",
    "ax[1].axvline(x = 2008.67, ymin =0, ymax = 1, color = 'black', alpha = 0.6, linestyle = \"dashed\", linewidth = linewidth)\n",
    "\n",
    "\n",
    "\n",
    "ax[2].axvline(x = 2001.17, ymin =0, ymax = 1, color = 'black', alpha = 0.6, linestyle = \"dashed\", linewidth = linewidth)\n",
    "num = entities_df.loc[(entities_df.text == 'celera')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[2], label = \"Celera\", color =  \"#FF8811\" ,\\\n",
    "              estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "num = entities_df.loc[(entities_df.text == 'g5') | (entities_df.text == 'broad') |\\\n",
    "                      (entities_df.text == 'whitehead') | (entities_df.text == 'washu') | \\\n",
    "                     (entities_df.text == 'jgi') | (entities_df.text == 'sanger')]\n",
    "sns.lineplot(data = num, x = 'year', y = 'entity', ax = ax[2], label = \"G5\", color =  \"#C65B7C\", \\\n",
    "              estimator=lambda x: sum(x==1)*100.0/len(x), linewidth = linewidth)\n",
    "\n",
    "ax[0].spines['right'].set_linewidth(0)\n",
    "ax[0].spines['top'].set_linewidth(0)\n",
    "ax[1].spines['right'].set_linewidth(0)\n",
    "ax[1].spines['top'].set_linewidth(0)\n",
    "ax[2].spines['right'].set_linewidth(0)\n",
    "ax[2].spines['top'].set_linewidth(0)\n",
    "\n",
    "\n",
    "\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].set_ylim([0, 100])\n",
    "ax[0].set_xlim([1988, 2012])\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_ylim([0, 100])\n",
    "ax[1].set_xlim([1988, 2012])\n",
    "ax[2].set_ylabel(\"\")\n",
    "ax[2].set_ylim([0, 100])\n",
    "ax[2].set_xlim([1988, 2012])\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[2].set_xlabel(\"\")\n",
    "\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "ax[2].get_legend().remove()\n",
    "ax[0].minorticks_on()\n",
    "ax[0].yaxis.set_tick_params(which='minor', bottom=True)\n",
    "# ax[0].xaxis.tick_top()\n",
    "ax[0].xaxis.set_ticks([1990, 2000, 2010])\n",
    "\n",
    "ax[1].minorticks_on()\n",
    "ax[1].yaxis.set_tick_params(which='minor', bottom=True)\n",
    "# ax[1].xaxis.tick_top()\n",
    "ax[1].xaxis.set_ticks([1990, 2000, 2010])\n",
    "\n",
    "ax[2].minorticks_on()\n",
    "ax[2].yaxis.set_tick_params(which='minor', bottom=True)\n",
    "# ax[2].xaxis.tick_top()\n",
    "ax[2].xaxis.set_ticks([1990, 2000, 2010])\n",
    "# plt.savefig('../figures/figure_2_temporal_controls_1.pdf', transparent=True, dpi = 300,  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9cd98-e6eb-43b7-b482-8645af40fc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba18178-8cbf-4198-93cf-8cf9d83a717b",
   "metadata": {},
   "source": [
    "This is the boxenplot of dates corresponding to each project in SI Figure 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7c695-eeee-477b-953a-1751cd4c27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def projects(x):\n",
    "\n",
    "    if x['secondary'] == 'human sequence':\n",
    "        return \"Human Genome Project\"\n",
    "    elif x['secondary'] == \"Box026-010.pdf\" and x['folders'] == \"Large scale sequence\":\n",
    "        return \"LSAC\"\n",
    "    elif x['folders'] == 'sequencingrampupfiles':\n",
    "        return \"Human Genome Project\"\n",
    "    elif x['folders'] == \"eMERGE\":\n",
    "        return \"eMERGE\"\n",
    "    elif x['folders'] == \"PAGE\":\n",
    "        return \"PAGE\"\n",
    "    elif x['folders'] == \"ENCODE\":\n",
    "        return \"ENCODE\"\n",
    "    elif x['folders'] == 'modENCODE':\n",
    "        return 'modENCODE'\n",
    "    elif x['folders'] == 'ELSI':\n",
    "        return 'ELSI'\n",
    "    elif x['folders'] == 'Celera':\n",
    "        return \"Human Genome Project\"\n",
    "    elif x['folders'] == \"H3Africa\":\n",
    "        return \"H3Africa\"\n",
    "    elif x['folders'] == 'Sequence target files':\n",
    "        return \"LSAC\"\n",
    "    elif x['folders'] == \"Haplotype Map Project\":\n",
    "        return \"HapMap\"\n",
    "    elif x['folders'] == \"GWAS materials\":\n",
    "        return \"GWAS\"\n",
    "    else:\n",
    "        return x['folders']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290463b1-9e4e-4574-a09b-4f5b05be766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'ELSI': '#F04A3B',\n",
    " 'GWAS': '#9FB13A',\n",
    " 'HapMap': '#E1BE15',\n",
    " 'LSAC': '#51AF4D',\n",
    " 'ENCODE': '#095393',\n",
    " 'modENCODE': '#AC5D95',\n",
    " 'eMERGE': 'maroon',\n",
    " 'Human Genome Project': 'gray',\n",
    " 'H3Africa': '#06B4DB',\n",
    " 'PAGE': '#4A4EA1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f775f2b-0f8c-423e-806c-da360cfd39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = all_documents_with_dates.loc[all_documents_with_dates.type == \"pdf\"]\n",
    "merged = pd.concat([pdfs, all_documents_with_dates.loc[all_documents_with_dates.type != 'pdf']])\n",
    "merged = pd.merge(merged, folder_structure, on = 'nodeID')\n",
    "\n",
    "merged['all_folders'] = merged['path'].apply(lambda x: Path(x).parts)\n",
    "merged['folders'] = merged['all_folders'].apply(lambda x: x[2])\n",
    "merged['secondary'] = merged['path'].apply(lambda x: x.split('/')[3] if len(x.split('/')) > 2 else None)\n",
    "merged['tertiary'] = merged['path'].apply(lambda x: x.split('/')[4] if len(x.split('/')) > 4 else x.split('/')[3])\n",
    "merged['project'] = merged.apply(lambda x: projects(x), axis = 1)\n",
    "merged['color'] = merged['project'].map(colors)\n",
    "\n",
    "merged['year'] = merged.date.dt.year\n",
    "\n",
    "merged['order'] = merged['project'].map({\n",
    "    \"ELSI\": 0,\n",
    "    \"Human Genome Project\": 1,\n",
    "    \"HapMap\": 3,\n",
    "    \"LSAC\": 2, \n",
    "    \"ENCODE\": 4,\n",
    "    \"modENCODE\": 5,\n",
    "    \"eMERGE\": 6,\n",
    "    \"PAGE\": 7,\n",
    "    \"GWAS\":8,\n",
    "    \"H3Africa\":9})\n",
    "merged = merged.sort_values('order')\n",
    "\n",
    "merged = merged.loc[merged.color.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284db23-697b-4fc5-9b60-e9250e6b7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 5,\n",
    "\"font.size\":7})\n",
    "\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4 , 3), dpi = 300 )\n",
    "\n",
    "sns.boxenplot(merged.loc[merged.project.notna()], x = 'year', y ='project', palette = sns.color_palette(merged.loc[merged.project.notna()].color.unique()), linewidth = 0.5, \n",
    "                  flier_kws={'marker': 'o', 's' : 1}, dodge = False)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "ax.xaxis.label.set_color('black')\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a0c92-8eed-4831-b819-971ee36965a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065fb9c-396d-4715-9cd7-c6a54474624b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
