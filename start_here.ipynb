{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code associated with **A digital archive reveals how a funding agency cooperated with academics to support a nascent field of science.**\n",
    "\n",
    "Contact: spencerhong@u.northwestern.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline here starts roughly as follows:\n",
    "1. Put archival artifacts into Tiramisu\n",
    "2. Run PDF splitting and file conversion\n",
    "3. Page stream segmentation task\n",
    "4. Handwriting extraction\n",
    "5. Text extraction\n",
    "6. Entity recognition & disambiguation & redaction\n",
    "\n",
    "We show below how to achieve each step. If you would like a streamlined, automated pipeline, our next platform as part of the Born Physical, Studied Digitally NSF consortium may be of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can request the Core Collection of the NHGRI archive by following the data availability section of the manuscript. For this tutorial, we use an example dataset of business documents sourced from the Industry Documents Library under Fair Use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step One - Put archival artifacts into Tiramisu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the path that contains your archive/corpus that you wish to track and convert. In this example, we will do so by setting `TIRAMISU_ROOT` in [`core/.env`](tiramisu/core/.env) as `../../example_data`. Also set your favorite `NEO4J_PASSWORD` in the same file.\n",
    "\n",
    "If you are on a AArch64 architecture, please move [`digest-0.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl`](tiramisu/core/rust/target/options/digest-0.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl) inside [`core/rust/target/options`](tiramisu/core/rust/target/options) to [`core/rust/target/wheels`](tiramisu/core/rust/target/wheels). Check that only one wheel file exists there. \n",
    "\n",
    "Then build and start Tiramisu inside `tiramisu` folder with  \n",
    "\n",
    "```bash\n",
    "docker-compose -f core/docker-compose_aarch64.yaml up --build\n",
    "```\n",
    "\n",
    "If you are on a x86_64 architecture, please move [`digest-0.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`](tiramisu/core/rust/target/options/digest-0.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) to [`core/rust/target/wheels`](tiramisu/core/rust/target/wheels). Check that only one wheel file exists there. \n",
    "\n",
    "Then build and start Tiramisu inside `tiramisu` folder with  \n",
    "\n",
    "```bash\n",
    "docker-compose -f core/docker-compose_x86_64.yaml up --build\n",
    "```\n",
    "If you run into the LabelStudio permission issues in the logs, you can check out the [FAQs](tiramisu/README.md#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Neo4J service and the frontend will be the last to start up. Once you see\n",
    "\n",
    "<img src=\"imgs/neo4j_start.png\" width=\"800\" /> \n",
    "\n",
    "and \n",
    "\n",
    "<img src=\"imgs/frontend_start.png\" width=\"800\" />,   \n",
    "you're ready to head to http://localhost:8080! It should look like below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/tiramisu_actions.png\" width=\"800\" />   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can start the initial step by clicking the button \"digest\". Please only click once as multiple clicks will repeat the same action and crowd out your graph database. Once finished, you can check that digestion has finished by checking the task dashboard at http://localhost:8080/flower/dashboard or going to the graph database at http://localhost:7474 and running the following query:\n",
    "\n",
    "```cypher\n",
    "\n",
    "MATCH (n) RETURN n\n",
    "\n",
    "```\n",
    "\n",
    "It will look like below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neo4j_digest.png\" width=\"800\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These artifacts are now part of Tiramisu! You're ready to start converting and preprocessing for data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two - Run PDF splitting and file conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiramisu has already built-in functions that will convert MS Word documents to PDFs, split multi-page PDFs to single-page PDFs (this will later come in handy for page stream segmentation), and convert all PDFs to images (for future OCR & labeling). It will also determine what type of PDF (e.g. born-physical PDFs which are scanned, and digital-native PDFs) and add as metadata. \n",
    "\n",
    "To do all of the following, you can simply click the rest of the buttons (e.g. `CONVERT MS TO PDFS`, `SPLIT PDFS`, etc) buttons **in order** of shown in the Tiramisu Actions page. **Make sure each task is finished before clicking the next task.** When finished, your graph database will look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiramisu has already built-in functions that will convert MS Word documents to PDFs, split multi-page PDFs to single-page PDFs (this will later come in handy for page stream segmentation), and convert all PDFs to images (for future OCR & labeling). It will also determine what type of PDF (e.g. born-physical PDFs which are scanned, and digital-native PDFs) and add as metadata. \n",
    "\n",
    "To do all of the following, you can simply click the rest of the buttons (e.g. `) button in the Tiramisu Actions page. When finished, your graph database will look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/neo4j_processed.png\" width=\"800\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you visualize some of these documents? Let's again use Tiramisu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to look at an artifact with an unique nodeID of `0x3dee1ff7+++0x86a9fb43`, which is page 4 of the processed PDF `ppwl0228.pdf` in `/tiramisu/fossil_fuel` (try to trace back these steps using the graph database!).\n",
    "\n",
    "First, let's head to http://localhost:8085. This is our labeling interface using LabelStudio. You can create an account (which is stored locally, so no online data transfer) and then go to `Account & Settings` and copy API access token. \n",
    "\n",
    "We can input these parameters in the Tiramisu Actions page for `Visualize a specific PDF or image`. `api` field is the access token, `nodeID` is the specific nodeID you'd like to visualize, and `configuration` is either `image` to visualize an image, or `pdf` to visualize a pdf. When the button is clicked, it will create a labeling task inside LabelStudio, which will look like this:\n",
    "\n",
    "<img src=\"imgs/labelstudio.png\" width=\"800\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Three - Page stream segmentation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page stream segmentation is a computer vision task that attempts to separate combined PDFs into logically separate documents. This is often an artifact of high-throughput scanning systems and quite prevalent in born-physical archives.\n",
    "\n",
    "While we have trained our own page stream segmentation model using synthetic data (contact us for more info), the NHGRI Core Collection was split manually using this following procedure. Upon request, the separated document boundaries are available as secondary data from NHGRI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must prepare a corpus to be labeled. We can use Tiramisu's API endpoint to query the necessary PDFs to be labeled using LabelStudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib import request\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This query does not get PDFs converted from MS documents as they are one logical document per one file\n",
    "GET_PDFS_ORDERED = \"\"\"MATCH (e) - [:CONTAINS] -> (c) - [:SPLIT_INTO] -> (n) - [:CONVERT_TO] -> (d)\n",
    "WHERE n.fileExtension = \"pdf\"\n",
    "RETURN c.originalPath as originalPDF, c.nodeID as originalNodeID, n.nodeID as nodeID, n.page as page, d.tiramisuPath as image\n",
    "ORDER BY originalNodeID ASC, page ASC\"\"\"\n",
    "\n",
    "def return_from_neo4j(query):\n",
    "\t\"\"\"This function returns the objects from Tiramisu's graph database given a valid query.\"\"\"\n",
    "\t\n",
    "\tdigest_list = [\n",
    "\t{\n",
    "\n",
    "\t\t\"action\": \"query_neo4j\",\n",
    "\t\t 'kwargs': {'query': query}\n",
    "\t}]\n",
    "\t\n",
    "\tdata = json.dumps({ \n",
    "\t\t\t\"action_list\": digest_list \n",
    "\t\t}).encode()\n",
    "\t\n",
    "\treq = request.Request(\"http://localhost:8080/api/action/concurrent\", data)\n",
    "\treq.add_header(\"Content-Type\", \"application/json\")\n",
    "\tres = request.urlopen(req)\n",
    "\tout_data = res.read()\n",
    "\tresult = json.loads(out_data)\n",
    "\t\n",
    "\ttime.sleep(1)\n",
    "\t\n",
    "\tresponse = request.urlopen(\"http://localhost:8080/api/status/\" + result['task_id'][0])\n",
    "\n",
    "\tfinished = False\n",
    "\twhile not finished:\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\tfinished = check_status(\"http://localhost:8080/api/status/\" + result['task_id'][0])\n",
    "\tresponse = request.urlopen(\"http://localhost:8080/api/status/\" + result['task_id'][0])\n",
    "\tdata = json.loads(response.read())\n",
    "\t\n",
    "\treturn pd.DataFrame(data['task_result'])\n",
    "\n",
    "def submit_to_tiramisu(query):\n",
    "    digest_list = [\n",
    "    {\n",
    "        \"action\": \"write_neo4j\",\n",
    "         'kwargs': {'query': query}\n",
    "    }]\n",
    "    data = json.dumps({ \n",
    "            \"action_list\": digest_list \n",
    "        }).encode()\n",
    "    req = request.Request(\"http://localhost:8080/api/action/concurrent\", data)\n",
    "    req.add_header(\"Content-Type\", \"application/json\")\n",
    "    res = request.urlopen(req)\n",
    "    out_data = res.read()\n",
    "    result = json.loads(out_data)\n",
    "\n",
    "def check_status(url):\n",
    "\t\"\"\"This function checks the status of the task.\"\"\"\n",
    "\tr = requests.get(url)\n",
    "\tstatus = r.json()['task_status']\n",
    "\n",
    "\treturn status == 'SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>nodeID</th>\n",
       "      <th>originalNodeID</th>\n",
       "      <th>originalPDF</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/tiramisu/.tiramisu/___tiramisu_versions/0x188...</td>\n",
       "      <td>0xf2507a6a+++0x3a05a53a</td>\n",
       "      <td>1202194586+++173522494</td>\n",
       "      <td>/tiramisu/food/ythh0257.pdf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/tiramisu/.tiramisu/___tiramisu_versions/0x188...</td>\n",
       "      <td>0xb2404f64+++0x3a05a53a</td>\n",
       "      <td>1202194586+++173522494</td>\n",
       "      <td>/tiramisu/food/ythh0257.pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/tiramisu/.tiramisu/___tiramisu_versions/0x188...</td>\n",
       "      <td>0x7bea55b6+++0x3a05a53a</td>\n",
       "      <td>1202194586+++173522494</td>\n",
       "      <td>/tiramisu/food/ythh0257.pdf</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image                   nodeID  \\\n",
       "0  /tiramisu/.tiramisu/___tiramisu_versions/0x188...  0xf2507a6a+++0x3a05a53a   \n",
       "1  /tiramisu/.tiramisu/___tiramisu_versions/0x188...  0xb2404f64+++0x3a05a53a   \n",
       "2  /tiramisu/.tiramisu/___tiramisu_versions/0x188...  0x7bea55b6+++0x3a05a53a   \n",
       "\n",
       "           originalNodeID                  originalPDF  page  \n",
       "0  1202194586+++173522494  /tiramisu/food/ythh0257.pdf     0  \n",
       "1  1202194586+++173522494  /tiramisu/food/ythh0257.pdf     1  \n",
       "2  1202194586+++173522494  /tiramisu/food/ythh0257.pdf     2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see all the PDFs ready for page stream segmentation.\n",
    "\n",
    "results = return_from_neo4j(GET_PDFS_ORDERED)\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create valid sequential pairs for labeling.\n",
    "\n",
    "# Sort by nodeID and page\n",
    "results = results.sort_values(by=['originalNodeID', 'page']).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "# Generate overlapping pairs\n",
    "pairs = []\n",
    "for i in range(len(results) - 1):\n",
    "    current_row = results.iloc[i]\n",
    "    next_row = results.iloc[i + 1]\n",
    "    \n",
    "    # If nodeID changes, add an empty second pair element\n",
    "    if current_row['originalNodeID'] != next_row['originalNodeID']:\n",
    "         pairs.append({\n",
    "            \"image1\": current_row['image'],\n",
    "            \"image2\": None,\n",
    "            \"originalNodeID\": current_row['originalNodeID'],\n",
    "            \"page1\": current_row['page'],\n",
    "            \"page2\": None,\n",
    "            \"originalPDF\": current_row['originalPDF']\n",
    "        })\n",
    "    else:\n",
    "         pairs.append({\n",
    "            \"image1\": current_row['image'],\n",
    "            \"image2\": next_row['image'],\n",
    "            \"originalNodeID\": current_row['originalNodeID'],\n",
    "            \"page1\": current_row['page'],\n",
    "            \"page2\": next_row['page'],\n",
    "            \"originalPDF\": current_row['originalPDF']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>nodeID</th>\n",
       "      <th>originalNodeID</th>\n",
       "      <th>originalPDF</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>/tiramisu/.tiramisu/___tiramisu_versions/0x3de...</td>\n",
       "      <td>0xd0b59d09+++0x1c75e10b</td>\n",
       "      <td>565574989+++1900608366</td>\n",
       "      <td>/tiramisu/fossil_fuel/ppwl0228.pdf</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>/tiramisu/.tiramisu/___tiramisu_versions/0x826...</td>\n",
       "      <td>0xf24a1fd+++0x1df57eef</td>\n",
       "      <td>647597939+++1067243026</td>\n",
       "      <td>/tiramisu/chemical/ljfd0346.pdf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>/tiramisu/.tiramisu/___tiramisu_versions/0x826...</td>\n",
       "      <td>0x253d9781+++0x1df57eef</td>\n",
       "      <td>647597939+++1067243026</td>\n",
       "      <td>/tiramisu/chemical/ljfd0346.pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                image  \\\n",
       "41  /tiramisu/.tiramisu/___tiramisu_versions/0x3de...   \n",
       "42  /tiramisu/.tiramisu/___tiramisu_versions/0x826...   \n",
       "43  /tiramisu/.tiramisu/___tiramisu_versions/0x826...   \n",
       "\n",
       "                     nodeID          originalNodeID  \\\n",
       "41  0xd0b59d09+++0x1c75e10b  565574989+++1900608366   \n",
       "42   0xf24a1fd+++0x1df57eef  647597939+++1067243026   \n",
       "43  0x253d9781+++0x1df57eef  647597939+++1067243026   \n",
       "\n",
       "                           originalPDF  page  \n",
       "41  /tiramisu/fossil_fuel/ppwl0228.pdf     4  \n",
       "42     /tiramisu/chemical/ljfd0346.pdf     0  \n",
       "43     /tiramisu/chemical/ljfd0346.pdf     1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image1': '/tiramisu/.tiramisu/___tiramisu_versions/0x66efcb5a+++0xc29ab2c5/gqxp0324_page_0.png',\n",
       " 'image2': '/tiramisu/.tiramisu/___tiramisu_versions/0x66efcb5a+++0x2e84fafa/gqxp0324_page_1.png',\n",
       " 'originalNodeID': '144429932+++4128447885',\n",
       " 'page1': 0,\n",
       " 'page2': 1,\n",
       " 'originalPDF': '/tiramisu/opioid/gqxp0324.pdf'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the pairs in JSONL format, we can add this back to LabelStudio for labeling for page stream segmentation. You can add the following parameters into Tiramisu Actions as such:\n",
    "\n",
    "<img src=\"imgs/parameters.png\" width=\"800\" />   \n",
    "\n",
    "Then, LabelStudio will look like this for your labeling task:\n",
    "\n",
    "<img src=\"imgs/labeling.png\" width=\"800\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once labeled, you can update Tiramisu with a new `Document` node type which gets used in downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def crc32(data):\n",
    "\tdata = bytes(data, 'UTF-8')\n",
    "\n",
    "\treturn hex(zlib.crc32(data) & 0xffffffff)  # crc32 returns a signed value, &-ing it will match py3k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a fake labeled dataset to upload to Tiramisu\n",
    "# candidates are originalPDF paths\n",
    "candidates = [\n",
    "    \"/tiramisu/food/ythh0257.pdf\",\n",
    "    \"/tiramisu/opioid/gqxp0324.pdf\"\n",
    "]\n",
    "\n",
    "# note the inclusive left end (starting from page 1, not 0) and exclusive right end\n",
    "candidate_pages = [\n",
    "    [[0,2], # pages 1 to 2 of ythh0257.pdf are part of one document\n",
    "     [2,4]],# pages 3 to 4 of ythh0257.pdf are part of one document        \n",
    "    [[0,1], # pages 1 of gqxp0324.pdf are part of one document\n",
    "     [1,6]] # pages 2 to 6 of gqxp0324.pdf are part of one document\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new documentID: 0x137f9419_0_1\n",
      "new documentID: 0x137f9419_2_3\n",
      "new documentID: 0xf76d7f16_0_0\n",
      "new documentID: 0xf76d7f16_1_5\n"
     ]
    }
   ],
   "source": [
    "for doc, path in zip(candidate_pages, candidates):\n",
    "    for pages in doc:\n",
    "        \n",
    "        newID = crc32(path) + \"_\" + str(pages[0]) + \"_\" + str(pages[-1]-1)\n",
    "        print(f\"new documentID: {newID}\")\n",
    "        submit_to_tiramisu(f\"\"\"\n",
    "                    MERGE (a:Document {{nodeID: \"{newID}\"}})   \n",
    "                    \"\"\")\n",
    "        time.sleep(2) # to ensure that the NEO4J query for document creation runs first\n",
    "        for i in range(pages[0], pages[-1]):\n",
    "            submit_to_tiramisu(f\"\"\"\n",
    "                    match (e:Folder) - [:CONTAINS] -> (n:File) - [:SPLIT_INTO] -> (c:File) \n",
    "                    where n.fileExtension = \"pdf\" and n.originalPath = \n",
    "                    \"{path}\" \n",
    "                    and c.page = {i} \n",
    "                    MATCH (a:Document {{nodeID: \"{newID}\"}})   \n",
    "                    CREATE (c) - [:PART_OF] -> (a)\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the documents will look like the following:\n",
    "\n",
    "<img src=\"imgs/document_creation.png\" width=\"800\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always undo the document creation step by running the following query:\n",
    "```cypher\n",
    "MATCH (n) - [r:PART_OF] -> (c:Document)  DELETE r, c\n",
    "```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Four - Handwriting extraction\n",
    "\n",
    "Some PDFs have handwriting which must be removed both to increase the accuracy of OCR and to mitigate the potential risk of re-identifying individuals. We have provided all the steps to create synthetic handwriting training data, the training script, and the inference code to remove handwriting all in a separate folder called [`handwriting_extraction`](handwriting_extraction/README.md). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Five - Text extraction\n",
    "\n",
    "We can extract text from the scanned PDFs, digital-native PDFs, and Microsoft Documents. We use Apache Tika and TesseractOCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get all the necessary files from Tiramisu for extraction. The requirements for this step is listed at [`requirements.txt`](entity_recognition/requirements.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tika import parser\n",
    "import pytesseract\n",
    "from pathlib import Path\n",
    "\n",
    "import shutil\n",
    "\n",
    "TIRAMISU_PATH = #set your local path that contains .tiramisu\n",
    "all_pdfs = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:CONVERT_TO] -> (f:File) \n",
    "where e.fileExtension = 'pdf' and f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, c.tiramisuPath as tiramisu_path, c.name as name, f.tiramisuPath as image_path, c.scanned as scanned\n",
    "\"\"\")\n",
    "all_pdfs['local_path'] = all_pdfs['tiramisu_path'].apply(lambda x: (Path(TIRAMISU_PATH) / Path(x).relative_to('/tiramisu/')).as_posix())\n",
    "all_pdfs['image_local_path'] = all_pdfs['image_path'].apply(lambda x: (Path(TIRAMISU_PATH) / Path(x).relative_to('/tiramisu/')).as_posix())\n",
    "\n",
    "all_ms = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['xls', 'xlsx', 'ppt', 'pptx', 'doc', 'docx'] \n",
    "return e.nodeID as nodeID, e.tiramisuPath as tiramisu_path, e.name as name, e.fileExtension as file_extension\n",
    "\"\"\")\n",
    "all_ms['local_path'] = all_ms['tiramisu_path'].apply(lambda x: (Path(TIRAMISU_PATH) / Path(x).relative_to('/tiramisu/')).as_posix())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have assumed that you have already run handwriting extraction on scanned PDFs by sending the following results to handwriting extraction step: \n",
    "```python\n",
    "all_pdfs.loc[all_pdfs.scanned == True]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first Apache Tika all MS documents by using the hosted Tika at http://localhost:9998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(all_ms.iterrows(), total = all_ms.shape[0]):\n",
    "    parsed = parser.from_file(row['local_path'], 'http://localhost:9998/tika')\n",
    "    with open(f\"ms_tika/{row['nodeID']}.json\", \"w\") as f:\n",
    "        json.dump(parsed, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then OCR the scanned and electronic PDFs. We OCR electronic PDFs because the Core Collection has lots of digital -native PDFs that contain images (which are not part of the digital content). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(all_pdfs.loc[all_pdfs.scanned == False].iterrows(), total = all_pdfs.loc[all_pdfs.scanned == False].shape[0]):\n",
    "    config = \"-l eng --oem 1 --psm 3  -c preserve_interword_spaces=1\"\n",
    "    nodeID = row['nodeID']\n",
    "    path = row['image_local_path']\n",
    "    if not Path(\"pdf_ocr/\" + nodeID + \".txt\").exists():\n",
    "        with open(\"pdf_ocr/\" + nodeID + \".txt\", \"w\") as f:\n",
    "            f.write(pytesseract.image_to_string(Image.open(path), config = config))\n",
    "            \n",
    "# redacted images is from handwriting extraction\n",
    "for path in tqdm(Path(\"redacted_images/\").glob(\"*.png\"),\\\n",
    "                 total = all_pdfs.loc[all_pdfs.scanned == True].shape[0]):\n",
    "    nodeID = path.stem\n",
    "    config = \"-l eng --oem 1 --psm 3  -c preserve_interword_spaces=1\"\n",
    "    \n",
    "    if not Path(\"pdf_ocr/\" + nodeID + \".txt\").exists():\n",
    "        with open(\"pdf_ocr/\" + nodeID + \".txt\", \"w\") as f:\n",
    "            f.write(pytesseract.image_to_string(Image.open(path.as_posix()), config = config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pool all the text in one folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in Path(\"ms_tika/\").glob(\"*.json\"):\n",
    "    with open(text, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(\"ms_text/\" + text.stem + '.txt', \"w\" ) as f:\n",
    "        if data['content'] is not None:\n",
    "            f.write(data['content'].strip())\n",
    "\n",
    "# copying the OCR text \n",
    "for i in Path(\"pdf_ocr\").glob(\"*.txt\"):\n",
    "    shutil.copy(i, f\"all_text/{i.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Six - Entity recognition & disambiguation & redaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use spaCy's entity recognition pipeline to train our own entity recognition models based on NHGRI data. We fine-tune models with small labeled samples of the Core Collection. We have provided the training script, hyperparameters, and dataset creation scripts all in a separate folder called [`entity_recognition`](entity_recognition/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the entities are detected, we can apply entity disambiguation. We use a \"seed and expand\" approach where we start with a known list of individuals with their aliases, and then we use fuzzy matching to match against those known list. Then, the remaining ones are separated into separate individuals with their own aliases again with fuzzy matching.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "1. generate aliases from a starting list of individuals\n",
    "2. match all detected names to these aliases\n",
    "3. match remainder of names to these aliases by edit distance (fuzzy matching)\n",
    "4. find valid new names that represent new individuals from remainder\n",
    "5. match valid new names to one another by edit distance (fuzzy matching)\n",
    "6. match the remainder of names to the valid new names with generated aliases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a list of starting individuals at `starting_individuals.txt`. The text file looks like:\n",
    "\n",
    "    john smith; johnny smith; j. c. smith    \n",
    "    jane doe; jan doe; jane c. doe\n",
    "\n",
    "where each line represents a known individual and the `;` separate known aliases.\n",
    "\n",
    "Also create a list of individuals that are easily missed, perhaps due to uncommon name structure at `missed_strings.txt`. The text file looks like:\n",
    "\n",
    "    peter john david; p. john de david; P. John de David   \n",
    "    la caprio; lacaprio; Hela LaCaprio\n",
    "    \n",
    "where each line represents a known individual and the last element is the name you wish to associate the known aliases with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rust_utils import _sliding_window\n",
    "\n",
    "from polyfuzz.models import EditDistance\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = {}\n",
    "with open(\"knowledge_base/starting_individuals.txt\", \"r\",  encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        IDs[i+1] = [i.strip() for i in line.split(\";\")]\n",
    "        \n",
    "# difficult disambiguations should be manually put (or ones you are for sure about!)\n",
    "missed = {}\n",
    "with open(\"knowledge_base/missed_strings.txt\", \"r\",  encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        lines = [i.strip() for i in line.split(\";\")]\n",
    "        \n",
    "        for i in lines[:-1]:\n",
    "            \n",
    "            missed[i] = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove prefixes/suffixes \n",
    "prefixes_suffixes = [\"mr.\", \"mr\", \"mrs\", \"mrs.\", \"dr\", \"dr.\", \"phd\", \"ph.d\", \"ph.d.\", \"ms\", \"ms.\", \"m.p.h.\", \"mph\", \"m.p.h\", \"mister\", \"miss\", \"doctor\",\"frs\", \"professor\", \"prof\", \"prof.\"]\n",
    "\n",
    "prefix_suffix_pattern = r'\\b(?:' + \"|\".join(map(re.escape, prefixes_suffixes)) + r')\\b'\n",
    "\n",
    "# normalize all names before creating aliases\n",
    "def normalize_name(name):\n",
    "\n",
    "    # a very long name, most likely an error in detecting a name\n",
    "    if len(name) > 40:\n",
    "        return \"\"\n",
    "\n",
    "    \n",
    "    # remove all prefix and suffix\n",
    "    cleaned_name = re.sub(prefix_suffix_pattern, '', unidecode(name.lower()))\n",
    "\n",
    "    # remove every leading and trailing commas\n",
    "    cleaned_name = re.sub(r'^[^a-zA-Z]+|[^a-zA-Z]+$', '', cleaned_name)\n",
    "    \n",
    "    # remove everything that is not a alphabetic character and a comma\n",
    "    # this also removes periods, hyphens, and dashes\n",
    "    cleaned_name = re.sub(r\"[^a-zA-Z\\s,]\" ,'', cleaned_name)\n",
    "\n",
    "    # if there is a comma left, it's an inverted name\n",
    "    if \",\" in cleaned_name:\n",
    "        \n",
    "        # if there is a comma, there should only be one as its usually lastname, first name\n",
    "        if len(cleaned_name.split(\",\")) > 2:\n",
    "            return \"\"\n",
    "        else:\n",
    "            first = cleaned_name.split(\",\")[0]\n",
    "            last = cleaned_name.split(\",\")[-1]\n",
    "\n",
    "            if len(first) == 1 or len(last) == 1:\n",
    "                return \"\"\n",
    "            else:\n",
    "                return last.strip() + \" \" + \" \".join(cleaned_name.split(\",\")[1:-1]).strip() +  first.strip()\n",
    "    \n",
    "    return cleaned_name\n",
    "\n",
    "# with normalized names, find alises that match those names\n",
    "def provide_aliases(name):\n",
    " \n",
    "    aliases = []\n",
    "    \n",
    "    firstname = name.split()[0]\n",
    "    lastname = name.split()[-1]\n",
    "    middlename = \" \".join(name.split()[1:-1])\n",
    "    \n",
    "    aliases.append(name)\n",
    "    \n",
    "    # first initial lastname\n",
    "    aliases.append(firstname[0]+ \" \" + lastname)\n",
    "    \n",
    "    # firstname lastname\n",
    "    aliases.append(firstname + \" \" + lastname)\n",
    "    \n",
    "    # firstname lastname no spaces\n",
    "    aliases.append(firstname+lastname)\n",
    "\n",
    "    # first initial lastname no spaces\n",
    "    aliases.append(firstname[0] + lastname)\n",
    "    \n",
    "    # last name, first name\n",
    "    aliases.append(lastname + \", \" + firstname)\n",
    "    \n",
    "\n",
    "    # if there is more than an initial for middle name, find more possible aliases\n",
    "    if len(middlename) > 0:\n",
    "        aliases.append(firstname + \" \" + middlename[0] + \" \"+ lastname )\n",
    "        aliases.append(firstname + \" \" + middlename + lastname)\n",
    "        aliases.append(firstname[0] + \" \" + middlename[0] + \" \" + lastname)\n",
    "        aliases.append(firstname + middlename[0] + \" \" + lastname)\n",
    "        aliases.append(firstname[0]+middlename[0]+lastname)\n",
    "        aliases.append(firstname[0]+middlename[0]+ \" \" + lastname)\n",
    "        aliases.append(lastname + \" \"+ firstname[0] + middlename[0])\n",
    "        if len(middlename) > 1:\n",
    "            aliases.append(middlename + \" \"+ lastname)\n",
    "            aliases.append(firstname + \" \"+ middlename)\n",
    "\n",
    "    return aliases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alias_counter = {}\n",
    "for ID, names in tqdm(IDs.items()):\n",
    "    for i, name in enumerate(names):\n",
    "        normalized = normalize_name(name)\n",
    "        if i == 0:\n",
    "            ID_name = normalized\n",
    "        if normalized != \"\":\n",
    "            if normalized not in alias_counter:\n",
    "                alias_counter[normalized] = set([ID_name])\n",
    "            else:\n",
    "                alias_counter[normalized].add(ID_name)\n",
    "        for alias in provide_aliases(normalized):\n",
    "            if alias not in alias_counter:\n",
    "                alias_counter[alias] = set([ID_name])\n",
    "            else:\n",
    "                alias_counter[alias].add(ID_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the model from entity recognition step\n",
    "nlp = spacy.load('../entity_recognition/data_curve/500_samples/runs_1/model-best/')\n",
    "# load the texts from text extraction step\n",
    "corpus = Path(\"all_text\").glob(\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, detect all individuals in the corpus. We use a sliding window function written in Rust; the source code is provided below. You can use [maturin](https://github.com/PyO3/maturin) to create a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```rust\n",
    "use pyo3::prelude::*;\n",
    "\n",
    "#[pyfunction]\n",
    "fn _sliding_window(_py: Python, words: Vec<String>, window_size: usize, overlap_size: usize)\n",
    "    -> PyResult<Vec<Vec<String>>>\n",
    "{\n",
    "    let mut result = Vec::new();\n",
    "    if words.len() <= window_size {\n",
    "        result.push(words)\n",
    "    }\n",
    "    else {\n",
    "      \n",
    "    let mut start = 0;\n",
    "\n",
    "    while start + window_size <= words.len() {  \n",
    "        let end = start + window_size;  \n",
    "        let window: Vec<String> = words[start..end].to_vec();   \n",
    "        result.push(window);   \n",
    " \n",
    "        start += window_size - overlap_size;\n",
    "    }\n",
    "    let window: Vec<String> = words[start - overlap_size - overlap_size..words.len()].to_vec();   \n",
    "    result.push(window);   \n",
    "}\n",
    " Ok(result)\n",
    "}   \n",
    "\n",
    "#[pymodule]\n",
    "fn rust_utils(_py: Python, m: &PyModule) -> PyResult<()> {   \n",
    "    m.add_function(wrap_pyfunction!(_sliding_window, m)?)?;  \n",
    "    Ok(())   \n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = defaultdict(set)\n",
    "all_orgs = defaultdict(set)\n",
    "for i, row in tqdm(corpus, total = len(corpus)):\n",
    "\n",
    "    # use the sliding window approach in Rust\n",
    "    # to ensure that all of the sequences fit into the spacy RoBERTa model\n",
    "    windows = _sliding_window(row['text'].split(), 300, 50)\n",
    "    \n",
    "    for window in windows:\n",
    "        doc = nlp(\" \".join(window))\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                all_names[row['nodeID']].add(ent.text)\n",
    "            elif ent.label_ == \"ORG\":\n",
    "                all_orgs[row['nodeID']].add(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a fuzzy matching model based on edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model to find the distances from one another\n",
    "# this grows at O(n^2) where n is the total number of detected names in a corpus\n",
    "model = EditDistance(n_jobs=-1, scorer=fuzz.WRatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First match based on direct match to the known aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "not_matched = []\n",
    "\n",
    "for name in [j for i in all_names.values() for j in i]:\n",
    "    cleaned_name = normalize_name(name)\n",
    "    \n",
    "    if cleaned_name in alias_counter:\n",
    "        continue\n",
    "\n",
    "    if cleaned_name in missed:\n",
    "        continue\n",
    "    \n",
    "    not_matched.append(cleaned_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of unique names\n",
    "len(set([j for i in all_names.values() for j in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of names not matched by the initial starting list or their aliases\n",
    "len(set(not_matched))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now match all of the unmatched names to the aliases of the starting known list using fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matches = model.match(not_matched,\n",
    "                      [i for i in alias_counter.keys()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find names that were matched to an alias with a match score higher than 0.92\n",
    "\n",
    "valid = matches.loc[matches.Similarity > .92]\n",
    "\n",
    "valid['matched_to_full_name'] = valid['To'].str.split().str.len()\n",
    "\n",
    "\n",
    "# dictionary of those matched to the original starting list\n",
    "matched_to_starting_list = valid.loc[(valid.matched_to_full_name > 1)].groupby('From').agg({\"To\": set})['To'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of additional names from the unmatched pool that got matched to an alias\n",
    "len(matched_to_starting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get new individuals from the remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_matched_round_2 = []\n",
    "new_individuals = []\n",
    "for name in not_matched:\n",
    "\n",
    "    # from the unmatched pool earlier,\n",
    "    # now consider the name matched\n",
    "    # if the name is matched from the edit distance grouping\n",
    "    if name in matched_to_starting_list:\n",
    "        continue\n",
    "    \n",
    "    split_name = name.split()\n",
    "    if len(split_name) > 1:\n",
    "\n",
    "        # if any of the first or lastnames are just initials, \n",
    "        # move to additional matching round\n",
    "        if len(split_name[0]) == 1 or len(split_name[-1]) == 1:\n",
    "            not_matched_round_2.append(name)\n",
    "\n",
    "        # if both of the first and lastnames are more than initials,\n",
    "        # consider the name to be a new individual\n",
    "        elif len(split_name[0]) > 2 and len(split_name[-1]) > 2:\n",
    "            new_individuals.append(name)\n",
    "\n",
    "        # move to additional matching round\n",
    "        else:\n",
    "            not_matched_round_2.append(name)\n",
    "    \n",
    "    # move to additional matching round\n",
    "    else:\n",
    "        not_matched_round_2.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return True if the two strings have higher than 92 Wratio score\n",
    "def match(s1, s2):\n",
    "    return fuzz.WRatio(s1, s2) >= 93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from all of the possible new names \n",
    "# group similar ones together by edit distance\n",
    "grs = [] \n",
    "for i, name in tqdm(enumerate(set(new_individuals)), total = len(set(new_individuals))):\n",
    "    for j, g in enumerate(grs):\n",
    "        if all(match(name, w) for w in g):\n",
    "            g.append(name)\n",
    "            break\n",
    "    else:\n",
    "        grs.append([name, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from the similar groups\n",
    "# take the longest string to be the key and rest to be aliases\n",
    "# this is to ensure that we do not lost information\n",
    "\n",
    "new_individual_list = {}\n",
    "for i in grs:\n",
    "    max_name = max(i, key = len)\n",
    "    for j in i:\n",
    "        if j not in new_individual_list:\n",
    "            new_individual_list[j] = set([max_name])\n",
    "        else:\n",
    "            new_individual_list[j].add(max_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now match the unmatched names by the generated aliases of the new individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases_new = {}\n",
    "for group in grs:\n",
    "    ID_name = max(group, key = len)\n",
    "    for name in group:\n",
    "        normalized = normalize_name(name)\n",
    "        if normalized != \"\":\n",
    "            if normalized not in aliases_starting:\n",
    "                aliases_new[normalized] = set([ID_name])\n",
    "            else:\n",
    "                aliases_new[normalized].add(ID_name)\n",
    "        for alias in provide_aliases(normalized):\n",
    "            if alias not in aliases_new:\n",
    "                aliases_new[alias] = set([ID_name])\n",
    "            else:\n",
    "                aliases_new[alias].add(ID_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created new aliases for new individuals, let's go through the whole process and disambiguate names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_names = []\n",
    "\n",
    "names_with_initial_kb_and_aliases_matched = []\n",
    "\n",
    "names_final_assigned = []\n",
    "\n",
    "\n",
    "IDs_matched = []\n",
    "nodeIDs = []\n",
    "\n",
    "for nodeID, list_of_names in tqdm(all_names.items()):\n",
    "    for name in list_of_names:\n",
    "\n",
    "        # normalize the name\n",
    "        cleaned_name = normalize_name(name)\n",
    "        total_names.append(cleaned_name)\n",
    "\n",
    "        # if the name was manually disambiguated\n",
    "        if cleaned_name in missed:\n",
    "            IDs_matched.append((cleaned_name, normalize_name(missed[cleaned_name]), nodeID))\n",
    "            names_with_initial_kb_and_aliases_matched.append(normalize_name(missed[cleaned_name]))\n",
    "            names_final_assigned.append(normalize_name(missed[cleaned_name]))\n",
    "            continue\n",
    "\n",
    "        # if the name is part of the aliases of the starting list of individuals\n",
    "        if cleaned_name in aliases_starting:\n",
    "            if len(list(aliases_starting[cleaned_name])) == 1:\n",
    "                IDs_matched.append((cleaned_name, list(aliases_starting[cleaned_name])[0], nodeID))\n",
    "                names_with_initial_kb_and_aliases_matched.append(list(aliases_starting[cleaned_name])[0])\n",
    "                names_final_assigned.append(list(aliases_starting[cleaned_name])[0])\n",
    "            else:\n",
    "                IDs_matched.append((cleaned_name, \"##PERSON##\", nodeID))\n",
    "                names_with_initial_kb_and_aliases_matched.append(cleaned_name)\n",
    "                names_final_assigned.append(cleaned_name)\n",
    "            continue\n",
    "\n",
    "        # if the name was fuzzy matched to aliases of the starting list of individuals\n",
    "        if cleaned_name in matched_to_starting_list:\n",
    "\n",
    "            if len(list(alias_counter[list(matched_to_starting_list[cleaned_name])[0]])) == 1:\n",
    "                IDs_matched.append((cleaned_name, list(alias_counter[list(matched_to_starting_list[cleaned_name])[0]])[0], nodeID))\n",
    "                names_final_assigned.append(list(alias_counter[list(matched_to_starting_list[cleaned_name])[0]])[0])\n",
    "            else:\n",
    "                IDs_matched.append((cleaned_name, \"##PERSON##\", nodeID))\n",
    "                names_final_assigned.append(cleaned_name)\n",
    "            continue\n",
    "\n",
    "        # if the name is part of the aliases of the new list of individuals \n",
    "        if cleaned_name in aliases_new:\n",
    "            if len(list(aliases_new[cleaned_name])) == 1:\n",
    "                IDs_matched.append((cleaned_name, list(aliases_new[cleaned_name])[0], nodeID))\n",
    "                names_final_assigned.append(list(aliases_new[cleaned_name])[0])\n",
    "            else:\n",
    "                IDs_matched.append((cleaned_name, \"##PERSON##\", nodeID))\n",
    "                names_final_assigned.append(cleaned_name)\n",
    "            names_with_initial_kb_and_aliases_matched.append(cleaned_name)\n",
    "            continue\n",
    "\n",
    "        split_name = cleaned_name.split()\n",
    "\n",
    "        # if still unmatched then check \n",
    "        if len(split_name) > 1:\n",
    "            if len(split_name[0]) == 1 or len(split_name[-1]) == 1:\n",
    "                IDs_matched.append((cleaned_name, \"##PERSON##\", nodeID))\n",
    "                names_final_assigned.append(cleaned_name)\n",
    "            elif len(split_name[0]) > 2 and len(split_name[-1]) > 2:\n",
    "                IDs_matched.append((cleaned_name, list(new_individual_list[cleaned_name])[0], nodeID))\n",
    "                names_final_assigned.append(list(new_individual_list[cleaned_name])[0])\n",
    "            else:\n",
    "                IDs_matched.append((cleaned_name, \"##PERSON##\", nodeID))\n",
    "                names_final_assigned.append(cleaned_name)\n",
    "        else:\n",
    "            if cleaned_name in new_individual_list:\n",
    "                IDs_matched.append((cleaned_name, list(new_individual_list[cleaned_name])[0], nodeID))\n",
    "                names_final_assigned.append(list(new_individual_list[cleaned_name])[0])\n",
    "            else:\n",
    "                IDs_matched.append((cleaned_name, \"##PERSON##\", nodeID))\n",
    "                names_final_assigned.append(cleaned_name)\n",
    "        names_with_initial_kb_and_aliases_matched.append(cleaned_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view if this has worked well. If not, consider changing the score cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(IDs_matched, columns = [\"input\", \"matched\", \"nodeID\"])\n",
    "\n",
    "results.loc[temp.input.str.contains(\"smith\")].sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the identifier system for downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(IDs_matched, columns = [\"input\", \"matched\", \"nodeID\"]).to_parquet(\"../../pii_detection/knowledge_base/matched_identifiers_240220.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"knowledge_base/new_individuals.txt\", \"w\") as f:\n",
    "    for i in set(new_individual_list.values()):\n",
    "        f.write(i + \"\\n\")\n",
    "        \n",
    "aliases_starting_json = {k:list(v) for k, v in aliases_starting.items()}\n",
    "with open(\"knowledge_base/starting_individuals_aliases.json\", \"w\") as f:\n",
    "    json.dump(aliases_starting_json, f)\n",
    "\n",
    "\n",
    "\n",
    "aliases_new_json = {k:list(v) for k, v in aliases_new.items()}\n",
    "with open(\"knowledge_base/new_individuals_aliases.json\", \"w\") as f:\n",
    "    json.dump(aliases_new_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the same procedure for organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now replace names of individuals by their disambiguated identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "whitespace_regex = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "email_regex = re.compile(r'[%\\w.+-—:]+@[\\w-]+\\.[\\w.-]+')\n",
    "parentheses_regex = re.compile(r'\\[(?:[^\\]]*)\\]|\\((?:[^)]*)\\)')\n",
    "\n",
    "prefixes_suffixes = [\"mr.\", \"mr\", \"mrs\", \"mrs.\", \"dr\", \"dr.\", \"phd\", \"ph.d\", \"ms\", \"ms.\", \"mister\", \"miss\", \"doctor\", \"jr.\", \"jr\", \"frs\"]\n",
    "\n",
    "prefix_suffix_pattern = r'\\b(?:' + \"|\".join(map(re.escape, prefixes_suffixes)) + r')\\b'\n",
    "\n",
    "class KnowledgeBase:\n",
    "    \"\"\"\n",
    "    A class representing a knowledge base.\n",
    "\n",
    "    Attributes:\n",
    "    - knowledge_dict (dict): A dictionary to store knowledge entries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a new KnowledgeBase object.\n",
    "        \"\"\"\n",
    "        self.knowledge_dict = {}\n",
    "        with open(\"knowledge_base/starting_individuals.txt\", \"r\",  encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                self.knowledge_dict[normalize_name(line.strip().split(';')[0].strip())]= i + 1\n",
    "        total = i \n",
    "\n",
    "        with open(\"knowledge_base/new_individuals.txt\", \"r\",  encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if line.strip() not in self.knowledge_dict:\n",
    "                    self.knowledge_dict[normalize_name(line.strip())] = i+1+total\n",
    "                else:\n",
    "                    print(line.strip())\n",
    "\n",
    "        with open(\"knowledge_base/starting_individuals_aliases.json\", \"r\") as f:\n",
    "            alias_counter = json.load(f)\n",
    "        with open(\"knowledge_base/new_individuals_aliases.json\", \"r\") as f:\n",
    "            alias_counter_new_kb = json.load(f)\n",
    "\n",
    "        self.alias_counter = alias_counter\n",
    "        self.alias_counter_new_kb = alias_counter_new_kb\n",
    "        \n",
    "    def get_entry(self, name):\n",
    "        \"\"\"\n",
    "        Retrieve an entry from the knowledge base.\n",
    "\n",
    "        Args:\n",
    "        - name: potential name to find in the knowledge base\n",
    "        Returns:\n",
    "        - The name and the value associated with the key, or an empty list if the key does not exist.\n",
    "        \"\"\"\n",
    "\n",
    "        cleaned_name = normalize_name(name)\n",
    "\n",
    "        if cleaned_name in self.IDs_merged:\n",
    "            return cleaned_name, self.IDs_merged[cleaned_name]\n",
    "        elif cleaned_name in self.alias_counter:\n",
    "\n",
    "            if len(self.alias_counter[cleaned_name]) == 1:\n",
    "                return self.alias_counter[cleaned_name][0], self.knowledge_dict[normalize_name(self.alias_counter[cleaned_name][0])]\n",
    "            else:\n",
    "                return cleaned_name, []\n",
    "        elif cleaned_name in self.alias_counter_new_kb:\n",
    "            if len(self.alias_counter_new_kb[cleaned_name]) == 1:\n",
    "                return self.alias_counter_new_kb[cleaned_name][0], self.knowledge_dict[normalize_name(self.alias_counter_new_kb[cleaned_name][0])]\n",
    "            else:\n",
    "                return cleaned_name, []\n",
    "        else:\n",
    "            return cleaned_name, []\n",
    "\n",
    "    \n",
    "def normalize_name(name):\n",
    "    \n",
    "    if len(name) > 40:\n",
    "        return name\n",
    "    \n",
    "    cleaned_name = name.replace(\"\\n\", \" \")\n",
    "    # remove all prefix and suffix\n",
    "    cleaned_name = re.sub(prefix_suffix_pattern, '', unidecode(cleaned_name.lower()))\n",
    "    \n",
    "    \n",
    "    # remove every leading and trailing commas\n",
    "    cleaned_name = re.sub(r'^[^a-zA-Z]+|[^a-zA-Z]+$', '', cleaned_name)\n",
    "    \n",
    "    # remove everything that is not a alphabetic character and a comma\n",
    "    \n",
    "    cleaned_name = re.sub(r\"[^a-zA-Z\\s,]\" ,'', cleaned_name)\n",
    "    if \",\" in cleaned_name:\n",
    "        \n",
    "        # if there is a comma, there should only be one as its usually lastname, first name\n",
    "        if len(cleaned_name.split(\",\")) > 2:\n",
    "            return name\n",
    "        else:\n",
    "            first = cleaned_name.split(\",\")[0]\n",
    "            last = cleaned_name.split(\",\")[-1]\n",
    "\n",
    "            if len(first) == 1 or len(last) == 1:\n",
    "                return name\n",
    "            else:\n",
    "                return last + \" \" + first\n",
    "    \n",
    "    return cleaned_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "starting_id = len(IDs)\n",
    "\n",
    "def replace_substring(s, replacement, position, length_of_replaced):\n",
    "    s = s[:position] + replacement + s[position+length_of_replaced:]\n",
    "    return(s)\n",
    "\n",
    "def find_candidate(string):\n",
    "    # remove extraneous spaces in the middle\n",
    "    string = string.lower().strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    # unscramble inverted names\n",
    "    if ',' in string:\n",
    "        string = string.split(',')[1].strip() + ' ' + string.split(',')[0].strip()\n",
    "    else:\n",
    "        candidate = list(set([c.entity_ for c in kb.get_entry(string)]))\n",
    "        \n",
    "        if len(candidate) == 0:\n",
    "            if len(string.split()) >= 2 and len(string) > 5:\n",
    "                return [find_from_custom_corpus(string.lower().strip().replace('\\n', ' '))]\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return candidate\n",
    "    \n",
    "def pre_criteria(string):\n",
    "    if len(string) < 2:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# additional regex patterns for those that may have been missed by the model\n",
    "idnum_pattern = re.compile(\"(?:\\+?(\\d{1,3}))?[-.(]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})(?: *x(\\d+))?\")\n",
    "email_pattern = re.compile(\"([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,6})\")\n",
    "\n",
    "def redact(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    s = text\n",
    "    \n",
    "    for ent in reversed(doc.ents):\n",
    "        if ent.label_ == 'PERSON':\n",
    "            if pre_criteria(ent.text):\n",
    "                _, ID = find_candidate(ent.text)\n",
    "\n",
    "                if len(ID) > 0:\n",
    "                    \n",
    "                    replacement_string = \"##PERSON\" + \"-\" + str(ID[0]) + \"##\"\n",
    "\n",
    "                else:\n",
    "\n",
    "                    replacement_string = \"##PERSON##\"\n",
    "                \n",
    "                length_of_replaced = ent.end_char - ent.start_char\n",
    "                s = replace_substring(s, replacement_string, ent.start_char, length_of_replaced)\n",
    "        elif ent.label_ == \"IDNUM\" or ent.label_ == 'LOC' or ent.label_ == 'EMAIL':\n",
    "\n",
    "            \n",
    "            replacement_string = \"##\" + ent.label_ + \"##\"\n",
    "            \n",
    "            length_of_replaced = ent.end_char - ent.start_char\n",
    "            s = replace_substring(s, replacement_string, ent.start_char, length_of_replaced)\n",
    "    \n",
    "    for m in reversed(list(idnum_pattern.finditer(s))):\n",
    "        length_of_replaced = m.end() - m.start()\n",
    "        s = replace_substring(s, '##IDNUM##', m.start(), length_of_replaced)\n",
    "\n",
    "    for m in reversed(list(email_pattern.finditer(s))):\n",
    "        length_of_replaced = m.end() - m.start()\n",
    "        s = replace_substring(s, '##EMAIL##', m.start(), length_of_replaced)\n",
    "    del text, doc\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase()\n",
    "\n",
    "for path in tqdm(Path(\"all_text/\").glob(\"*txt\")):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    redacted = redact(data)\n",
    "    \n",
    "    with open(f\"redacted_text/{path.stem}.txt\", \"w\") as f:\n",
    "        f.write(redacted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text is all redacted and ready for analyses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
