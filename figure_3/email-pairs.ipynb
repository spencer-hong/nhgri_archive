{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26738a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "# import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils_tiramisu import *\n",
    "\n",
    "# this is the same TIRAMISU_PATH as shown in start_here.ipynb\n",
    "TIRAMISU_PATH = \n",
    "\n",
    "\n",
    "# import datasets\n",
    "import pandas as pd\n",
    "import json\n",
    "# from pytesseract import Output\n",
    "# import pytesseract\n",
    "# import cv2\n",
    "import re\n",
    "from math import ceil, floor\n",
    "from unidecode import unidecode\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609b9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(path):\n",
    "\timage = Image.open(path).convert('RGB')\n",
    "\n",
    "\tinputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "\toutputs = model(**inputs)\n",
    "\tlogits = outputs.logits\n",
    "\n",
    "\t# model predicts one of the 16 RVL-CDIP classes\n",
    "\tpredicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "\n",
    "\treturn (path, model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9896e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10529876",
   "metadata": {},
   "source": [
    "We first need to detect all email documents and get recipients/senders from them. All emails are from scanned PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed648b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_documents = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:PART_OF] -> (d:Document) \n",
    "where e.fileExtension = 'pdf' \n",
    "WITH c, d \n",
    "match (c) - [:CONVERT_TO] -> (f:File) \n",
    "where f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, d.nodeID as documentID, c.page as page, f.tiramisuPath as tiramisuPath \n",
    "\"\"\")\n",
    "\n",
    "first_pages = pdf_documents.sort_values(['documentID', 'page']).groupby('documentID').head(1)\n",
    "first_pages['tiramisuPath'] = first_pages['tiramisuPath'].apply(lambda x: (Path(TIRAMISU_PATH) / Path(x).relative_to('/tiramisu/')).as_posix())\n",
    "pdf_documents['tiramisuPath'] = pdf_documents['tiramisuPath'].apply(lambda x: (Path(TIRAMISU_PATH) / Path(x).relative_to('/tiramisu/')).as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b84df1",
   "metadata": {},
   "source": [
    "We first find the categories of each document. Upon request, we can provide the `../cache/categories_of_first_pages_071023.parquet` which is the output of a [Document Image Transformer](https://arxiv.org/abs/2203.02378) model that was trained on 16 classes in [RVL-CDIP](https://adamharley.com/rvl-cdip/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a1596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emails = pd.read_parquet('../cache/categories_of_first_pages_071023.parquet')\n",
    "emails = pd.merge(pdf_documents, emails[['path', 'class']], left_on = 'tiramisuPath', right_on = 'path')\n",
    "emails = emails.sort_values(['documentID', 'page'])\n",
    "documents = emails.groupby('documentID').head(1)\n",
    "\n",
    "# We uncomment out the following line of code to send to a labeling interface\n",
    "# emails.loc[emails.documentID.isin(documents.loc[documents['class'] == 'email'].documentID)]\\\n",
    "# [['documentID', 'nodeID', 'page', 'path']].\\\n",
    "# to_parquet(\"../models/email_parsing/emails_to_inference_240414.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b41f88",
   "metadata": {},
   "source": [
    "We sent just the emails to a LayoutLMV3 model fine-tuned for detecting email fields. We used LabelStudio for labeling similar to the page stream segmentation and entity recognition tasks. Upon request, we can provide the fields of the detected email outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1358d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_inferenced = pd.read_parquet(\"../models/email_parsing/inference_result_on_240414.parquet\").drop_duplicates(subset = \"nodeID\")\n",
    "all_emails_inferenced = pd.merge(all_emails_inferenced, emails.loc[emails.documentID.isin(documents.loc[documents['class'] == 'email'].documentID)]\\\n",
    "[['nodeID', 'path']], on = 'nodeID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903999e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_labels = all_emails_inferenced.groupby([\"documentID\"]).agg({\"labels\": lambda x: list(itertools.chain.from_iterable(x))}).reset_index()\n",
    "only_labels['length'] = only_labels['labels'].apply(lambda x: len([i for i in x if i != \"O\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d850b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e82cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_box(bbox, width, height):\n",
    "\t return [\n",
    "\t\t width * (bbox[0] / 1000),\n",
    "\t\t height * (bbox[1] / 1000),\n",
    "\t\t width * (bbox[2] / 1000),\n",
    "\t\t height * (bbox[3] / 1000),\n",
    "\t ]\n",
    "    \n",
    "def find_text(box, df, threshold):\n",
    "    return df.loc[(df.left >= box[0]-threshold) & (df.top >= box[1]-threshold) & (df.left <=box[2]+threshold) & (df.top <= box[3] + threshold)]\n",
    "\n",
    "def translate(outputs, encoding, img):\n",
    "\tpredictions = outputs.logits.cpu().argmax(-1).squeeze().tolist()\n",
    "\ttoken_boxes = encoding.bbox.cpu().squeeze().tolist()\n",
    "\n",
    "\theight, width = img.shape[0], img.shape[1]\n",
    "\n",
    "\tif len(predictions) == 512:\n",
    "\t\tpredictions = [predictions]\n",
    "\t\ttoken_boxes = [token_boxes]\n",
    "\n",
    "\ttrue_predictions =[]\n",
    "\ttrue_boxes = []\n",
    "\tSTRIDE_COUNT = 128\n",
    "\tfor i, (pred, box, mapped) in enumerate(zip(predictions, token_boxes, offset_mapping)):\n",
    "\t\tis_subword = np.array(mapped.squeeze().tolist())[:,0] != 0\n",
    "\t\tif i == 0:\n",
    "\t\t\ttrue_predictions += [id2label[pred_] for idx, pred_ in enumerate(pred) if (not is_subword[idx])]\n",
    "\t\t\ttrue_boxes += [unnormalize_box(box_, width, height) for idx, box_ in enumerate(box) if not is_subword[idx]]\n",
    "\t\telse:\n",
    "\t\t\ttrue_predictions += [id2label[pred_] for idx, pred_ in enumerate(pred) if (not is_subword[idx])][STRIDE_COUNT + 1 - sum(is_subword[:STRIDE_COUNT + 1]):]\n",
    "\t\t\ttrue_boxes += [unnormalize_box(box_, width, height) for idx, box_ in enumerate(box) if not is_subword[idx]][STRIDE_COUNT + 1 - sum(is_subword[:STRIDE_COUNT + 1]):]\n",
    "\n",
    "\treturn true_predictions, true_boxes\n",
    "\n",
    "def prepare_input(path):\n",
    "\timg = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "\n",
    "\tencoding = processor(img, return_tensors=\"pt\", truncation=True, stride = 128,\\\n",
    "\t\t\t\t\t\tpadding = 'max_length', max_length = 512, return_overflowing_tokens=True, return_offsets_mapping = True)\n",
    "\n",
    "\tx = []\n",
    "\tfor i in range(0, len(encoding['pixel_values'])):\n",
    "\t\tx.append(encoding['pixel_values'][i])\n",
    "\tx = torch.stack(x)\n",
    "\tencoding['pixel_values'] = x\n",
    "\toffset_mapping = encoding.pop('offset_mapping')\n",
    "\toverflow_to_sample_mapping = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "\tfor k,v in encoding.items():\n",
    "\t\tencoding[k] = v.to(device)\n",
    "\treturn encoding, img, offset_mapping, overflow_to_sample_mapping\n",
    "def get_entities_from_email(row):\n",
    "    chosen = row\n",
    "    encoding, img, offset_mapping, overflow_to_sample_mapping = prepare_input(chosen['path'])\n",
    "\n",
    "    img = cv2.imread(chosen['path'], cv2.IMREAD_COLOR)\n",
    "    \n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    \n",
    "    d = pytesseract.image_to_data(chosen['path'], output_type=Output.DATAFRAME)\n",
    "    d = d.loc[d.text.notna()]\n",
    "    entities = []\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    current_entity = None\n",
    "    bboxes = []\n",
    "    for i, (box, label) in enumerate(zip(chosen['boxes'], chosen['labels'])):\n",
    "        text_df = find_text(box, d, 0)\n",
    "        if label != \"O\":\n",
    "            entity = label.split('-')[1].replace(\"CC\", \"TO\").strip()\n",
    "#             entity = label.split(\"-\")[1].strip()\n",
    "            \n",
    "            \n",
    "            if box[0] < box[2] and box[1] < box[3]:\n",
    "                if text_df.shape[0] == 0:\n",
    "                    text = pytesseract.image_to_string(img[max(floor(box[1]), \\\n",
    "                            0):min(ceil(box[3]),height), max(floor(box[0]), 0):min(ceil(box[2]), width)]).strip()\n",
    "                elif text_df.shape[0] >1:\n",
    "                    text = pytesseract.image_to_string(img[max(floor(box[1]), 0)\\\n",
    "                                            :min(ceil(box[3]), height), max(floor(box[0]), 0)\\\n",
    "                                                           :min(ceil(box[2]), width)]).strip()\n",
    "                else:\n",
    "                    text = text_df.iloc[0]['text']\n",
    "            else:\n",
    "                text = \"\"\n",
    "            if current_entity is None:\n",
    "                entities.append([text])\n",
    "#                 token_list.append([i])\n",
    "                current_entity = entity\n",
    "                bboxes.append(box)\n",
    "                labels.append(current_entity)\n",
    "                \n",
    "                tokens.append((current_entity, entities[-1]))\n",
    "            elif current_entity == entity:\n",
    "#                 print(find_text(box, d, 0)['text'].shape[0])\n",
    "                if not text == entities[-1][-1]:\n",
    "                    entities[-1].append(text)\n",
    "#                     token_list[-1].append(i)\n",
    "            else:\n",
    "#                 print(find_text(box, d, 0)['text'].shape[0])\n",
    "                entities.append([text])\n",
    "#                 token_list.append([i])\n",
    "                current_entity = entity\n",
    "                labels.append(current_entity)\n",
    "                bboxes.append(box)\n",
    "            \n",
    "                tokens.append((current_entity, entities[-1]))\n",
    "        else:\n",
    "            if text_df.shape[0] == 1:\n",
    "\n",
    "                tokens.append((\"TEXT\", text_df.iloc[0]['text']))\n",
    "                bboxes.append(box)\n",
    "                \n",
    "            else:\n",
    "                tokens.append((\"TEXT\", \"\"))\n",
    "                bboxes.append(box)\n",
    "\n",
    "    \n",
    "    return (entities, labels, tokens, row['nodeID'], chosen['boxes'], chosen['labels'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\n",
    "# based on the checkpoint we provide from the hub\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bed6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_inferenced = all_emails_inferenced.loc[all_emails_inferenced.documentID.isin(only_labels.loc[only_labels.length != 0].documentID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_inferenced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae460668",
   "metadata": {},
   "source": [
    "We now get the different email fields such as `To`, `From`, `CC`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c54d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities_collected = []\n",
    "for i, row in tqdm(all_emails_inferenced.iterrows(), total = all_emails_inferenced.shape[0]):\n",
    "    all_entities_collected.append(get_entities_from_email(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b4d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/email_parsing/all_entities_df_240415.pkl\", \"rb\") as f:\n",
    "    all_entities_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb4c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities_df['label'] = all_entities_df['label'].apply(lambda x: [i if i != \"CC\" else \"TO\" for i in x ])\n",
    "all_entities_df['tokens'] = all_entities_df['tokens'].apply(lambda x: [i if i[0] != \"CC\" else (\"TO\", i[1]) for i in x ])\n",
    "all_entities_df['tokens_label'] = all_entities_df['tokens'].apply(lambda x: [i[0] for i in x])\n",
    "all_entities_df['tokens_word'] = all_entities_df['tokens'].apply(lambda x: [i[1] if isinstance(i[1], list) else [i[1]] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a94eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged = pd.merge(all_entities_df, emails[[\"nodeID\", \"documentID\", \"page\"]], left_on = \"nodeID\", right_on =\"nodeID\", how = 'inner')\n",
    "merged = merged.drop_duplicates('nodeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac045d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.sort_values(['documentID', 'page'])\n",
    "\n",
    "last = None\n",
    "lasttoken = 0\n",
    "documentIDs = []\n",
    "tokens = []\n",
    "nodeIDs = []\n",
    "labels = []\n",
    "label = []\n",
    "name = []\n",
    "boxes = []\n",
    "for i, row in merged.iterrows():\n",
    "    if row['documentID'] != last:\n",
    "        lasttoken = 0\n",
    "    tokens.append([(k[0],k[1]) for k in row['tokens']])\n",
    "    name.append([k for k in row['name']])\n",
    "    documentIDs.append(row['documentID'])\n",
    "    boxes.append([(i[0], i[1] + lasttoken, i[2], i[3] + lasttoken) for i in row['boxes']])\n",
    "    nodeIDs.append(row['nodeID'])\n",
    "    label.append(row['label'])\n",
    "    labels.append(row['labels'])\n",
    "    last = row['documentID']\n",
    "    lasttoken += max([i[-1] for i in row['boxes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "271d604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc = pd.DataFrame({\n",
    "    \"documentID\": documentIDs,\n",
    "    \"name\": name,\n",
    "    \"tokens\": tokens,\n",
    "    \"boxes\": boxes,\n",
    "    'label': label,\n",
    "    \"labels\": labels,\n",
    "    \"nodeID\": nodeIDs\n",
    "})\n",
    "per_doc = per_doc.groupby([\"documentID\"]).agg({'label': 'sum', \\\n",
    "                                        \"nodeID\": lambda x: list(x), \"name\": \"sum\", \"tokens\": \"sum\", \"boxes\": \"sum\", \"labels\" : lambda x: list(itertools.chain.from_iterable(x))}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec78fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualifier(x):\n",
    "    if \"TO\" in x and \"FROM\" in x:\n",
    "        return True\n",
    "    elif \"TO\" in x and \"HEADER\" in x:\n",
    "        return True\n",
    "    elif \"FROM\" in x and \"HEADER\" in x:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def combine(row):\n",
    "    results = []\n",
    "    for i in range(len(row['label'])):\n",
    "        if row['label'][i] == \"TO\" or row['label'][i] == \"FROM\" or row['label'][i] == 'HEADER':\n",
    "            results.append((\" \".join(row['name'][i]), row['label'][i]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ac10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc = per_doc[per_doc['label'].apply(qualifier)]\n",
    "per_doc[\"names_normalized\"] = per_doc.apply(lambda x: combine(x), axis = 1)\n",
    "per_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d249be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_O(tokens):\n",
    "    consecutive_Os = []\n",
    "#     end = 0\n",
    "    \n",
    "    # Initialize variables to track consecutive \"O\"s\n",
    "    start_index = None\n",
    "    consecutive_count = 0\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # If current label is \"O\"\n",
    "        if token[0] != \"FROM\" and token[0] != \"TO\" and token[0] != \"FROM\" and token[0] != \"TO\":\n",
    "            # If this is the first \"O\" in a consecutive sequence\n",
    "            if consecutive_count == 0:\n",
    "                start_index = i\n",
    "            consecutive_count += 1\n",
    "        # If current label is not \"O\" and we have a consecutive sequence of \"O\"s\n",
    "        elif consecutive_count > 0:\n",
    "            end_index = i - 1\n",
    "#             end_y = bounding_boxes[end_index][3]  # Ending vertical position\n",
    "#             consecutive_Os.append((start_index, bounding_boxes[start_index][1], end_y, consecutive_count))\n",
    "            consecutive_Os.append((start_index, end_index, consecutive_count))\n",
    "            # Reset consecutive sequence variables\n",
    "            start_index = None\n",
    "            consecutive_count = 0\n",
    "#         end = max(end, bounding_boxes[i-1][3])\n",
    "    \n",
    "    # If there are consecutive \"O\"s at the end of the sequence\n",
    "    if consecutive_count > 0:\n",
    "        end_index = len(labels) - 1\n",
    "#         end_y = bounding_boxes[-1][3]  # Ending vertical position\n",
    "#         consecutive_Os.append((start_index, bounding_boxes[start_index][1], end, consecutive_count))\n",
    "        consecutive_Os.append((start_index, end_index, consecutive_count))\n",
    "    \n",
    "    return sorted(consecutive_Os, key = lambda x: x[-1], reverse = True)\n",
    "\n",
    "def get_conversation(tokens):\n",
    "    \n",
    "    conversations_index = find_consecutive_O(tokens)\n",
    "    \n",
    "    conversations = []\n",
    "    \n",
    "    for i in conversations_index:\n",
    "        try:\n",
    "            conversations.append((\" \".join([i[-1] for i in tokens[i[0]+1:i[1]]]), i[0]))\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16bd26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['conversations'] = per_doc['tokens'].apply(lambda x: get_conversation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802eb2d",
   "metadata": {},
   "source": [
    "We use a simple name splitting model since bounding boxes captured in the Layout model includes all of the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load(\"../models/entity_splitting/nhgri_version/model-best/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503899ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, row in tqdm(per_doc.iterrows(), total = per_doc.shape[0]):\n",
    "    \n",
    "    temp = []\n",
    "    for name in row['names_normalized']:\n",
    "        doc = nlp(name[0])\n",
    "        temp_temp = []\n",
    "        for ent in doc.ents:\n",
    "            temp_temp.append(ent.text)\n",
    "        temp.append(temp_temp)\n",
    "    results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5712a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['split_names'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07cea383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "whitespace_regex = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "email_regex = re.compile(r'[%\\w.+-—:]+@[\\w-]+\\.[\\w.-]+')\n",
    "# parentheses_regex = re.compile(r\"\\([^)]*\\)\")\n",
    "parentheses_regex = re.compile(r'\\[(?:[^\\]]*)\\]|\\((?:[^)]*)\\)')\n",
    "def separate_institutions(text):\n",
    "    \n",
    "    matches = []\n",
    "    for match in parentheses_regex.findall(text):\n",
    "        text = text.replace(match, \"\")\n",
    "        matches.append(match)\n",
    "    return text, matches\n",
    "\n",
    "def remove_emails(text):\n",
    "    matches = []\n",
    "    for match in email_regex.findall(text):\n",
    "        text = text.replace(match, \"\")\n",
    "        matches.append(match)\n",
    "    return text, matches\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \n",
    "\n",
    "    return whitespace_regex.sub(\"\", text)\n",
    "\n",
    "def remove_keywords(text):\n",
    "    return text.replace(\"[mailto:]\", \"\").replace(\"<mailto:>\", \"\").replace(\"[mailto: ]\", \"\").\\\n",
    "replace(\"[c]\", \"\").replace(\"[e]\", \"\").replace(\"phd\", \"\").replace(\"ph.d\", \"\").replace(\"@ internet\", \"\").replace(\"@internet\", \"\")\n",
    "def reorder_names(text):\n",
    "    \n",
    "    if text[0] != \",\" or text[-1] != \",\":\n",
    "        if len(text.split(',')) == 2:\n",
    "            return text.split(',')[-1].strip() + \" \" + text.split(\",\")[0].strip()\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "\n",
    "def remove_trailing_char(text):\n",
    "    start_index = 0\n",
    "    end_index = len(text)\n",
    "\n",
    "    # Find the starting index of alphanumeric character\n",
    "    for i, char in enumerate(text):\n",
    "        if char.isalnum():\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    # Find the ending index of alphanumeric character\n",
    "    for i in range(len(text) - 1, -1, -1):\n",
    "        if text[i].isalnum():\n",
    "            end_index = i + 1\n",
    "            break\n",
    "\n",
    "    # Remove trailing and leading punctuation\n",
    "    trimmed_text = text[start_index:end_index].strip(punctuation)\n",
    "    return trimmed_text\n",
    "\n",
    "\n",
    "def normalize_names(row):\n",
    "    normalize_names = []\n",
    "    \n",
    "    orgs = []\n",
    "    emails = []\n",
    "    for group in row['split_names']:\n",
    "        temp = []\n",
    "        for name in group:\n",
    "            text, extracted_email = remove_emails(name.strip().lower())\n",
    "            \n",
    "            emails.extend(extracted_email)\n",
    "            \n",
    "            text = remove_keywords(text.strip()).strip()\n",
    "            \n",
    "            text, extracted_org = separate_institutions(text)\n",
    "            \n",
    "            orgs.extend(extracted_org)\n",
    "            \n",
    "            text = remove_trailing_char(text.strip()).strip()\n",
    "            \n",
    "            if len(text) < 3:\n",
    "                if len(extracted_email) > 0:\n",
    "                    temp.append(extracted_email[0])\n",
    "                else:\n",
    "                    temp.append(text)\n",
    "            else:\n",
    "                temp.append(reorder_names(text).strip())\n",
    "        normalize_names.append(temp)\n",
    "    return normalize_names\n",
    "\n",
    "prefixes_suffixes = [\"mr.\", \"mr\", \"mrs\", \"mrs.\", \"dr\", \"dr.\", \"phd\", \"ph.d\", \"ms\", \"ms.\", \"mister\", \"miss\", \"doctor\", \"jr.\", \"jr\", \"frs\"]\n",
    "\n",
    "prefix_suffix_pattern = r'\\b(?:' + \"|\".join(map(re.escape, prefixes_suffixes)) + r')\\b'\n",
    "def normalize_name(name):\n",
    "    \n",
    "    if len(name) > 40:\n",
    "        return name\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # remove all prefix and suffix\n",
    "    cleaned_name = re.sub(prefix_suffix_pattern, '', unidecode(name.lower()))\n",
    "    \n",
    "    \n",
    "    # remove every leading and trailing commas\n",
    "    cleaned_name = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$', '', cleaned_name)\n",
    "    \n",
    "    # remove everything that is not a alphabetic character and a comma\n",
    "    \n",
    "    cleaned_name = re.sub(r\"[^a-zA-Z\\s,@0-9]\" ,'', cleaned_name)\n",
    "    if \",\" in cleaned_name:\n",
    "        \n",
    "        # if there is a comma, there should only be one as its usually lastname, first name\n",
    "        if len(cleaned_name.split(\",\")) > 2:\n",
    "            return name\n",
    "        else:\n",
    "            first = cleaned_name.split(\",\")[0]\n",
    "            last = cleaned_name.split(\",\")[-1]\n",
    "\n",
    "            if len(first) == 1 or len(last) == 1:\n",
    "                return name\n",
    "            else:\n",
    "                return cleaned_name\n",
    "    \n",
    "    return cleaned_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9901d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['orgs'] = per_doc.apply(lambda x: normalize_names(x), axis = 1)\n",
    "per_doc['emails'] = per_doc.apply(lambda x: normalize_names(x), axis = 1)\n",
    "per_doc['normalized_names'] = per_doc.apply(lambda x: normalize_names(x), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bd402",
   "metadata": {},
   "source": [
    "We now encode all names into identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0d6c1",
   "metadata": {},
   "source": [
    "This cached file mapped disambiguated names to identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "050620dd-9ac5-4329-a2f2-8239130da8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../cache/name_to_ID_email_network_240502.json\", \"r\") as f:\n",
    "    name_to_ID = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b217d00-e16c-45e8-a9f4-a21cfd23ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29e2ab4b-a664-4626-afc2-c3988045b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_name_to_ID(x):\n",
    "    mapped = []\n",
    "    \n",
    "    for names in x:\n",
    "        temp = []\n",
    "        for name in names:\n",
    "            if name in name_to_ID:\n",
    "                temp.append(name_to_ID[name])\n",
    "            else:\n",
    "                temp.append(name)\n",
    "        mapped.append(temp)\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b2c15bec-c181-4e31-a793-a91ca547e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['IDs'] = per_doc['normalized_names'].apply(map_name_to_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b67fa-7914-4e4e-bb86-ab5c15c02375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36b03bd6-3bec-41ac-bdbe-47a1e34dd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_indices_for_paired_pattern(lst):\n",
    "    indices_A = [i for i, val in enumerate(lst) if val == 'FROM']\n",
    "    indices_B = [i for i, val in enumerate(lst) if val == 'TO']\n",
    "\n",
    "    sorted_indices_A = sorted(indices_A)\n",
    "    sorted_indices_B = sorted(indices_B)\n",
    "\n",
    "    sorted_indices = []\n",
    "    for a, b in zip(sorted_indices_A, sorted_indices_B):\n",
    "        sorted_indices.extend([a, b])\n",
    "    # print(sorted_indices)\n",
    "    return sorted_indices\n",
    "\n",
    "\n",
    "def get_pairs_of_emails(row):\n",
    "    number_of_conversations = 0\n",
    "    conversations = []\n",
    "    filtered_label = [i for i in row['label'] if (i == 'HEADER') or (i == 'TO') or (i == 'FROM')]\n",
    "    pairs = []\n",
    "    if 'FROM' not in filtered_label and 'HEADER' in filtered_label:\n",
    "        from_entity = row['IDs'][filtered_label.index('HEADER')]\n",
    "        to_entity = None\n",
    "        for i, (label, field) in enumerate(zip(filtered_label, row['IDs'])):\n",
    "            if label == 'TO':\n",
    "                if from_entity is not None:\n",
    "                    for ent in field:\n",
    "                        for ent_ in from_entity:\n",
    "                            pairs.append((ent_, ent, ))\n",
    "                    conversations.append(pairs)\n",
    "                    pairs = []\n",
    "                    number_of_conversations += 1\n",
    "                else:\n",
    "                    if i == len(filtered_label) -1:\n",
    "                        for ent in field:\n",
    "                            for ent_ in to_entity:\n",
    "                                pairs.append((ent, ent_))\n",
    "                        conversations.append(pairs)\n",
    "                        pairs = []\n",
    "                        number_of_conversations += 1\n",
    "                    else:\n",
    "                        # from_entity = None\n",
    "                        to_entity = field\n",
    "            if label == 'FROM':\n",
    "                if to_entity is not None:\n",
    "                    # print(to_entity)\n",
    "                    for ent in field:\n",
    "                        for ent_ in to_entity:\n",
    "                            pairs.append((ent, ent_))\n",
    "                    conversations.append(pairs)\n",
    "                    pairs = []\n",
    "                    number_of_conversations += 1\n",
    "                else:\n",
    "                    if i == len(filtered_label) -1:\n",
    "                        for ent in from_entity:\n",
    "                            for ent_ in field:\n",
    "                                pairs.append((ent, ent_))\n",
    "                        conversations.append(pairs)\n",
    "                        pairs = []\n",
    "                        number_of_conversations += 1\n",
    "                    else:\n",
    "                        from_entity = field\n",
    "    elif 'TO' not in row['label'] and 'HEADER' in filtered_label:\n",
    "        to_entity = row['IDs'][filtered_label.index('HEADER')]\n",
    "        from_entity = None\n",
    "        for i, (label, field) in enumerate(zip(filtered_label, row['IDs'])):\n",
    "            if label == 'TO':\n",
    "                if from_entity is not None:\n",
    "                    for ent in field:\n",
    "                        for ent_ in from_entity:\n",
    "                            pairs.append((ent_, ent))\n",
    "                    number_of_conversations += 1\n",
    "                    conversations.append(pairs)\n",
    "                    pairs = []\n",
    "                else:\n",
    "                    if i == len(filtered_label) -1:\n",
    "                        for ent in field:\n",
    "                            for ent_ in to_entity:\n",
    "                                pairs.append((ent, ent_))\n",
    "                        number_of_conversations += 1\n",
    "                        conversations.append(pairs)\n",
    "                        pairs = []\n",
    "                    else:\n",
    "                        # from_entity = None\n",
    "                        to_entity = field\n",
    "            if label == 'FROM':\n",
    "                if to_entity is not None:\n",
    "                    # print(to_entity)\n",
    "                    for ent in field:\n",
    "                        for ent_ in to_entity:\n",
    "                            pairs.append((ent, ent_))\n",
    "                    number_of_conversations += 1\n",
    "                    conversations.append(pairs)\n",
    "                    pairs = []\n",
    "                else:\n",
    "                    if i == len(filtered_label) -1:\n",
    "                        for ent in from_entity:\n",
    "                            for ent_ in field:\n",
    "                                pairs.append((ent, ent_))\n",
    "                        number_of_conversations += 1\n",
    "                        conversations.append(pairs)\n",
    "                        pairs = []\n",
    "                    else:\n",
    "                        from_entity = field\n",
    "\n",
    "    else:\n",
    "        to_entity = None\n",
    "        from_entity = None\n",
    "\n",
    "        sorted_indices = sort_indices_for_paired_pattern(filtered_label)\n",
    "        for i, index in enumerate(sorted_indices):\n",
    "        # for i, (label, field) in enumerate(zip(filtered_label, row['name_ID'])):\n",
    "            if filtered_label[index] == 'TO':\n",
    "                if from_entity is not None:\n",
    "                    for ent in row['IDs'][index]:\n",
    "                        for ent_ in from_entity:\n",
    "                            pairs.append((ent_, ent))\n",
    "                    number_of_conversations += 1\n",
    "                    conversations.append(pairs)\n",
    "                    pairs = []\n",
    "                else:\n",
    "                    if i == len(filtered_label) -1:\n",
    "                        for ent in row['IDs'][index]:\n",
    "                            for ent_ in to_entity:\n",
    "                                pairs.append((ent, ent_))\n",
    "                        number_of_conversations += 1\n",
    "                        conversations.append(pairs)\n",
    "                        pairs = []\n",
    "                    else:\n",
    "                        # from_entity = None\n",
    "                        to_entity = row['IDs'][index]\n",
    "            if filtered_label[index] == 'FROM':\n",
    "                if to_entity is not None:\n",
    "                    # print(to_entity)\n",
    "                    for ent in row['IDs'][index]:\n",
    "                        for ent_ in to_entity:\n",
    "                            pairs.append((ent, ent_))\n",
    "                    number_of_conversations += 1\n",
    "                    conversations.append(pairs)\n",
    "                    pairs = []\n",
    "                else:\n",
    "                    if i == len(filtered_label) -1:\n",
    "                        for ent in from_entity:\n",
    "                            for ent_ in row['IDs'][index]:\n",
    "                                pairs.append((ent, ent_))\n",
    "                        number_of_conversations += 1\n",
    "                        conversations.append(pairs)\n",
    "                        pairs = []\n",
    "                    else:\n",
    "                        from_entity = row['IDs'][index]\n",
    "                    # to_entity = None\n",
    "        \n",
    "    return conversations, number_of_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8653853d-a9ca-486e-bd89-72cc3f8e9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['pairs'] = per_doc.apply(lambda x: get_pairs_of_emails(x), axis =1)\n",
    "per_doc['num_conversations'] = per_doc['pairs'].apply(lambda x: x[-1])\n",
    "per_doc['pairs'] = per_doc['pairs'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "614b9fef-87c2-4c87-81b7-af8d5bf734a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888431e5",
   "metadata": {},
   "source": [
    "We manually fix frequently missed dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b6c35-6366-4a56-80da-994e4aaf6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = []\n",
    "counter = 0\n",
    "counter_1 = 0\n",
    "date_reg_exp = re.compile('(\\d{1,2}/\\\\d{1,2}/\\\\d{2,4})')\n",
    "for i, row in per_doc.iterrows():\n",
    "    temp = []\n",
    "    try:\n",
    "        dates = datefinder.find_dates(\" \".join(list(row['name'])[list(row['label']).index('DATE')]))\n",
    "        datestring= \" \".join(list(row['name'])[list(row['label']).index('DATE')])\n",
    "        if \"4/15/94 4:11PM (2058 bytes: 41 In)\" == datestring:\n",
    "            temp.append(datetime(1994, 4, 15))\n",
    "        elif \"Wednesday, Apri! 28, 2004 8:26 PM\" == datestring:\n",
    "            temp.append(datetime(2004, 4, 28))\n",
    "        elif \"4 Wednesday, December 20, 2000 6:40 PM\" == datestring:\n",
    "            temp.append(datetime(2000, 12, 20))\n",
    "        elif \"Thursday, July 14,2005 3:49PM .\" == datestring:\n",
    "            temp.append(datetime(2005, 7, 14))\n",
    "        elif \"9/15/94 9:05PM (1973 bytes:\" == datestring:\n",
    "            temp.append(datetime(1997, 9, 15))\n",
    "        elif \"Monday, Apri! 28, 2003 11:20 AM\" == datestring:\n",
    "            temp.append(datetime(2003, 4, 28))\n",
    "        elif \"12/01/98 TUE 13:35 FAX 301 SIP™837\" == datestring:\n",
    "            temp.append(datetime(1998, 12, 1))\n",
    "        elif \"10/2/92 1:15PM (1019 bytes: 11 ln)\" == datestring:\n",
    "            temp.append(datetime(1992, 10, 2))\n",
    "        elif \"4/11/95 1:51PM (5693 bytes: 86\" == datestring:\n",
    "            temp.append(datetime(1995, 4, 11))\n",
    "        elif \"9/19/94 10:28AM (684 bytes: 9 ln)\" == datestring:\n",
    "            temp.append(datetime(1994, 9, 19))\n",
    "        elif \"8/5/94 1:29PM (1619 bytes: 31 1n)\" == datestring:\n",
    "            temp.append(datetime(1994, 8, 5))\n",
    "        elif \"7/29/94 3:01PM (2168 bytes: 36 ln)\" == datestring:\n",
    "            temp.append(datetime(1994, 7, 29))\n",
    "        elif \"Friday, November 08; 2002 5:39 PM\" == datestring:\n",
    "            temp.append(datetime(2002, 11, 8))\n",
    "        elif \"11/17/93 10:44AM (520 bytes: 10 ln)\" == datestring:\n",
    "            temp.append(datetime(1993, 11, 17))\n",
    "        elif \"Monday, Apri! 25, 2005 6:42 PM\" == datestring:\n",
    "            temp.append(datetime(2005, 4, 25))\n",
    "        elif \" December 08, 1999 6)08 AM\" == datestring:\n",
    "            temp.append(datetime(1999, 12, 8))\n",
    "        for date in dates:\n",
    "            if date.year > 1980 and date.year < 2016:\n",
    "                new_datetime_object = datetime(date.year, date.month, date.day)\n",
    "                temp.append(new_datetime_object)\n",
    "            else:\n",
    "                if date in temp:\n",
    "                    print(date, \\\n",
    "                          \" \".join(list(row['name'])[list(row['label']).index('DATE')].tolist()))\n",
    "                    counter_1 += 1\n",
    "        all_dates.append(list(set(temp)))\n",
    "    except:\n",
    "        counter += 1\n",
    "#         traceback.print_exc()\n",
    "        all_dates.append([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6a5173d-3469-4a27-83b3-f8c4a330778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['date'] = [i[0] if len(i) > 0 else None for i in all_dates]\n",
    "per_doc = per_doc.loc[per_doc.date.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36d45ea3-f0cc-411b-a4d1-037d8c38f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_doc['conversations'] = per_doc.apply(lambda x: sorted(x['conversations'][:x['num_conversations']], key = lambda y: y[-1]), \n",
    "                                         axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d0050d94-e78b-4e03-baa2-d30f1995373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = []\n",
    "\n",
    "for i, row in per_doc.iterrows():\n",
    "    \n",
    "    for conversation, pair in zip(row['conversations'], row['pairs']):\n",
    "        if pair is None:\n",
    "            print(row)\n",
    "        all_email_pairs.append((conversation[0], pair, row['date'], row['documentID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "053b2abf-62cb-451f-91f6-52d5ab976a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = pd.DataFrame(all_email_pairs, columns = [\"conversation\", \"pair\", \"date\", \"documentID\"]).explode('pair').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "99074433-b76e-4536-bc37-401aca367bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = all_pairs.loc[all_pairs.pair.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e320b68a-5be8-4fd4-8e60-41989d0c451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = all_pairs.loc[all_pairs.pair.notna()].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7b26fe02-f962-4647-8063-251d9c7bb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names_from_email = all_pairs['pair'].to_list()\n",
    "all_names_from_email= [e for l in all_names_from_email for e in l if l is not None]\n",
    "all_identifiers = [i for i in all_names_from_email if str(i).isnumeric()]\n",
    "counter_pre_disambiguated = Counter(all_names_from_email)\n",
    "counter_all_ids = Counter(all_identifiers)\n",
    "\n",
    "\n",
    "counter_names_pre_disambiguated = pd.DataFrame([(counter_pre_disambiguated[i], i) for i in sorted(counter_pre_disambiguated, \\\n",
    "                                                        key = lambda x: counter_pre_disambiguated.get(x),  reverse = True)], columns\\\n",
    "                         = [\"count\", \"ID\"])\n",
    "\n",
    "counter_names_pre_disambiguated['cumsum'] = counter_names_pre_disambiguated['count'].cumsum() / sum(counter_pre_disambiguated.values())\n",
    "counter_names_pre_disambiguated['cumsum'] = 1 - counter_names_pre_disambiguated['cumsum']\n",
    "\n",
    "counter_names_pre_disambiguated.loc[-1] = [0, 'origin', 1]\n",
    "counter_names_pre_disambiguated = counter_names_pre_disambiguated.sort_index()\n",
    "counter_names = pd.DataFrame([(counter_all_ids[i], i) for i in sorted(counter_all_ids, \\\n",
    "                                                        key = lambda x: counter_all_ids.get(x),  reverse = True)], columns\\\n",
    "                         = [\"count\", \"ID\"])\n",
    "\n",
    "counter_names['cumsum'] = counter_names['count'].cumsum() / sum(counter_all_ids.values())\n",
    "counter_names['cumsum'] = 1 - counter_names['cumsum']\n",
    "\n",
    "counter_names.loc[-1] = [0, 'origin', 1]\n",
    "counter_names = counter_names.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "62742c50-40ec-4e9e-9fdb-9aa91f51f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7a55d-daee-4755-9463-d123d5e297ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(3,2), dpi = 300)\n",
    "ax.spines['right'].set_linewidth(0)\n",
    "ax.spines['top'].set_linewidth(0)\n",
    "\n",
    "counter_names['percentage'] = counter_names['cumsum'] * 100\n",
    "counter_names_pre_disambiguated['percentage'] = counter_names_pre_disambiguated['cumsum'] * 100\n",
    "sns.lineplot(data = counter_names.reset_index(), x = 'index', \\\n",
    "            y ='percentage', linewidth = 1, color = 'k', ax = ax, label = \"Disambiguated emails\")\n",
    "\n",
    "sns.lineplot(data = counter_names_pre_disambiguated.reset_index(), x = 'index', \\\n",
    "            y ='percentage', linewidth = 1, color = 'k', ax = ax, alpha = 0.4, label = \"Pre-disambiguated emails\")\n",
    "\n",
    "# ax.axhline(xmin = 0, xmax = 5000, y = 100 * ((counter_all_names.total() - counter_all_ids.total()) / counter_all_names.total()), \\\n",
    "#            c = 'k', linewidth = 0.5, linestyle = '--')\n",
    "\n",
    "# ax.axhline(xmin = 0, xmax = 500, y = 100 * ((counter_all_names.total() - counter_all_ids_valid.total()) / counter_all_names.total()), \\\n",
    "#            c = 'darkred', linewidth = 0.5, linestyle = '--')\n",
    "\n",
    "ax.axvline(ymin=0.05, ymax = 0.97, x = 500, c = 'darkred', linewidth = 0.5, linestyle = '--')\n",
    "# ax.axhline(xmin=0, xmax=1, y = 100 * (counter_names.iloc[500]['cumsum']), c = 'darkred', linewidth = 0.5, linestyle = '--')\n",
    "\n",
    "ax.tick_params(axis='x', which='minor', bottom=True)\n",
    "# ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "# ax.text(550, 12, \"{:.1f}%\".format(100 * ( 1- counter_names.iloc[500]['cumsum'])), c = 'darkred')\n",
    "\n",
    "ax.fill_between(np.arange(0, 500), np.zeros(500), counter_names.iloc[0:500]['cumsum'] *100, color = 'grey')\n",
    "ax.text(550, 55, \"500 most mentioned email sender/recipients\", c = 'darkred')\n",
    "\n",
    "# ax.text(400, -2, \"{:.1f}%\".format(100 * (1 - ((counter_all_names.total() - counter_all_ids.total()) / \\\n",
    "#                                               counter_all_names.total()))), c = 'k')\n",
    "\n",
    "# ax.text(1500, -2, \"disambiguation\", c = 'k')\n",
    "\n",
    "plt.legend(frameon = False)\n",
    "ax.set_ylabel(\"Cumulative % of mentions\\nof individuals\")\n",
    "ax.set_xlabel(\"Individual rank\")\n",
    "plt.show()\n",
    "# plt.savefig(\"../figures/SI_email_disambiguation_pareto.pdf\", dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54adad-b8f9-4cf9-85ce-2274d372658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([i[-1] for i in counter_all_ids.most_common(500)])/sum(counter_pre_disambiguated.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c6539-492b-4648-91f1-85afcdcb2971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21afc8bb-e1cf-42d7-94ce-1925c19acb75",
   "metadata": {},
   "source": [
    "We also cleaned frequently mentioned domains and institutions. This cached file is available upon request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84b12f06-35a5-460f-a415-9816fb260c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_dict = pd.read_excel(\"../models/email_clean_manual/cleaned_domains.xlsx\", sheet_name = \"domains_dict\")[['domain', 'name']]\n",
    "entities_dict = pd.read_excel(\"../models/email_clean_manual/cleaned_domains.xlsx\", sheet_name = \"entities_dict\")[[\"name\", \"fullname\", \"country\", \"city\", \"category\", \"NIH\"]]\n",
    "\n",
    "entities_dict['NIH'] = entities_dict['NIH'].apply(lambda x: True if x == 1 else False)\n",
    "\n",
    "domains_total = pd.merge(domains_dict, entities_dict, left_on = 'name', right_on = 'name')\n",
    "\n",
    "domain_list_total  = domains_total['domain'].tolist()\n",
    "\n",
    "domains_total = domains_total.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b09e5eec-8f1a-467e-ad31-f73fa7fc0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliations_500 = pd.read_csv(\"../models/email_clean_manual/top_500_affiliations_completed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bdb20-745b-4e55-83ec-29d38b15fdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febed62-d29e-4d32-a5bb-e816736894fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_all_ids.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8c02c1a5-8a9c-424c-b411-163674dfbeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = []\n",
    "\n",
    "for i, row in per_doc.iterrows():\n",
    "    \n",
    "    for conversation, pair in zip(row['conversations'], row['pairs']):\n",
    "        if pair is None:\n",
    "            print(row)\n",
    "        all_email_pairs.append((conversation[0], pair, row['date'], row['documentID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a07bac4b-f063-4f2b-8c63-400876c462d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = pd.DataFrame(all_email_pairs, columns = [\"conversation\", \"pair\", \"date\", \"documentID\"]).explode('pair').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "069b0b44-ec90-4560-98f5-4ed3915b30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = all_email_pairs.loc[all_email_pairs.pair.notna()]\n",
    "# all_email_pairs['pair'] =all_email_pairs['pair'].apply(lambda x:  correct(x))\n",
    "all_email_pairs['To'] = all_email_pairs['pair'].str[-1].astype(str)\n",
    "all_email_pairs['From'] = all_email_pairs['pair'].str[0].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "217297e0-d2ce-4254-8e5a-56fa41568f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = all_email_pairs.loc[all_email_pairs.To != all_email_pairs.From]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d64a6927-5322-4c0a-8194-cd17de037ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = all_email_pairs.loc[all_email_pairs.To != all_email_pairs.From]\n",
    "all_email_pairs = all_email_pairs.loc[(all_email_pairs.To.str.isnumeric()) & (all_email_pairs.From.str.isnumeric())]\n",
    "all_email_pairs = all_email_pairs.loc[(all_email_pairs.To.isin([str(i[0]) for i in counter_all_ids.most_common(507)])) &\n",
    "                                      (all_email_pairs.From.isin([str(i[0]) for i in counter_all_ids.most_common(507)]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947e302-9b5b-4c35-94be-ef4e9f02c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(all_email_pairs.To.astype(int).to_list() + all_email_pairs.From.astype(int).to_list())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423b8d7-499f-45bb-b00c-008086605fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5d3d1947-47aa-4577-bafc-9dfe80637961",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_affiliation_from_ID = affiliations_500.set_index(\"ID\")['groundtruth'].to_dict()\n",
    "all_email_pairs['To_org'] = all_email_pairs['To'].apply(lambda x: map_affiliation_from_ID[int(x)] if int(x) in map_affiliation_from_ID else \"other\")\n",
    "all_email_pairs['From_org'] = all_email_pairs['From'].apply(lambda x: map_affiliation_from_ID[int(x)] if int(x) in map_affiliation_from_ID else \"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "66af2c86-602b-4b6a-bbc9-fde8074169a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = entities_dict.set_index(\"name\")['category'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d34d4e39-49ff-4bae-a340-ffb88286bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(x):\n",
    "    if x == \"nih\":\n",
    "        return \"nih\"\n",
    "    elif x in categories_dict:\n",
    "        return categories_dict[x]\n",
    "    else:\n",
    "        return \"other\"\n",
    "all_email_pairs['To_category'] = all_email_pairs[\"To_org\"].apply(lambda x: category(x))\n",
    "all_email_pairs['From_category'] = all_email_pairs[\"From_org\"].apply(lambda x: category(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d717186-ac56-4f57-9a23-4b2df3843e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_email_pairs['To']).union(\n",
    "set(all_email_pairs['From'])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83ed5e5-554a-438b-b4bc-be660b66f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_email_pairs.loc[(all_email_pairs.To_category.isin([\"nih\", \"academic\", \"private-nonprofit\"]))]['To']).union(\n",
    "set(all_email_pairs.loc[(all_email_pairs.From_category.isin([\"nih\", \"academic\", \"private-nonprofit\"]))]['From'])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "25e4ea7d-f284-42fb-bbe6-08d6f990b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_email_pairs['pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9457ce4b-6251-4fcc-a8b1-90c7544409c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs.to_parquet(\"../cache/all_email_pairs_240520.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6f048-c958-432f-a7f6-57e210fcf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs[['conversation', 'date', 'documentID', 'To', 'From']].to_parquet('../models/sbm/emails_network_pairs_top500_240416.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
