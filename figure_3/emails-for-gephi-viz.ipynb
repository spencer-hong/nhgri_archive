{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d700f4-4e4f-43ac-ac8c-2cc5386b0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils_tiramisu import *\n",
    "\n",
    "import itertools \n",
    "from pathlib import Path \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e07f2",
   "metadata": {},
   "source": [
    "`../cache/pdfs_word_excel_powerpoint_010924.parquet` is simply a Pandas DataFrame that contains the combined texts of the scanned/electronic PDFs and MS documents. The columns are `text`, which is the raw text, and `nodeID` which is the nodeIDs of the split single-page PDFs or the MS documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f6bf3-11e5-4fb1-8e1c-61dbbd8ff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "together = pd.read_parquet(\n",
    "    \"../cache/pdfs_word_excel_powerpoint_010924.parquet\"\n",
    ")\n",
    "# together = pd.merge(nhgri_text.reset_index(drop = True).reset_index(), nhgri_text_paths, on=\"nodeID\")\n",
    "map_nodeID_to_docID = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:PART_OF] -> (d:Document) \n",
    "where e.fileExtension = 'pdf' \n",
    "return c.nodeID as nodeID, c.page as page, d.nodeID as documentID, e.originalPath as path\n",
    "\"\"\")\n",
    "all_pdfs = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:CONVERT_TO] -> (f:File) \n",
    "where e.fileExtension = 'pdf' and f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "all_ms = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['doc', 'docx', 'ppt', 'pptx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "folder_structure = pd.concat([all_pdfs, all_ms])\n",
    "\n",
    "\n",
    "map_nodeID_to_page = map_nodeID_to_docID.set_index('nodeID').to_dict()['page']\n",
    "# map_nodeID_to_path = map_nodeID_to_docID.set_index(\"nodeID\").to_dict()['path']\n",
    "map_nodeID_to_docID = map_nodeID_to_docID.set_index('nodeID').to_dict()['documentID']\n",
    "\n",
    "together['docID'] = together['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else x)\n",
    "together['page'] = together['nodeID'].apply(lambda x: map_nodeID_to_page[x] if x in map_nodeID_to_page else 0)\n",
    "together = pd.merge(together, folder_structure, left_on = 'nodeID', right_on = 'nodeID')\n",
    "\n",
    "all_excel = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['xls', 'xlsx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "together = together.loc[~together.nodeID.isin(all_excel['nodeID'].to_list())]\n",
    "\n",
    "together['text'] = together['text'].apply(lambda x: x + \" \")\n",
    "\n",
    "together = together.sort_values(['docID', 'page']).groupby('docID').agg({\"text\": \"sum\", \"path\": set}).reset_index()\n",
    "\n",
    "together['path'] = together['path'].apply(lambda x: list(x)[0])\n",
    "together['text'] = together['text'].str.lower()\n",
    "\n",
    "project_folders = {\n",
    "        \"ENCODE\":[\n",
    "            \"ENCODE/Participants\", \"ENCODE/MS\", \"ENCODE/SAP\", \"ENCODE/OC Information\",\n",
    "            \"ENCODE/PressRelease\", \"ENCODE/ENCODE_2004\", \"ENCODE/publications\", \"ENCODE/Drafts\",\n",
    "        \"ENCODE/Data Standards\", \"ENCODE/encode_align_sop.pdf\", \"ENCODE/ENCODE-PublicationGuidelines 3-29-06.doc\",\n",
    "        \"ENCODE/Minutes\", \"ENCODE/CACR\", \"ENCODE/SAP call minutes 3-15-06.doc\", \"ENCODE/Data release\",\n",
    "        \"ENCODE/Abstracts\", \"ENCODE/Presentations\", \"ENCODE/Scaling\", \"ENCODE/Meeting\", \"ENCODE/MS2\",\n",
    "        \"ENCODE/WorkingGroups\", \"ENCODE/Documents\", \"ENCODE/criteria\", \"ENCODE/Web_site\", \"ENCODE/Hox.doc\", \"ENCODE/Policy\"],\n",
    "        \"modENCODE\": [\"ENCODE/modENCODE\", \"modENCODE\"],\n",
    "        \"HapMap\":[\n",
    " 'Haplotype Map Project'],\n",
    "     \"HGP\": [\n",
    "         \"Large scale sequence/human sequence\", \"Celera\", \"HGP History Summer 2011\", \"sequencingrampupfiles\"],\n",
    "    \"sequence\": [\"Large scale sequence/Box026-010.pdf\", \"Sequence target files\"],\n",
    "    \"ELSI\": [\"ELSI\"]\n",
    "}\n",
    "\n",
    "list_of_entities = []\n",
    "\n",
    "\n",
    "for i, row in tqdm(together.iterrows(), total = together.shape[0]):\n",
    "    temp = []\n",
    "    for group, (folder) in enumerate(project_folders):\n",
    "        \n",
    "        \n",
    "        if any([Path(\"/tiramisu/\"+ subfolder) in Path(row['path']).parents for subfolder in project_folders[folder]]):\n",
    "            \n",
    "            list_of_entities.append((True, folder, row['docID'], row['path']))\n",
    "        elif any([Path(\"/tiramisu/\"+ subfolder) == Path(row['path']) for subfolder in project_folders[folder]]):\n",
    "            list_of_entities.append((True, folder, row['docID'], row['path']))\n",
    "        else:\n",
    "            list_of_entities.append((False, folder, row['docID'], row['path']))\n",
    "projects_df = pd.DataFrame(list_of_entities, columns = [\"entity\", \"text\", 'docID', 'path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7997d13e-ac99-4f4e-8e73-b3b433e39b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgp = projects_df.loc[(projects_df.text == \"HGP\") & (projects_df.entity)].docID.unique()\n",
    "hapmap = projects_df.loc[(projects_df.text == \"HapMap\") & (projects_df.entity)].docID.unique()\n",
    "lsac = projects_df.loc[(projects_df.text == \"sequence\") & (projects_df.entity)].docID.unique()\n",
    "encode = projects_df.loc[(projects_df.text == \"ENCODE\") & (projects_df.entity)].docID.unique()\n",
    "modencode = projects_df.loc[(projects_df.text == \"modENCODE\") & (projects_df.entity)].docID.unique()\n",
    "elsi = projects_df.loc[(projects_df.text == \"ELSI\") & (projects_df.entity)].docID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8005acd",
   "metadata": {},
   "source": [
    "We load the email pairs detected in the other [email-pairs](email-pairs.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6395b592-6c4e-4ac0-813a-98bd50fafeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs = pd.read_parquet(\"../cache/all_email_pairs_240520.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed9155c-2b4e-449d-8695-8f5b8a556740",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs[\"hgp\"] = all_email_pairs['documentID'].apply(lambda x: x in hgp)\n",
    "all_email_pairs[\"hapmap\"] = all_email_pairs['documentID'].apply(lambda x: x in hapmap)\n",
    "all_email_pairs[\"lsac\"] = all_email_pairs['documentID'].apply(lambda x: x in lsac)\n",
    "all_email_pairs[\"encode\"] = all_email_pairs['documentID'].apply(lambda x: x in encode)\n",
    "all_email_pairs[\"modencode\"] = all_email_pairs['documentID'].apply(lambda x: x in modencode)\n",
    "all_email_pairs['elsi'] = all_email_pairs['documentID'].apply(lambda x: x in elsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf68a8-a3b3-4148-917b-91f3330949aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10f784ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs['To'] = all_email_pairs['To'].astype(int)\n",
    "all_email_pairs['From'] = all_email_pairs['From'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7240eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = \\\n",
    "pd.DataFrame(set(all_email_pairs['To'].tolist()).union(set(all_email_pairs['From'].tolist())), \\\n",
    "            columns = ['node'])\n",
    "node_list['org'] = node_list['node'].map(dict(zip(np.hstack([all_email_pairs['To'], all_email_pairs['From']]),\n",
    "    np.hstack([all_email_pairs['To_org'], all_email_pairs['From_org']]))))\n",
    "node_list['category'] = node_list['org'].apply(lambda x: dict(zip(np.hstack([all_email_pairs['To_org'], all_email_pairs['From_org']]),\n",
    "    np.hstack([all_email_pairs['To_category'], all_email_pairs['From_category']])))[x])\n",
    "node_list['ID'] = node_list['node']\n",
    "node_list.to_csv('../models/email_clean_manual/nodes_240520_not_randomized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97647be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16cf3101-504a-4ccf-848d-bfdc524eb9d4",
   "metadata": {},
   "source": [
    "We randomize the keys before publishing the supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21cff35c-36cc-4e8a-aecf-cedcedde33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37453aed-03c8-4f70-af9f-31ab42728975",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(all_email_pairs.To.astype(int).to_list() + all_email_pairs.From.astype(int).to_list())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58c93654-ddda-4d48-879e-bf8b70e729b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list = list(set(all_email_pairs.To.astype(int).to_list() + all_email_pairs.From.astype(int).to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a26a85fc-dbbc-4a9c-a5f2-b75b2c2e5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys = random.sample(range(0, 500), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efb78b02-6121-4760-922d-dfcb3e891b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict(zip(original_list, new_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa277df9-1fa3-49fe-b968-39ebe3b22855",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs['To'] = all_email_pairs['To'].astype(int)\n",
    "all_email_pairs['From'] = all_email_pairs['From'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f85fed0-6554-422d-a037-f9f0b06150f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs['To_new'] = all_email_pairs['To'].map(mapping)\n",
    "all_email_pairs['From_new'] = all_email_pairs['From'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82dd2dda-99d5-40eb-9a57-1bba47c000e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = \\\n",
    "pd.DataFrame(set(all_email_pairs['To_new'].tolist()).union(set(all_email_pairs['From_new'].tolist())), \\\n",
    "            columns = ['node'])\n",
    "node_list['org'] = node_list['node'].map(dict(zip(np.hstack([all_email_pairs['To_new'], all_email_pairs['From_new']]),\n",
    "    np.hstack([all_email_pairs['To_org'], all_email_pairs['From_org']]))))\n",
    "node_list['category'] = node_list['org'].apply(lambda x: dict(zip(np.hstack([all_email_pairs['To_org'], all_email_pairs['From_org']]),\n",
    "    np.hstack([all_email_pairs['To_category'], all_email_pairs['From_category']])))[x])\n",
    "node_list['ID'] = node_list['node']\n",
    "node_list.to_csv('../models/email_clean_manual/nodes_240520.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f2ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ca49e4d-9767-4581-9486-1dc32e9fe990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projects(row):\n",
    "    if row['hgp']:\n",
    "        project = \"HGP\"\n",
    "    elif row['elsi']:\n",
    "        project = 'ELSI'\n",
    "    elif row['hapmap']:\n",
    "        project = 'HapMap'\n",
    "    elif row['lsac']:\n",
    "        project = \"LSAC\"\n",
    "    else:\n",
    "        project= \"Other\"\n",
    "\n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba5ffe-bd7f-472d-9e66-43b70a714772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81e35d16-ca30-4db8-a013-7377be3d180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email_pairs['project'] = all_email_pairs.apply(lambda x: projects(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ef21f48-941e-4ba4-962e-22513c7031fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = all_email_pairs[['From_new', 'To_new', 'date', 'project']]\n",
    "edges = edges.groupby([\"From_new\", \"To_new\", \"project\"]).count().reset_index()\n",
    "# edges['date'] = edges['date'].dt.year\n",
    "edges.columns = ['SOURCE', 'TARGET', 'project', 'WEIGHT']\n",
    "edges.to_csv( '../models/email_clean_manual/edges_240520.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ebc16-9fc1-4c63-9a60-d4e278cbed63",
   "metadata": {},
   "source": [
    "We do all of the network visualization in Gephi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
