{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39c3000-bbdb-491b-9428-30fb7467ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from graph_tool.all import *\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/spencer/manuscript/src/')\n",
    "\n",
    "from utils_tiramisu import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d50573",
   "metadata": {},
   "source": [
    "`../cache/pdfs_word_excel_powerpoint_010924.parquet` is simply a Pandas DataFrame that contains the combined texts of the scanned/electronic PDFs and MS documents. The columns are `text`, which is the raw text, and `nodeID` which is the nodeIDs of the split single-page PDFs or the MS documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df21eb-48f1-4b54-9c86-8ea3f879140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "together = pd.read_parquet(\n",
    "    \"../cache/pdfs_word_excel_powerpoint_010924.parquet\"\n",
    ")\n",
    "# together = pd.merge(nhgri_text.reset_index(drop = True).reset_index(), nhgri_text_paths, on=\"nodeID\")\n",
    "map_nodeID_to_docID = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:PART_OF] -> (d:Document) \n",
    "where e.fileExtension = 'pdf' \n",
    "return c.nodeID as nodeID, c.page as page, d.nodeID as documentID, e.originalPath as path\n",
    "\"\"\")\n",
    "all_pdfs = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:CONVERT_TO] -> (f:File) \n",
    "where e.fileExtension = 'pdf' and f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "all_ms = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['doc', 'docx', 'ppt', 'pptx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "folder_structure = pd.concat([all_pdfs, all_ms])\n",
    "\n",
    "\n",
    "map_nodeID_to_page = map_nodeID_to_docID.set_index('nodeID').to_dict()['page']\n",
    "# map_nodeID_to_path = map_nodeID_to_docID.set_index(\"nodeID\").to_dict()['path']\n",
    "map_nodeID_to_docID = map_nodeID_to_docID.set_index('nodeID').to_dict()['documentID']\n",
    "\n",
    "together['docID'] = together['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else x)\n",
    "together['page'] = together['nodeID'].apply(lambda x: map_nodeID_to_page[x] if x in map_nodeID_to_page else 0)\n",
    "together = pd.merge(together, folder_structure, left_on = 'nodeID', right_on = 'nodeID')\n",
    "\n",
    "all_excel = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['xls', 'xlsx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "together = together.loc[~together.nodeID.isin(all_excel['nodeID'].to_list())]\n",
    "\n",
    "together['text'] = together['text'].apply(lambda x: x + \" \")\n",
    "\n",
    "together = together.sort_values(['docID', 'page']).groupby('docID').agg({\"text\": \"sum\", \"path\": set}).reset_index()\n",
    "\n",
    "together['path'] = together['path'].apply(lambda x: list(x)[0])\n",
    "together['text'] = together['text'].str.lower()\n",
    "\n",
    "project_folders = {\n",
    "        \"ENCODE\":[\n",
    "            \"ENCODE/Participants\", \"ENCODE/MS\", \"ENCODE/SAP\", \"ENCODE/OC Information\",\n",
    "            \"ENCODE/PressRelease\", \"ENCODE/ENCODE_2004\", \"ENCODE/publications\", \"ENCODE/Drafts\",\n",
    "        \"ENCODE/Data Standards\", \"ENCODE/encode_align_sop.pdf\", \"ENCODE/ENCODE-PublicationGuidelines 3-29-06.doc\",\n",
    "        \"ENCODE/Minutes\", \"ENCODE/CACR\", \"ENCODE/SAP call minutes 3-15-06.doc\", \"ENCODE/Data release\",\n",
    "        \"ENCODE/Abstracts\", \"ENCODE/Presentations\", \"ENCODE/Scaling\", \"ENCODE/Meeting\", \"ENCODE/MS2\",\n",
    "        \"ENCODE/WorkingGroups\", \"ENCODE/Documents\", \"ENCODE/criteria\", \"ENCODE/Web_site\", \"ENCODE/Hox.doc\", \"ENCODE/Policy\"],\n",
    "        \"modENCODE\": [\"ENCODE/modENCODE\", \"modENCODE\"],\n",
    "        \"HapMap\":[\n",
    " 'Haplotype Map Project'],\n",
    "     \"HGP\": [\n",
    "         \"Large scale sequence/human sequence\", \"Celera\", \"HGP History Summer 2011\", \"sequencingrampupfiles\"],\n",
    "    \"sequence\": [\"Large scale sequence/Box026-010.pdf\", \"Sequence target files\"],\n",
    "    \"ELSI\": [\"ELSI\"]\n",
    "}\n",
    "\n",
    "list_of_entities = []\n",
    "\n",
    "\n",
    "for i, row in tqdm(together.iterrows(), total = together.shape[0]):\n",
    "    temp = []\n",
    "    for group, (folder) in enumerate(project_folders):\n",
    "        \n",
    "        \n",
    "        if any([Path(\"/tiramisu/\"+ subfolder) in Path(row['path']).parents for subfolder in project_folders[folder]]):\n",
    "            \n",
    "            list_of_entities.append((True, folder, row['docID'], row['path']))\n",
    "        elif any([Path(\"/tiramisu/\"+ subfolder) == Path(row['path']) for subfolder in project_folders[folder]]):\n",
    "            list_of_entities.append((True, folder, row['docID'], row['path']))\n",
    "        else:\n",
    "            list_of_entities.append((False, folder, row['docID'], row['path']))\n",
    "projects_df = pd.DataFrame(list_of_entities, columns = [\"entity\", \"text\", 'docID', 'path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d577535e-0f0e-4838-ad21-ecf3fb451f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgp = projects_df.loc[(projects_df.text == \"HGP\") & (projects_df.entity)].docID.unique()\n",
    "hapmap = projects_df.loc[(projects_df.text == \"HapMap\") & (projects_df.entity)].docID.unique()\n",
    "lsac = projects_df.loc[(projects_df.text == \"sequence\") & (projects_df.entity)].docID.unique()\n",
    "encode = projects_df.loc[(projects_df.text == \"ENCODE\") & (projects_df.entity)].docID.unique()\n",
    "modencode = projects_df.loc[(projects_df.text == \"modENCODE\") & (projects_df.entity)].docID.unique()\n",
    "elsi = projects_df.loc[(projects_df.text == \"ELSI\") & (projects_df.entity)].docID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d456f37d-0abc-4c4c-8f5c-34bd5fdcf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df = pd.read_parquet(\"../cache/all_email_pairs_240520.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b17348",
   "metadata": {},
   "source": [
    "# which emails fall under which projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b72772-5eb7-4eb1-875e-99936336d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df[\"hgp\"] = network_df['documentID'].progress_apply(lambda x: x in hgp)\n",
    "network_df[\"hapmap\"] = network_df['documentID'].progress_apply(lambda x: x in hapmap)\n",
    "network_df[\"lsac\"] = network_df['documentID'].progress_apply(lambda x: x in lsac)\n",
    "network_df[\"encode\"] = network_df['documentID'].progress_apply(lambda x: x in encode)\n",
    "network_df[\"modencode\"] = network_df['documentID'].progress_apply(lambda x: x in modencode)\n",
    "network_df['elsi'] = network_df['documentID'].progress_apply(lambda x: x in elsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3152e7fa-ae5b-4d17-9441-6a898bd8306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['year'] = network_df['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430a489f-2db8-4fa1-ae61-7a11f6302265",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(network_df, source = 'From', target = \"To\", edge_attr = \\\n",
    "                    [ 'hgp', 'hapmap', 'lsac'], create_using=nx.MultiDiGraph())\n",
    "G.remove_edges_from(list(nx.selfloop_edges(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e14a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d333557e-b0ef-4218-ae87-ea4bb5133ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = pd.read_csv('../models/email_clean_manual/nodes_240520_not_randomized.csv')\n",
    "category_dict = node_list.set_index(\"ID\")['category'].to_dict()\n",
    "out_degrees_map = dict([(n, d) for n, d in G.out_degree()])\n",
    "degrees_map = dict([(n, d) for n, d in G.in_degree()])\n",
    "nih_map = {}\n",
    "\n",
    "for n in G.nodes():\n",
    "    if category_dict[int(n)] == \"nih\":\n",
    "        nih_map[n] = '1'\n",
    "    elif category_dict[int(n)] == \"private-nonprofit\" or  category_dict[int(n)] == \"academic\":\n",
    "        nih_map[n] = \"2\"\n",
    "    else:\n",
    "        nih_map[n] = \"3\"\n",
    "# deg_central= pd.DataFrame([g for g in G.nodes()], columns = [\"node\"])\n",
    "# deg_central['indegrees'] = deg_central['node'].map(degrees_map)\n",
    "# deg_central['outdegrees'] = deg_central['node'].map(out_degrees_map)\n",
    "# deg_central['affiliation'] = deg_central['node'].map(nih_map)\n",
    "# deg_central['indegrees'] = deg_central['indegrees'].astype(int)\n",
    "# deg_central['outdegrees'] = deg_central['outdegrees'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edd747",
   "metadata": {},
   "source": [
    "### venn diagram of Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8880ae39-238b-4cdc-a67e-0be1c3d106be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing_nodes = set()\n",
    "hapmap_nodes = set()\n",
    "lsac_nodes = set()\n",
    "for i, row in network_df.iterrows():\n",
    "    if row['hgp']:\n",
    "        sequencing_nodes.add(row['From'])\n",
    "        sequencing_nodes.add(row['To'])\n",
    "    if row['hapmap']:\n",
    "        hapmap_nodes.add(row['From'])\n",
    "        hapmap_nodes.add(row['To'])\n",
    "    if row['lsac'] :\n",
    "        lsac_nodes.add(row['From'])\n",
    "        lsac_nodes.add(row['To'])\n",
    "        \n",
    "\n",
    "a_two = set([i for i in sequencing_nodes if int(i) in category_dict and (category_dict[int(i)] == \"nih\")])\n",
    "# b_two = set([i for i in lsac_nodes if  int(i) in category_dict and (category_dict[int(i)] == \"nih\")])\n",
    "c_two = set([i for i in hapmap_nodes if  int(i) in category_dict and (category_dict[int(i)] == \"nih\")])\n",
    "\n",
    "\n",
    "\n",
    "second_set = a_two.union(c_two)\n",
    "total_second = len(second_set)\n",
    "\n",
    "a = set([i for i in sequencing_nodes if int(i) in category_dict and (category_dict[int(i)] == \"private-nonprofit\" or  category_dict[int(i)] == \"academic\")])\n",
    "# b = set([i for i in lsac_nodes if int(i) in category_dict and (category_dict[int(i)] == \"private-nonprofit\" or  category_dict[int(i)] == \"academic\")])\n",
    "c = set([i for i in hapmap_nodes if int(i) in category_dict and (category_dict[int(i)] == \"private-nonprofit\" or  category_dict[int(i)] == \"academic\")])\n",
    "\n",
    "third_set = a.union(c)\n",
    "total_third = len(third_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d2d7e-7f6d-4209-b634-453a1091127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "lsaccolor = \"green\"\n",
    "hapmapcolor = \"#f1cc32\"\n",
    "sequencingcolor = \"gray\"\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('font', serif='Helvetica') \n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 30,\n",
    "\"xtick.labelsize\": 30,\n",
    "\"ytick.labelsize\": 30,\n",
    "\"legend.fontsize\": 20,\n",
    "\"font.size\":50})\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(40,40), dpi = 300)\\\n",
    "#                             gridspec_kw=dict(width_ratios=norm))\n",
    "axes[0].spines['right'].set_linewidth(0)\n",
    "axes[0].spines['top'].set_linewidth(0)\n",
    "\n",
    "venntwo = venn2([a_two, c_two], ('HGP', 'HapMap'), ax = axes[0],\n",
    "                 subset_label_formatter=lambda x: f\"{x} ({(x/total_second):1.0%})\")\n",
    "# #hapmap\n",
    "\n",
    "venntwo.get_patch_by_id('01').set_color(hapmapcolor)\n",
    "venntwo.get_patch_by_id('10').set_color('gray')\n",
    "venntwo.get_patch_by_id('01').set_edgecolor('black')\n",
    "venntwo.get_patch_by_id('10').set_edgecolor('black')\n",
    "venntwo.get_patch_by_id('11').set_edgecolor('black')\n",
    "venntwo.get_patch_by_id('01').set_alpha(0.5)\n",
    "venntwo.get_patch_by_id('10').set_alpha(0.5)\n",
    "venntwo.get_patch_by_id('11').set_alpha(0.5)\n",
    "\n",
    "\n",
    "vennthree = venn2([a, c], ('HGP',  'HapMap'), ax = axes[1],\n",
    "                 subset_label_formatter=lambda x: f\"{x} ({(x/total_third):1.0%})\")\n",
    "\n",
    "vennthree.get_patch_by_id('01').set_color(hapmapcolor)\n",
    "vennthree.get_patch_by_id('10').set_color('gray')\n",
    "vennthree.get_patch_by_id('01').set_edgecolor('black')\n",
    "vennthree.get_patch_by_id('10').set_edgecolor('black')\n",
    "vennthree.get_patch_by_id('11').set_edgecolor('black')\n",
    "vennthree.get_patch_by_id('01').set_alpha(0.5)\n",
    "vennthree.get_patch_by_id('10').set_alpha(0.5)\n",
    "vennthree.get_patch_by_id('11').set_alpha(0.5)\n",
    "\n",
    "plt.savefig('../figures/figure_3_venndiagrams.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f47e4-9c62-4a88-972a-744dfe7e5c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d367c39c-a3bb-4c3d-bcf0-335ff6917f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d89c57-ae03-47b5-be17-87afa4f2e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_exact([[54, (31 + 29)], [41, (50 + 100)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf8e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c35eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05522ca3-e126-4d2e-9ed9-cb49b40d0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df.groupby([\"From_category\", \"To_category\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2156a93-c72b-4c5d-ac5d-39e043fc4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "16235/network_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "(5493 + 2520)/network_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1862 + 1535)/network_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a49a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaea5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52871e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e84fafd3-7561-48ca-b518-ecf31d379dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://bbengfort.github.io/2016/06/graph-tool-from-networkx/\n",
    "def get_prop_type(value, key=None):\n",
    "    \"\"\"\n",
    "    Performs typing and value conversion for the graph_tool PropertyMap class.\n",
    "    If a key is provided, it also ensures the key is in a format that can be\n",
    "    used with the PropertyMap. Returns a tuple, (type name, value, key)\n",
    "    \"\"\"\n",
    "    if isinstance(key, str):\n",
    "        # Encode the key as ASCII\n",
    "        key = key\n",
    "\n",
    "    # Deal with the value\n",
    "    if isinstance(value, bool):\n",
    "        tname = 'bool'\n",
    "\n",
    "    elif isinstance(value, int):\n",
    "        tname = 'float'\n",
    "        value = float(value)\n",
    "\n",
    "    elif isinstance(value, float):\n",
    "        tname = 'float'\n",
    "\n",
    "\n",
    "    elif isinstance(value, dict):\n",
    "        tname = 'object'\n",
    "\n",
    "    else:\n",
    "        tname = 'string'\n",
    "        value = str(value)\n",
    "\n",
    "    return tname, value, key\n",
    "\n",
    "\n",
    "def nx2gt(nxG):\n",
    "    \"\"\"\n",
    "    Converts a networkx graph to a graph-tool graph.\n",
    "    \"\"\"\n",
    "    # Phase 0: Create a directed or undirected graph-tool Graph\n",
    "    gtG = graph_tool.Graph(directed=nxG.is_directed())\n",
    "\n",
    "    # Add the Graph properties as \"internal properties\"\n",
    "    for key, value in nxG.graph.items():\n",
    "        # Convert the value and key into a type for graph-tool\n",
    "        tname, value, key = get_prop_type(value, key)\n",
    "\n",
    "        prop = gtG.new_graph_property(tname) # Create the PropertyMap\n",
    "        gtG.graph_properties[key] = prop     # Set the PropertyMap\n",
    "        gtG.graph_properties[key] = value    # Set the actual value\n",
    "\n",
    "    # Phase 1: Add the vertex and edge property maps\n",
    "    # Go through all nodes and edges and add seen properties\n",
    "    # Add the node properties first\n",
    "    nprops = set() # cache keys to only add properties once\n",
    "    for node, data in nxG.nodes(data=True):\n",
    "\n",
    "        # Go through all the properties if not seen and add them.\n",
    "        for key, val in data.items():\n",
    "            \n",
    "            if key in nprops: continue # Skip properties already added\n",
    "\n",
    "            # Convert the value and key into a type for graph-tool\n",
    "            tname, _, key  = get_prop_type(val, key)\n",
    "\n",
    "            prop = gtG.new_vertex_property(tname) # Create the PropertyMap\n",
    "            gtG.vertex_properties[key] = prop     # Set the PropertyMap\n",
    "\n",
    "            # Add the key to the already seen properties\n",
    "            nprops.add(key)\n",
    "\n",
    "    # Also add the node id: in NetworkX a node can be any hashable type, but\n",
    "    # in graph-tool node are defined as indices. So we capture any strings\n",
    "    # in a special PropertyMap called 'id' -- modify as needed!\n",
    "    gtG.vertex_properties['id'] = gtG.new_vertex_property('string')\n",
    "\n",
    "    # Add the edge properties second\n",
    "    eprops = set() # cache keys to only add properties once\n",
    "    for src, dst, data in nxG.edges(data=True):\n",
    "\n",
    "        # Go through all the edge properties if not seen and add them.\n",
    "        for key, val in data.items():\n",
    "            if key in eprops: continue # Skip properties already added\n",
    "\n",
    "            # Convert the value and key into a type for graph-tool\n",
    "            tname, _, key = get_prop_type(val, key)\n",
    "\n",
    "            prop = gtG.new_edge_property(tname) # Create the PropertyMap\n",
    "            gtG.edge_properties[key] = prop     # Set the PropertyMap\n",
    "\n",
    "            # Add the key to the already seen properties\n",
    "            eprops.add(key)\n",
    "\n",
    "    # Phase 2: Actually add all the nodes and vertices with their properties\n",
    "    # Add the nodes\n",
    "    vertices = {} # vertex mapping for tracking edges later\n",
    "    for node, data in nxG.nodes(data=True):\n",
    "\n",
    "        # Create the vertex and annotate for our edges later\n",
    "        v = gtG.add_vertex()\n",
    "        vertices[node] = v\n",
    "\n",
    "        # Set the vertex properties, not forgetting the id property\n",
    "        data['id'] = str(node)\n",
    "        for key, value in data.items():\n",
    "            gtG.vp[key][v] = value # vp is short for vertex_properties\n",
    "\n",
    "    # Add the edges\n",
    "    for src, dst, data in nxG.edges(data=True):\n",
    "\n",
    "        # Look up the vertex structs from our vertices mapping and add edge.\n",
    "        e = gtG.add_edge(vertices[src], vertices[dst])\n",
    "\n",
    "        # Add the edge properties\n",
    "        for key, value in data.items():\n",
    "            gtG.ep[key][e] = value # ep is short for edge_properties\n",
    "\n",
    "    # Done, finally!\n",
    "    return gtG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79369fd-b909-4bbb-9ba9-c785cf8d8079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e25f80-cafe-44d9-bd9d-c02aab692eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a92789-7370-48af-b5aa-24613cd13acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477606b-4848-4fe3-8b80-8904a37a5d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e8f9a-6055-428c-a720-a61063e88a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a4b2d-0a46-497c-82ef-a1f023b2668b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa869586-6ab2-4982-890a-6d50155b5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "network_df[\"hgp\"] = network_df['documentID'].progress_apply(lambda x: x in hgp)\n",
    "network_df[\"hapmap\"] = network_df['documentID'].progress_apply(lambda x: x in hapmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f489f917-18da-4f4c-8dbb-0993bc976bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63cc1877-dcbe-4d44-81e4-1538a0fc53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)\n",
    "temp = network_df.loc[network_df.hapmap].groupby('tuple').count().reset_index()[[\"tuple\", \"conversation\"]]\n",
    "temp.columns = [\"tuple\", \"weight\"]\n",
    "temp = temp.loc[temp.weight >0]\n",
    "temp['To'] = temp.tuple.apply(lambda x: x[1])\n",
    "temp['From'] = temp.tuple.apply(lambda x: x[0])\n",
    "G = nx.from_pandas_edgelist(temp, target = 'To', source = \"From\", edge_attr = \\\n",
    "                    [\"weight\"], create_using=nx.DiGraph())\n",
    "G.remove_edges_from(list(nx.selfloop_edges(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f19b96c0-f33c-4b04-9e70-c97b6fd43bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nx2gt(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442903a-d842-4e69-85a3-b1844e27ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kitchen = [129, 224, 258, 260, 440, 488, 503, 505, 590, 67, 74]\n",
    "steering = [8175, 1491, 508, 128, 132, 13462, 14505, 146, 14825, 15958, 188, 19, 19960, 20, 20420, 21759, 22, 22345, 23541, 24, 247, 25, 253, 272, 28, 29182, 294, 30, 3036, 32, 324, 327, 331, 339, 383, 388, 429, 444, 448, 478, 495, 546, 560, 566, 568, 5857, 68, 6903, 7, 76, 7899, 8, 9537, 129, 148, 224, 258, 260, 440, 488, 503, 505, 590, 67, 74]\n",
    "rest_of_hapmap = set([int(i) for i in G.nodes()]) - set(kitchen) - set(steering)\n",
    "\n",
    "len(set(rest_of_hapmap)) + len(set(steering)) + len(set(kitchen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf814c5-7e33-4554-9f5c-dc6898461253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "045db33e-ab9a-4fe8-bdbc-a06389e327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = pd.read_csv('../models/email_clean_manual/nodes_240520_not_randomized.csv')\n",
    "\n",
    "ID_to_gtID = {}\n",
    "\n",
    "for node in G.nodes():\n",
    "    ID_to_gtID[node] = graph_tool.util.find_vertex(network, network.vertex_properties['id'], node)[0]\n",
    "\n",
    "category_dict = node_list.set_index(\"ID\")['category'].to_dict()\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in kitchen:\n",
    "        v_prop[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "    elif int(i) in steering:\n",
    "        v_prop[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 0.5]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 1, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96a553-2cfa-4e01-9083-6683a18f52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = [\"real-exponential\", \"discrete-geometric\", \"discrete-poisson\", \"discrete-binomial\"]\n",
    "# ew = contract_parallel_edges(network)\n",
    "data_hapmap = []\n",
    "\n",
    "for distribution in tqdm(distributions):\n",
    "    for degree_corrected in [False, True]:\n",
    "\n",
    "        state_to_save = None\n",
    "        entropy = np.inf\n",
    "        for i in tqdm(range(10)):\n",
    "            \n",
    "            # state  = minimize_nested_blockmodel_dl(network, state_args=dict(deg_corr=degree_corrected, recs =[network.ep.weight], rec_types = [distribution]))\n",
    "            state = NestedBlockState(network, base_type=RankedBlockState, state_args=dict(eweight=network.ep.weight, deg_corr=degree_corrected, recs =[network.ep.weight], rec_types = [distribution]))\n",
    "            mcmc_anneal(state, beta_range=(1, 10), niter=1000, mcmc_equilibrate_args=dict(force_niter=10))\n",
    "        \n",
    "            if state.entropy() < entropy:\n",
    "                state_to_save = state\n",
    "                entropy = state.entropy()\n",
    "        data_hapmap.append((distribution, degree_corrected, entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a2d09-3d33-4e22-958a-e2b391e6b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hapmap_df = pd.DataFrame(data_hapmap, columns = [\"distribution\", \"degree corrected\", \"entropy\"])\n",
    "data_hapmap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d8927-1e27-42bf-9d12-d59723f42bbb",
   "metadata": {},
   "source": [
    "# find the lowest entropy configuration of HapMap emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4d7c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)\n",
    "temp = network_df.loc[network_df.hapmap].groupby('tuple').count().reset_index()[[\"tuple\", \"conversation\"]]\n",
    "temp.columns = [\"tuple\", \"weight\"]\n",
    "temp = temp.loc[temp.weight >0]\n",
    "temp['To'] = temp.tuple.apply(lambda x: x[1])\n",
    "temp['From'] = temp.tuple.apply(lambda x: x[0])\n",
    "G = nx.from_pandas_edgelist(temp, target = 'To', source = \"From\", edge_attr = \\\n",
    "                    [\"weight\"], create_using=nx.DiGraph())\n",
    "G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
    "\n",
    "network = nx2gt(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803356ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e364f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "322292fb",
   "metadata": {},
   "source": [
    "### finds the lowest entropy state of the exponential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30455c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_save = None\n",
    "entropy = np.inf\n",
    "for i in tqdm(range(20)):\n",
    "    state  = minimize_nested_blockmodel_dl(network, state_args=dict(deg_corr=True, recs =[network.ep.weight], rec_types = [\"real-exponential\"]))\n",
    "    mcmc_anneal(state, beta_range=(1, 10), niter=1000, mcmc_equilibrate_args=dict(force_niter=10))\n",
    "    \n",
    "    if state.entropy() < entropy:\n",
    "        state_to_save = state\n",
    "        entropy = state.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a7934a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_names = network.new_vertex_property(\"string\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in kitchen:\n",
    "        v_prop[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "    elif int(i) in steering:\n",
    "        v_prop[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 0.5]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 1, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 1, 0.5]\n",
    "    v_prop_names[ID_to_gtID[i]] = str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df937d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_save.draw( vertex_fill_color=v_prop, vertex_color=v_prop_edge , vertex_text = v_prop_names,  vertex_font_size=4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acad65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35fe1e64",
   "metadata": {},
   "source": [
    "## determine how many conversations happen between two large communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b3c70197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_to_gtID = {}\n",
    "\n",
    "for node in G.nodes():\n",
    "    ID_to_gtID[node] = graph_tool.util.find_vertex(network, network.vertex_properties['id'], node)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c71b4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_hapmap = [state_to_save.levels[0].get_blocks()[ID_to_gtID[i]] for i in G.nodes()]\n",
    "blocks_hapmap_df = pd.DataFrame({\"blocks\": blocks_hapmap, \"level2\":  [state_to_save.levels[1].get_blocks()[ID_to_gtID[i]] for i in G.nodes()], \"ID\": [i for i in G.nodes()]})\n",
    "blocks_hapmap_df['ID'] = blocks_hapmap_df['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4a84c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kitchen_or_steering_or_hapmap(x):\n",
    "    if x in kitchen:\n",
    "        return \"kitchen\"\n",
    "    elif x in steering:\n",
    "        return \"steering\"\n",
    "blocks_hapmap_df['type'] = blocks_hapmap_df[\"ID\"].apply(lambda x: kitchen_or_steering_or_hapmap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e1f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a62ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_hapmap_df.groupby(\"blocks\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2 + 17 + 22)/(2 + 4 + 17 + 153 + 101 + 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd18452",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_hapmap_df.loc[blocks_hapmap_df['ID'] == 312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af194c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_hapmap_df.loc[blocks_hapmap_df['type'] == \"kitchen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "171d46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "39b0e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = defaultdict(int)\n",
    "\n",
    "for group_name, data in blocks_hapmap_df.groupby(\"blocks\"):\n",
    "    \n",
    "    for group_name_2, data_2 in blocks_hapmap_df.groupby(\"blocks\"):\n",
    "        \n",
    "        for j in data['ID'].unique():\n",
    "            for k in data_2['ID'].unique():\n",
    "                \n",
    "                if  G.get_edge_data(str(j), str(k)) is not None:\n",
    "                    total[(group_name, group_name_2)] += G.get_edge_data(str(j), str(k))['weight']\n",
    "                if  G.get_edge_data(str(k), str(j)) is not None:\n",
    "                    total[(group_name, group_name_2)] += G.get_edge_data(str(k), str(j))['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c3ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fdb35865",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_data = []\n",
    "for node in ID_to_gtID.keys():\n",
    "    level_data.append((state_to_save.levels[0].get_blocks()[ID_to_gtID[node]],  node))\n",
    "level_data = pd.DataFrame(level_data, columns = [\"block\",  \"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57d23dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_data = pd.DataFrame(level_data, columns = [\"block\", \"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c385b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby(\"blocks\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93302fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[merged['type'] == \"kitchen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1e69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6686fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c9b730b",
   "metadata": {},
   "source": [
    "### now find the lowest state for HGP emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)\n",
    "temp = network_df.loc[network_df.hgp].groupby('tuple').count().reset_index()[[\"tuple\", \"conversation\"]]\n",
    "temp.columns = [\"tuple\", \"weight\"]\n",
    "temp = temp.loc[temp.weight >0]\n",
    "temp['To'] = temp.tuple.apply(lambda x: x[1])\n",
    "temp['From'] = temp.tuple.apply(lambda x: x[0])\n",
    "G_hgp = nx.from_pandas_edgelist(temp, target = 'To', source = \"From\", edge_attr = \\\n",
    "                    [\"weight\"], create_using=nx.DiGraph())\n",
    "G_hgp.remove_edges_from(list(nx.selfloop_edges(G_hgp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state  = minimize_nested_blockmodel_dl(network, state_args=dict(deg_corr=True, recs =[network.ep.weight], rec_types = ['real-exponential']))\n",
    "mcmc_equilibrate(state, wait=1000, mcmc_args=dict(niter=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.draw( vertex_fill_color=v_prop, vertex_color=v_prop_edge )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce31e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7f390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df955848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b08df0-f1b1-4810-84bf-5a10dba5d706",
   "metadata": {},
   "source": [
    "# create sankey diagram of HGP to HapMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07afb1b-2ae3-4ae8-a67a-945a32d25e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)\n",
    "temp = network_df.loc[network_df.hgp].groupby('tuple').count().reset_index()[[\"tuple\", \"conversation\"]]\n",
    "temp.columns = [\"tuple\", \"weight\"]\n",
    "temp = temp.loc[temp.weight >0]\n",
    "temp['To'] = temp.tuple.apply(lambda x: x[1])\n",
    "temp['From'] = temp.tuple.apply(lambda x: x[0])\n",
    "G_hgp = nx.from_pandas_edgelist(temp, target = 'To', source = \"From\", edge_attr = \\\n",
    "                    [\"weight\"], create_using=nx.DiGraph())\n",
    "G_hgp.remove_edges_from(list(nx.selfloop_edges(G_hgp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e060bf9-2f9a-4acf-9530-6f10c9788bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_hgp_df = [int(i) for i in G_hgp.nodes()]\n",
    "\n",
    "blocks_hgp_df = pd.DataFrame({\"ID\": blocks_hgp_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a50715-f56e-41a5-8860-7563097828f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_hgp = []\n",
    "in_hgp = []\n",
    "\n",
    "rest_of_hapmap_in_hgp = []\n",
    "rest_of_hapmap_not_in_hgp = []\n",
    "for i in kitchen:\n",
    "    if i not in blocks_hgp_df['ID'].to_list():\n",
    "        not_in_hgp.append(i)\n",
    "    else:\n",
    "        in_hgp.append(i)\n",
    "for i in steering:\n",
    "    if i not in blocks_hgp_df[\"ID\"].to_list():\n",
    "        not_in_hgp.append(i)\n",
    "    else:\n",
    "        in_hgp.append(i)\n",
    "for i in rest_of_hapmap:\n",
    "    if i not in blocks_hgp_df[\"ID\"].to_list():\n",
    "        not_in_hgp.append(i)\n",
    "        rest_of_hapmap_not_in_hgp.append(i)\n",
    "    else:\n",
    "        in_hgp.append(i)\n",
    "        rest_of_hapmap_in_hgp.append(i)\n",
    "not_in_hgp = list(set(not_in_hgp))\n",
    "in_hgp = list(set(in_hgp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbebb2-b9c3-4fcf-bffb-c75962bc98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgp_sankey = pd.concat([pd.DataFrame({\"ID\": in_hgp, \"rank\": \"In HGP\", \"name\": [IDs_inverted[i] for i in in_hgp], \"block\": -2}),\n",
    "           pd.DataFrame({\"ID\": not_in_hgp, \"rank\": \"Not in HGP\", \"name\": [IDs_inverted[i] for i in not_in_hgp], \n",
    "                         \"block\": -1})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ac89b-1828-4580-9e39-7c69d196bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(hgp_sankey, blocks_hapmap_df, on = \"ID\", how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef18ff9-e946-4af7-90c7-da693b9d4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kitchen_or_steering_or_hapmap(x):\n",
    "    if x in kitchen:\n",
    "        return \"kitchen\"\n",
    "    elif x in steering:\n",
    "        return \"steering\"\n",
    "    elif x in rest_of_hapmap_in_hgp:\n",
    "        return \"hapmap_in_hgp\"\n",
    "    elif x in rest_of_hapmap_not_in_hgp:\n",
    "        return \"hapmap_not_in_hgp\"\n",
    "merged['type'] = merged[\"ID\"].apply(lambda x: kitchen_or_steering_or_hapmap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fb63c-0e20-4322-9ee4-1c34b2317691",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['blocks'] = merged['blocks'].map({ 13: 2,188:1, 23:4, 190:3, 76:5,  136:6,})\n",
    "committee_by_type = Partition.Simple('type', ['kitchen', 'steering', \"hapmap_in_hgp\", \"hapmap_not_in_hgp\"])\n",
    "\n",
    "palette = {'kitchen': '#41521f', 'steering': '#e27982', \"hapmap_in_hgp\": \"#00ffff\", \"hapmap_not_in_hgp\": \"gray\"}\n",
    "flows = (\n",
    "    merged.groupby([\"rank\", \"blocks\", \"type\"])\n",
    "    .agg({\"ID\": \"count\"})\n",
    "    .dropna()\n",
    "    .reset_index()\n",
    "    .sort_values([\"rank\", \"blocks\"], ascending = [True, False])\n",
    ")\n",
    "\n",
    "\n",
    "nodes = {\n",
    "    \"rank\": ProcessGroup(flows[\"rank\"].unique().tolist()),\n",
    "    \"blocks\": ProcessGroup(flows[\"blocks\"].unique().tolist()),\n",
    "}\n",
    "\n",
    "embark_port = Partition.Simple(\"process\", flows[\"rank\"].unique().tolist())\n",
    "disembark_port = Partition.Simple(\"process\", flows[\"blocks\"].unique().tolist())\n",
    "\n",
    "flows = (\n",
    "    flows.rename(\n",
    "        columns={\n",
    "            \"rank\": \"source\",\n",
    "            \"blocks\": \"target\",\n",
    "            \"ID\": \"value\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "ordering = [[\"rank\"], [\"waypoint\"], [\"blocks\"]]\n",
    "\n",
    "nodes[\"rank\"].partition = embark_port\n",
    "nodes[\"blocks\"].partition = disembark_port\n",
    "nodes['waypoint'] = Waypoint(committee_by_type)\n",
    "bundles = [Bundle(\"rank\", \"blocks\",  waypoints = [\"waypoint\"])]\n",
    "sdd = SankeyDefinition(nodes, bundles, ordering, flow_partition=committee_by_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Sankey diagram\n",
    "weave(\n",
    "    sdd, flows, palette=palette\n",
    ").to_widget().auto_save_svg('sankey_sbm_with_distinction.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71211d9-f893-4e17-a1f4-6a5e67a18f0f",
   "metadata": {},
   "source": [
    "# visualize by count, membership, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bee43b-6736-4e12-8b22-50e2bcddfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in kitchen:\n",
    "        v_prop[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "    elif int(i) in steering:\n",
    "        v_prop[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 0.5]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 1, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 1, 0.5]\n",
    "state.draw(vertex_fill_color=v_prop, vertex_color=v_prop_edge, beta = 0.8, hedge_pen_width= 2,\n",
    "hvertex_fill_color= np.array([0., 0., 0., .5]),\n",
    "hedge_color= np.array([0., 0., 0., .5]),\n",
    "hedge_marker_size= 15,\n",
    "hvertex_size=15 , output = \"../models/sbm/hapmap_communities_by_committee.pdf\")\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "counter = Counter(np.vstack([network_df.loc[network_df.hapmap]['To'], network_df.loc[network_df.hapmap]['From']]).flatten())\n",
    "\n",
    "min_val, max_val = counter.most_common()[-1][-1], counter.most_common()[0][-1]\n",
    "\n",
    "# use the coolwarm colormap that is built-in, and goes from blue to red\n",
    "cmap = mpl.cm.coolwarm\n",
    "norm = mpl.colors.Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "# convert your distances to color coordinates\n",
    "for i in ID_to_gtID:\n",
    "    v_prop[ID_to_gtID[i]] = cmap(counter[i])\n",
    "    v_prop_edge[ID_to_gtID[i]] =  cmap(counter[i])\n",
    "  \n",
    "state.draw(vertex_fill_color=v_prop, vertex_color=v_prop_edge, beta = 0.8, hedge_pen_width= 2,\n",
    "    hvertex_fill_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_marker_size= 15,\n",
    "    hvertex_size=15, output = \"../models/sbm/hapmap_communities_by_count.pdf\")\n",
    "\n",
    "# state = minimize_nested_blockmodel_dl(network)\n",
    "\n",
    "category_dict = node_list.set_index(\"ID\")['category'].to_dict()\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in category_dict:\n",
    "        if category_dict[int(i)] == \"nih\":\n",
    "            v_prop[ID_to_gtID[i]] = [0, 0, 1, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [0, 0, 1, 1]\n",
    "        elif category_dict[int(i)] == \"private-nonprofit\" or  category_dict[int(i)] == \"academic\":\n",
    "            v_prop[ID_to_gtID[i]] = [1, 0, 0, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [1, 0, 0, 1]\n",
    "        else:\n",
    "            v_prop[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "state.draw(vertex_fill_color=v_prop, vertex_color=v_prop_edge, beta = 0.9, hedge_pen_width= 2,\n",
    "    hvertex_fill_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_marker_size= 15,\n",
    "    hvertex_size=15, output = \"../models/sbm/hapmap_communities_by_nih.pdf\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ffb1e-a7ee-4d93-9cef-cb481aa78783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "744b8e95-d612-4770-8628-bb98a1f773d2",
   "metadata": {},
   "source": [
    "# brokerage role analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c88ca4-8c56-4961-b68b-eb758e76ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module implements the disparity filter to compute a significance score of edge weights in networks\n",
    "\n",
    "taken from https://github.com/aekpalakorn/python-backbone-network\n",
    "'''\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "\n",
    "\n",
    "def disparity_filter(G, weight='weight'):\n",
    "    ''' Compute significance scores (alpha) for weighted edges in G as defined in Serrano et al. 2009\n",
    "        Args\n",
    "            G: Weighted NetworkX graph\n",
    "        Returns\n",
    "            Weighted graph with a significance score (alpha) assigned to each edge\n",
    "        References\n",
    "            M. A. Serrano et al. (2009) Extracting the Multiscale backbone of complex weighted networks. PNAS, 106:16, pp. 6483-6488.\n",
    "    '''\n",
    "    \n",
    "    if nx.is_directed(G): #directed case    \n",
    "        N = nx.DiGraph()\n",
    "        for u in G:\n",
    "            \n",
    "            k_out = G.out_degree(u)\n",
    "            k_in = G.in_degree(u)\n",
    "            \n",
    "            if k_out > 1:\n",
    "                sum_w_out = sum(np.absolute(G[u][v][weight]) for v in G.successors(u))\n",
    "                for v in G.successors(u):\n",
    "                    w = G[u][v][weight]\n",
    "                    p_ij_out = float(np.absolute(w))/sum_w_out\n",
    "                    alpha_ij_out = 1 - (k_out-1) * integrate.quad(lambda x: (1-x)**(k_out-2), 0, p_ij_out)[0]\n",
    "                    N.add_edge(u, v, weight = w, alpha_out=float('%.4f' % alpha_ij_out))\n",
    "                    \n",
    "            elif k_out == 1 and G.in_degree(list(G.successors(u))[0]) == 1:\n",
    "                #we need to keep the connection as it is the only way to maintain the connectivity of the network\n",
    "                print(list(G.successors(u)))\n",
    "                v = list(G.successors(u))[0]\n",
    "                w = G[u][v][weight]\n",
    "                N.add_edge(u, v, weight = w, alpha_out=0., alpha_in=0.)\n",
    "                #there is no need to do the same for the k_in, since the link is built already from the tail\n",
    "            \n",
    "            if k_in > 1:\n",
    "                sum_w_in = sum(np.absolute(G[v][u][weight]) for v in G.predecessors(u))\n",
    "                for v in G.predecessors(u):\n",
    "                    w = G[v][u][weight]\n",
    "                    p_ij_in = float(np.absolute(w))/sum_w_in\n",
    "                    alpha_ij_in = 1 - (k_in-1) * integrate.quad(lambda x: (1-x)**(k_in-2), 0, p_ij_in)[0]\n",
    "                    N.add_edge(v, u, weight = w, alpha_in=float('%.4f' % alpha_ij_in))\n",
    "        return N\n",
    "    \n",
    "    else: #undirected case\n",
    "        B = nx.Graph()\n",
    "        for u in G:\n",
    "            k = len(G[u])\n",
    "            if k > 1:\n",
    "                sum_w = sum(np.absolute(G[u][v][weight]) for v in G[u])\n",
    "                for v in G[u]:\n",
    "                    w = G[u][v][weight]\n",
    "                    p_ij = float(np.absolute(w))/sum_w\n",
    "                    alpha_ij = 1 - (k-1) * integrate.quad(lambda x: (1-x)**(k-2), 0, p_ij)[0]\n",
    "                    B.add_edge(u, v, weight = w, alpha=float('%.4f' % alpha_ij))\n",
    "        return B\n",
    "\n",
    "def disparity_filter_alpha_cut(G,weight='weight',alpha_t=0.4, cut_mode='or'):\n",
    "    ''' Performs a cut of the graph previously filtered through the disparity_filter function.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        G: Weighted NetworkX graph\n",
    "        \n",
    "        weight: string (default='weight')\n",
    "            Key for edge data used as the edge weight w_ij.\n",
    "            \n",
    "        alpha_t: double (default='0.4')\n",
    "            The threshold for the alpha parameter that is used to select the surviving edges.\n",
    "            It has to be a number between 0 and 1.\n",
    "            \n",
    "        cut_mode: string (default='or')\n",
    "            Possible strings: 'or', 'and'.\n",
    "            It works only for directed graphs. It represents the logic operation to filter out edges\n",
    "            that do not pass the threshold value, combining the alpha_in and alpha_out attributes\n",
    "            resulting from the disparity_filter function.\n",
    "            \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        B: Weighted NetworkX graph\n",
    "            The resulting graph contains only edges that survived from the filtering with the alpha_t threshold\n",
    "    \n",
    "        References\n",
    "        ---------\n",
    "        .. M. A. Serrano et al. (2009) Extracting the Multiscale backbone of complex weighted networks. PNAS, 106:16, pp. 6483-6488.\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    if nx.is_directed(G):#Directed case:   \n",
    "        B = nx.DiGraph()\n",
    "        for u, v, w in G.edges(data=True):\n",
    "            try:\n",
    "                alpha_in =  w['alpha_in']\n",
    "            except KeyError: #there is no alpha_in, so we assign 1. It will never pass the cut\n",
    "                alpha_in = 1\n",
    "            try:\n",
    "                alpha_out =  w['alpha_out']\n",
    "            except KeyError: #there is no alpha_out, so we assign 1. It will never pass the cut\n",
    "                alpha_out = 1  \n",
    "            \n",
    "            if cut_mode == 'or':\n",
    "                if alpha_in<alpha_t or alpha_out<alpha_t:\n",
    "                    B.add_edge(u,v, weight=w[weight])\n",
    "            elif cut_mode == 'and':\n",
    "                if alpha_in<alpha_t and alpha_out<alpha_t:\n",
    "                    B.add_edge(u,v, weight=w[weight])\n",
    "        return B\n",
    "\n",
    "    else:\n",
    "        B = nx.Graph()#Undirected case:   \n",
    "        for u, v, w in G.edges(data=True):\n",
    "            \n",
    "            try:\n",
    "                alpha = w['alpha']\n",
    "            except KeyError: #there is no alpha, so we assign 1. It will never pass the cut\n",
    "                alpha = 1\n",
    "                \n",
    "            if alpha<alpha_t:\n",
    "                B.add_edge(u,v, weight=w[weight])\n",
    "        return B           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d51b83-6a39-4eb5-a360-6ec7f01ec77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Alex Levenson\n",
    "alex@isnotinvain.com\t| www.isnotinvain.com\n",
    "(c) Reya Group \t\t\t| http://www.reyagroup.com\n",
    "Friday July 23rd 2010\n",
    "\n",
    "Calculates brokerage roles, as described by Steven Borgatti in http://www.analytictech.com/essex/Lectures/Brokerage.pdf\n",
    "\"\"\"\n",
    "\n",
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "class _RoleClassifier(object):\n",
    "\troleTypes = { \\\n",
    "\t\t\t\t \"coordinator\"\t\t: lambda pred,broker,succ: pred == broker == succ, \\\n",
    "\t\t\t\t \"gatekeeper\" \t \t: lambda pred,broker,succ: pred != broker == succ, \\\n",
    "\t\t\t\t \"representative\"\t: lambda pred,broker,succ: pred == broker != succ, \\\n",
    "\t\t\t\t \"consultant\"\t\t: lambda pred,broker,succ: pred == succ != broker, \\\n",
    "\t\t\t\t \"liaison\"\t\t\t: lambda pred,broker,succ: pred != succ and pred != broker and broker != succ, \\\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\n",
    "\t@classmethod\n",
    "\tdef classify(cls,predecessor_group,broker_group,successor_group):\n",
    "\t\tfor role,predicate in cls.roleTypes.items():\n",
    "\t\t\tif predicate(predecessor_group,broker_group,successor_group):\n",
    "\t\t\t\treturn role\n",
    "\t\traise Exception(\"Could not classify... this should never happen\")\n",
    "\t\n",
    "def getBrokerageRoles(graph,partition):\n",
    "\t\"\"\"\n",
    "\tCounts how many times each node in graph acts as one of the five brokerage roles described by Steven Borgatti in\n",
    "\thttp://www.analytictech.com/essex/Lectures/Brokerage.pdf\n",
    "\t\n",
    "\tgraph: a networx DiGraph\n",
    "\tpartition: a dictionary mapping node -> group, must map every node. If a node has no group associate then put it by itself in a new group\n",
    "\t\n",
    "\treturns: {node -> {\"cooridnator\": n, \"gatekeeper\": n, \"representative\": n, \"consultant\": n, \"liaison\": n}} where n is the number of times\n",
    "\tnode acted as that role\n",
    "\t\"\"\"\n",
    "\t\n",
    "\troleClassifier = _RoleClassifier()\n",
    "\t\n",
    "\troles = dict((node, dict((role,0) for role in roleClassifier.roleTypes)) for node in graph)\n",
    "\tfor node in graph:\n",
    "\t\t\n",
    "\t\tfor successor in graph.successors(node):\n",
    "\t\t\tfor predecessor in graph.predecessors(node):\n",
    "\t\t\t\t\n",
    "\t\t\t\tif successor == predecessor or successor == node or predecessor == node: continue\n",
    "\n",
    "\t\t\t\tif node == \"575\":\n",
    "\t\t\t\t\tprint(predecessor, node, successor)\n",
    "\t\t\t\tif not (graph.has_edge(predecessor, successor)):\n",
    "\t\t\t\t\t# found a broker!\n",
    "\t\t\t\t\t# now which kind depends on who is in which group\n",
    "\t\t\t\t\troles[node][roleClassifier.classify(partition[predecessor],partition[node],partition[successor])] += 1\n",
    "\treturn roles\n",
    "\n",
    "\n",
    "def get_brokerage(G, partition):\n",
    "\n",
    "    brokers = getBrokerageRoles(G, partition)\n",
    "    \n",
    "    totalbrokers = pd.DataFrame([(j, sum(brokers[j].values())) for j in brokers], columns = ['id', 'sum'])\n",
    "    totalbrokers['coordinator'] = [i['coordinator'] for i in brokers.values()] \n",
    "    totalbrokers['gatekeeper'] = [i['gatekeeper'] for i in brokers.values()]\n",
    "    totalbrokers['representative'] = [i['representative'] for i in brokers.values()]\n",
    "    totalbrokers['consultant'] = [i['consultant'] for i in brokers.values()]\n",
    "    totalbrokers['liaison'] = [i['liaison'] for i in brokers.values()]\n",
    "    \n",
    "    \n",
    "    totalbrokers['coordinator'] = totalbrokers['coordinator'] / totalbrokers['sum']\n",
    "    totalbrokers['gatekeeper'] = totalbrokers['gatekeeper'] / totalbrokers['sum']\n",
    "    totalbrokers['representative'] =totalbrokers['representative'] / totalbrokers['sum']\n",
    "    totalbrokers['consultant'] = totalbrokers['consultant'] / totalbrokers['sum']\n",
    "    totalbrokers['liaison'] = totalbrokers['liaison'] / totalbrokers['sum']\n",
    "    totalbrokers['committee'] = totalbrokers['id'].map(partition)\n",
    "\n",
    "    melted = totalbrokers.loc[totalbrokers['sum'] > 10].melt(id_vars = ['id',  'committee'], value_vars = ['coordinator', 'representative', 'liaison', 'gatekeeper', 'consultant'])\n",
    "    return melted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84fd38-60ea-40fe-a5c6-b3c75cf60d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)\n",
    "temp = network_df.loc[network_df.hapmap].groupby('tuple').count().reset_index()[[\"tuple\", \"conversation\"]]\n",
    "temp.columns = [\"tuple\", \"weight\"]\n",
    "temp = temp.loc[temp.weight >0]\n",
    "temp['To'] = temp.tuple.apply(lambda x: x[1])\n",
    "temp['From'] = temp.tuple.apply(lambda x: x[0])\n",
    "G = nx.from_pandas_edgelist(temp, target = 'To', source = \"From\", edge_attr = \\\n",
    "                    [\"weight\"], create_using=nx.DiGraph())\n",
    "G.remove_edges_from(list(nx.selfloop_edges(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e72b5d-0725-4a4a-8576-e2fcaab09663",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {}\n",
    "for g in G.nodes():\n",
    "    # partition[g] = category_dict[int(g)]\n",
    "    if category_dict[int(g)] == \"nih\":\n",
    "        partition[g] = 'NIH'\n",
    "    elif category_dict[int(g)] == \"academic\" or category_dict[int(g)] == \"private-nonprofit\":\n",
    "        partition[g] = 'external academia'\n",
    "    else:\n",
    "        partition[g] = 'other'\n",
    "\n",
    "committee_partition = {}\n",
    "for g in G.nodes():\n",
    "    if int(g) in kitchen:\n",
    "        committee_partition[g] = \"kitchen\"\n",
    "    elif int(g) in steering:\n",
    "        committee_partition[g] = \"steering\"\n",
    "    else:\n",
    "        committee_partition[g] = \"rest of hapmap\"\n",
    "\n",
    "test_partition = {}\n",
    "\n",
    "for g in G.nodes():\n",
    "    if int(g) in steering:\n",
    "        test_partition[g] = \"steering & kitchen cabinet\"\n",
    "    else:\n",
    "        test_partition[g] = \"rest of hapmap\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e191e7a-fb93-4492-8ad8-657ef5ee0464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39882dc3-f9a4-4b4f-8790-0d1b8be38625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in tqdm(np.arange(0, 1.1, 0.1)):\n",
    "    transformed = disparity_filter(G)\n",
    "    transformed = disparity_filter_alpha_cut(transformed, alpha_t = alpha, cut_mode=\"or\")\n",
    "    print(\"alpha: \", alpha)\n",
    "    print(\"nodes: \", len(transformed.nodes()))\n",
    "    print(\"edges: \", len(transformed.edges()))\n",
    "\n",
    "    melted = get_brokerage(transformed)\n",
    "\n",
    "    print(melted.groupby([\"committee\", \"variable\"]).count())\n",
    "    sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "    })\n",
    "    \n",
    "    sns.color_palette(\"Set1\")\n",
    "    \n",
    "    matplotlib.rc('font', family='Helvetica') \n",
    "    matplotlib.rc('pdf', fonttype=42)\n",
    "    matplotlib.rc('text', usetex='false') \n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    matplotlib.rcParams['xtick.major.size'] = 2\n",
    "    matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "    matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "    matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "    \n",
    "    matplotlib.rcParams['ytick.major.size'] = 2\n",
    "    matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "    matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "    matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "    \n",
    "    \n",
    "    matplotlib.rcParams.update({\"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"font.size\":7})\n",
    "    figure, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(2,2), dpi = 300)\n",
    "    PROPS = {\n",
    "        'boxprops':{ 'edgecolor':'k'},\n",
    "        'medianprops':{'color':'k'},\n",
    "        'whiskerprops':{'color':'k'},\n",
    "        'capprops':{'color':'k'},\n",
    "        'flierprops': {'markersize': 2, 'markeredgewidth': 0.5}}\n",
    "    \n",
    "    sns.stripplot(melted.sort_values(['committee', 'variable'], key = lambda x: x.map({\"kitchen\": 0, \"steering\": 1, \"rest of hapmap\": 2, \"consultant\":3,\n",
    "                                                                              \"coordinator\": 4, \"gatekeeper\": 5, \"liaison\": 6, \"representative\": 7})), x = 'variable', y = 'value', \n",
    "                palette = [\"#41521F\", \"#e27982\",\"#00FFFF\"  ], hue = 'committee', ax = axes, size = 3, dodge = True, alpha = 0.5)\n",
    "    sns.boxplot(melted.sort_values(['committee', 'variable'], key = lambda x: x.map({\"kitchen\": 0, \"steering\": 1, \"rest of hapmap\": 2, \"consultant\":3,\n",
    "                                                                              \"coordinator\": 4, \"gatekeeper\": 5, \"liaison\": 6, \"representative\": 7})), x = 'variable', y = 'value', \n",
    "                palette = [\"#41521F\", \"#e27982\",\"#00FFFF\"  ], hue = 'committee', ax = axes, linewidth =0.5,showfliers=False, **PROPS)\n",
    "    axes.set_xlabel(\"\")\n",
    "    axes.set_ylabel(\"Brokerage roles\", color = \"k\")\n",
    "    # axes.legend(handles=[mpatches.Patch(color=\"#41521F\", label='Kitchen Cabinet'),\n",
    "    #                      mpatches.Patch(color=\"#e27982\", label='Steering Committee'),\n",
    "    #                      mpatches.Patch(color=\"#00FFFF\", label='Rest of HapMap')\n",
    "    #                     ], frameon = False,  bbox_to_anchor = (1.4, .6))\n",
    "    \n",
    "    axes.legend().remove()\n",
    "    for i, line in enumerate(axes.get_lines()):\n",
    "        line.set_color('k')\n",
    "    \n",
    "    axes.tick_params(axis='x', colors='black')\n",
    "    axes.set_xticklabels(axes.get_xticklabels(), rotation=45, ha='right')\n",
    "    axes.yaxis.label.set_color('black')\n",
    "    axes.xaxis.label.set_color('black')\n",
    "    axes.tick_params(axis='y', colors='black')\n",
    "    axes.spines['bottom'].set_linewidth(0.5)\n",
    "    axes.spines['left'].set_linewidth(0.5)\n",
    "    \n",
    "    sns.despine()\n",
    "    plt.savefig(f\"../figures/triad_analysis_by_committee_{alpha}.pdf\", dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b5825-9b88-47fd-9578-0c89bd8f3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = get_brokerage(G, committee_partition)\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 10,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "figure, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(2,2), dpi = 300)\n",
    "\n",
    "PROPS = {\n",
    "    'boxprops':{ 'edgecolor':'k'},\n",
    "    'medianprops':{'color':'k'},\n",
    "    'whiskerprops':{'color':'k'},\n",
    "    'capprops':{'color':'k'},\n",
    "    'flierprops': {'markersize': 2, 'markeredgewidth': 0.5}}\n",
    "\n",
    "sns.stripplot(melted.sort_values(['committee', 'variable'], key = lambda x: x.map({\"kitchen\": 0, \"steering\": 1, \"rest of hapmap\": 2, \"consultant\":3,\n",
    "                                                                          \"coordinator\": 4, \"gatekeeper\": 5, \"liaison\": 6, \"representative\": 7})), x = 'variable', y = 'value', \n",
    "            palette = [\"#41521F\", \"#e27982\",\"#00FFFF\"  ], hue = 'committee', ax = axes, size = 3, dodge = True, alpha = 0.5)\n",
    "sns.boxplot(melted.sort_values(['committee', 'variable'], key = lambda x: x.map({\"kitchen\": 0, \"steering\": 1, \"rest of hapmap\": 2, \"consultant\":3,\n",
    "                                                                          \"coordinator\": 4, \"gatekeeper\": 5, \"liaison\": 6, \"representative\": 7})), x = 'variable', y = 'value', \n",
    "            palette = [\"#41521F\", \"#e27982\",\"#00FFFF\"  ], hue = 'committee', ax = axes, linewidth =0.5,showfliers=False, **PROPS)\n",
    "axes.set_xlabel(\"\")\n",
    "axes.set_ylabel(\"Brokerage roles\", color = \"k\")\n",
    "# axes.legend(handles=[mpatches.Patch(color=\"#41521F\", label='Kitchen Cabinet'),\n",
    "#                      mpatches.Patch(color=\"#e27982\", label='Steering Committee'),\n",
    "#                      mpatches.Patch(color=\"#00FFFF\", label='Rest of HapMap')\n",
    "#                     ], frameon = False,  bbox_to_anchor = (1.4, .6))\n",
    "\n",
    "axes.legend().remove()\n",
    "for i, line in enumerate(axes.get_lines()):\n",
    "    line.set_color('k')\n",
    "\n",
    "axes.tick_params(axis='x', colors='black')\n",
    "axes.set_xticklabels(axes.get_xticklabels(), rotation=45, ha='right')\n",
    "axes.yaxis.label.set_color('black')\n",
    "axes.xaxis.label.set_color('black')\n",
    "axes.tick_params(axis='y', colors='black')\n",
    "axes.spines['bottom'].set_linewidth(0.5)\n",
    "axes.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "sns.despine()\n",
    "plt.savefig(\"../figures/triad_analysis_by_committee.pdf\", dpi = 300, bbox_inches = \"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddea3d-e2a1-4c32-ab31-8ebf7b0bfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mannwhitneyu(melted.loc[(melted.variable == \"consultant\") & (melted.committee == \"kitchen\")]['value'],\n",
    "             melted.loc[(melted.variable == \"consultant\") & (melted.committee == \"steering\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"consultant\") & (melted.committee == \"steering\")]['value'],\n",
    "             melted.loc[(melted.variable == \"consultant\") & (melted.committee == \"rest of hapmap\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"coordinator\") & (melted.committee == \"kitchen\")]['value'],\n",
    "             melted.loc[(melted.variable == \"coordinator\") & (melted.committee == \"steering\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"coordinator\") & (melted.committee == \"steering\")]['value'],\n",
    "             melted.loc[(melted.variable == \"coordinator\") & (melted.committee == \"rest of hapmap\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"gatekeeper\") & (melted.committee == \"kitchen\")]['value'],\n",
    "             melted.loc[(melted.variable == \"gatekeeper\") & (melted.committee == \"steering\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"gatekeeper\") & (melted.committee == \"steering\")]['value'],\n",
    "             melted.loc[(melted.variable == \"gatekeeper\") & (melted.committee == \"rest of hapmap\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"liaison\") & (melted.committee == \"kitchen\")]['value'],\n",
    "             melted.loc[(melted.variable == \"liaison\") & (melted.committee == \"steering\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"liaison\") & (melted.committee == \"steering\")]['value'],\n",
    "             melted.loc[(melted.variable == \"liaison\") & (melted.committee == \"rest of hapmap\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"representative\") & (melted.committee == \"kitchen\")]['value'],\n",
    "             melted.loc[(melted.variable == \"representative\") & (melted.committee == \"steering\")]['value']))\n",
    "\n",
    "print(mannwhitneyu(melted.loc[(melted.variable == \"representative\") & (melted.committee == \"steering\")]['value'],\n",
    "             melted.loc[(melted.variable == \"representative\") & (melted.committee == \"rest of hapmap\")]['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387af5a3-71fc-450d-a95c-739e4144a9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b73edc4-2fa2-4a4f-9c58-6f5d0a63d0e3",
   "metadata": {},
   "source": [
    "# do the same for HGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb55c7-912c-4f39-bdb3-70556448e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df['tuple'] = network_df.apply(lambda x: (x['From'], x['To']), axis = 1)\n",
    "temp = network_df.loc[network_df.hgp].groupby('tuple').count().reset_index()[[\"tuple\", \"conversation\"]]\n",
    "temp.columns = [\"tuple\", \"weight\"]\n",
    "temp = temp.loc[temp.weight >0]\n",
    "temp['To'] = temp.tuple.apply(lambda x: x[1])\n",
    "temp['From'] = temp.tuple.apply(lambda x: x[0])\n",
    "G = nx.from_pandas_edgelist(temp, target = 'To', source = \"From\", edge_attr = \\\n",
    "                    [\"weight\"], create_using=nx.DiGraph())\n",
    "G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
    "\n",
    "\n",
    "network = nx2gt(G)\n",
    "\n",
    "\n",
    "node_list = pd.read_csv('../models/email_clean_manual/nodes_all_240417.csv')\n",
    "\n",
    "ID_to_gtID = {}\n",
    "\n",
    "for node in G.nodes():\n",
    "    ID_to_gtID[node] = graph_tool.util.find_vertex(network, network.vertex_properties['id'], node)[0]\n",
    "\n",
    "category_dict = node_list.set_index(\"ID\")['category'].to_dict()\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in category_dict:\n",
    "        if category_dict[int(i)] == \"nih\":\n",
    "            v_prop[ID_to_gtID[i]] = [0, 0, 1, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [0, 0, 1, 1]\n",
    "        elif category_dict[int(i)] == \"private-nonprofit\" or  category_dict[int(i)] == \"academic\":\n",
    "            v_prop[ID_to_gtID[i]] = [1, 0, 0, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [1, 0, 0, 1]\n",
    "        else:\n",
    "            v_prop[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "\n",
    "\n",
    "    \n",
    "state_hgp  = minimize_nested_blockmodel_dl(network, state_args=dict(deg_corr=True, recs =[network.ep.weight], rec_types = ['real-exponential']))\n",
    "mcmc_anneal(state_hgp, beta_range=(1, 10), niter=1000, mcmc_equilibrate_args=dict(force_niter=10))\n",
    "\n",
    "# improve solution with merge-split\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in kitchen:\n",
    "        v_prop[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [65/255, 82/255, 31/255, 1]\n",
    "    elif int(i) in steering:\n",
    "        v_prop[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [226/255, 121/255, 130/255, 0.5]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 1, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 1, 0.5]\n",
    "\n",
    "state_to_save.draw(vertex_fill_color=v_prop, vertex_color=v_prop_edge, beta = 0.8, hedge_pen_width= 2,\n",
    "hvertex_fill_color= np.array([0., 0., 0., .5]),\n",
    "hedge_color= np.array([0., 0., 0., .5]),\n",
    "hedge_marker_size= 15,\n",
    "hvertex_size=15 , output = \"../models/sbm/hgp_communities_by_committee.pdf\")\n",
    "\n",
    "\n",
    "counter = Counter(np.vstack([network_df.loc[network_df.hgp]['To'], network_df.loc[network_df.hgp]['From']]).flatten())\n",
    "\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "\n",
    "min_val, max_val = counter.most_common()[-1][-1], counter.most_common()[0][-1]\n",
    "print(min_val, max_val)\n",
    "# use the coolwarm colormap that is built-in, and goes from blue to red\n",
    "cmap = mpl.cm.coolwarm\n",
    "norm = mpl.colors.Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "# convert your distances to color coordinates\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in kitchen:\n",
    "        v_prop[ID_to_gtID[i]] = cmap(counter[i])\n",
    "        v_prop_edge[ID_to_gtID[i]] =  cmap(counter[i])\n",
    "    elif int(i) in steering:\n",
    "        v_prop[ID_to_gtID[i]] =  cmap(counter[i])\n",
    "        v_prop_edge[ID_to_gtID[i]] =  cmap(counter[i])\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] =  cmap(counter[i])\n",
    "        v_prop_edge[ID_to_gtID[i]] =  cmap(counter[i])\n",
    "v_prop_label = network.new_vertex_property(\"string\")\n",
    "for i in kitchen:\n",
    "    if str(i) in ID_to_gtID:\n",
    "        v_prop_label[ID_to_gtID[str(i)]] = str(i)\n",
    "for i in steering:\n",
    "    if str(i) in ID_to_gtID:\n",
    "        v_prop_label[ID_to_gtID[str(i)]] = str(i)\n",
    "state_to_save.draw(vertex_fill_color=v_prop, vertex_color=v_prop_edge, beta = 0.8, hedge_pen_width= 2,\n",
    "    hvertex_fill_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_marker_size= 15,\n",
    "    hvertex_size=15, output = \"../models/sbm/hgp_communities_by_count.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "v_prop = network.new_vertex_property(\"vector<double>\")\n",
    "v_prop_edge = network.new_vertex_property(\"vector<double>\")\n",
    "for i in ID_to_gtID:\n",
    "    if int(i) in category_dict:\n",
    "        if category_dict[int(i)] == \"nih\":\n",
    "            v_prop[ID_to_gtID[i]] = [0, 0, 1, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [0, 0, 1, 1]\n",
    "        elif category_dict[int(i)] == \"private-nonprofit\" or  category_dict[int(i)] == \"academic\":\n",
    "            v_prop[ID_to_gtID[i]] = [1, 0, 0, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [1, 0, 0, 1]\n",
    "        else:\n",
    "            v_prop[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "            v_prop_edge[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "    else:\n",
    "        v_prop[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "        v_prop_edge[ID_to_gtID[i]] = [0, 1, 0, 1]\n",
    "state_to_save.draw(vertex_fill_color=v_prop, vertex_color=v_prop_edge, beta = 0.9, hedge_pen_width= 2,\n",
    "    hvertex_fill_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_color= np.array([0., 0., 0., .5]),\n",
    "    hedge_marker_size= 20,\n",
    "    hvertex_size=20, output = \"../models/sbm/hgp_communities_by_nih.pdf\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c55d8-f934-4ea7-89d5-b927a791d21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d1519-46eb-4efe-9c3e-a386282864e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08841f57-ea1b-4206-8052-5e9360c15f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf41b3-09ea-437e-b144-481610b1d4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
