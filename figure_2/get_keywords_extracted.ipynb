{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "3d047e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./gustav/src\")\n",
    "from gustav import ncbi, nlm\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from utils_tiramisu import *\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# this is the same TIRAMISU_PATH as shown in start_here.ipynb\n",
    "TIRAMISU_PATH = \n",
    "\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9accbdb",
   "metadata": {},
   "source": [
    "# load biomedical mesh information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981a858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = nlm.mesh('descriptor')\n",
    "mesh = mesh.loc[(mesh.qualifier == \"MH\") | (mesh.qualifier == \"ENTRY\")]\n",
    "mesh['value'] = mesh['value'].apply(lambda x: x.split(\",\")[1].strip() + \" \" + x.split(\",\")[0].strip() if len(x.split(\",\")) == 2 else x)\n",
    "mesh['value'] = mesh['value'].apply(lambda x: x.lower() + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04560f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nlm.mesh(\"ui2mn\")\n",
    "merged = pd.merge(mesh, tree, left_on = \"UI\", right_on = \"ui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_patterns = []\n",
    "total_mesh = merged.loc[(merged.mn.str.startswith(\"E\")) | (merged.mn.str.startswith(\"G\"))].groupby(\"UI\").agg({\"value\": list}).reset_index()\n",
    "for i, row in tqdm(total_mesh.iterrows(), total = total_mesh.shape[0]):\n",
    "\tmesh_patterns.append(re.compile(\"|\".join(np.array([\"\\\\b\"+ re.escape(i.lower().strip()) + \"\\\\b\" for i in row['value']]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf9e96",
   "metadata": {},
   "source": [
    "`../cache/pdfs_word_excel_powerpoint_010924.parquet` is simply a Pandas DataFrame that contains the combined texts of the scanned/electronic PDFs and MS documents. The columns are `text`, which is the raw text, and `nodeID` which is the nodeIDs of the split single-page PDFs or the MS documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971391f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is the compilation of all of the extracted text\n",
    "together = pd.read_parquet(\n",
    "    \"../cache/pdfs_word_excel_powerpoint_010924.parquet\"\n",
    ")\n",
    "# together = pd.merge(nhgri_text.reset_index(drop = True).reset_index(), nhgri_text_paths, on=\"nodeID\")\n",
    "map_nodeID_to_docID = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:PART_OF] -> (d:Document) \n",
    "where e.fileExtension = 'pdf' \n",
    "return c.nodeID as nodeID, c.page as page, d.nodeID as documentID, e.originalPath as path\n",
    "\"\"\")\n",
    "all_pdfs = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) - [:SPLIT_INTO] -> (c:File) - [:CONVERT_TO] -> (f:File) \n",
    "where e.fileExtension = 'pdf' and f.fileExtension = 'png' \n",
    "return c.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "all_ms = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['doc', 'docx', 'ppt', 'pptx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "folder_structure = pd.concat([all_pdfs, all_ms])\n",
    "\n",
    "\n",
    "map_nodeID_to_page = map_nodeID_to_docID.set_index('nodeID').to_dict()['page']\n",
    "# map_nodeID_to_path = map_nodeID_to_docID.set_index(\"nodeID\").to_dict()['path']\n",
    "map_nodeID_to_docID = map_nodeID_to_docID.set_index('nodeID').to_dict()['documentID']\n",
    "\n",
    "together['docID'] = together['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else x)\n",
    "together['page'] = together['nodeID'].apply(lambda x: map_nodeID_to_page[x] if x in map_nodeID_to_page else 0)\n",
    "together = pd.merge(together, folder_structure, left_on = 'nodeID', right_on = 'nodeID')\n",
    "\n",
    "all_excel = return_from_neo4j(\"\"\"\n",
    "match (n:Folder) - [:CONTAINS] -> (e:File) \n",
    "where e.fileExtension in ['xls', 'xlsx'] \n",
    "return e.nodeID as nodeID, e.originalPath as path, e.fileExtension as fileExtension\n",
    "\"\"\")\n",
    "\n",
    "together = together.loc[~together.nodeID.isin(all_excel['nodeID'].to_list())]\n",
    "\n",
    "together['text'] = together['text'].apply(lambda x: x + \" \")\n",
    "\n",
    "together = together.sort_values(['docID', 'page']).groupby('docID').agg({\"text\": \"sum\", \"path\": set}).reset_index()\n",
    "\n",
    "together['path'] = together['path'].apply(lambda x: list(x)[0])\n",
    "together['text'] = together['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae8095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix = np.zeros((together.shape[0], len(mesh_patterns)))\n",
    "\n",
    "for i, pattern in tqdm(enumerate(mesh_patterns), total = len(mesh_patterns)):\n",
    "\n",
    "\tmatrix[:, i] = np.array([False if pattern.search(row['text']) is None else True for j, row in together.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = pd.DataFrame(matrix, columns = total_mesh[\"UI\"])\n",
    "\n",
    "matrix_df['docID'] = together['docID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79567d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_per_document = []\n",
    "for i, row in tqdm(matrix_df.iterrows(), total = matrix_df.shape[0]):\n",
    "    temp = []\n",
    "    for column in matrix_df.columns:\n",
    "        if row[column] == True:\n",
    "            temp.append(column)\n",
    "    mesh_per_document.append((row['docID'], temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d71c6d",
   "metadata": {},
   "source": [
    "`../../pii_detection/knowledge_base/matched_orgs_240206.parquet` and `../../pii_detection/knowledge_base/matched_identifiers_240220.parquet` is the same Pandas DataFrame that was saved during the entity recognition & disambiguation step in [start_here.ipynb](../start_here.ipynb). In that notebook, it is called `knowledge_base/matched_identifiers_240220.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d1bd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# people and orgs were already detected and disambiguated in earlier steps\n",
    "# now we load the saved information\n",
    "\n",
    "keywords = pd.DataFrame(mesh_per_document, columns = ['docID', 'mesh'])\n",
    "orgs = pd.read_parquet(\"../../pii_detection/knowledge_base/matched_orgs_240206.parquet\")\n",
    "orgs['docID'] = orgs['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else x)\n",
    "people = pd.read_parquet(\"../../pii_detection/knowledge_base/matched_identifiers_240220.parquet\")\n",
    "people['matched'] = people.apply(lambda x: x['input'] if x['matched'] == \"##PERSON##\" else x['matched'], axis = 1)\n",
    "people['docID'] = people['nodeID'].apply(lambda x: map_nodeID_to_docID[x] if x in map_nodeID_to_docID else x)\n",
    "all_merged = pd.merge(pd.merge(orgs.groupby(\"docID\").agg({\"matched\": list}).reset_index(), people.groupby(\"docID\").agg({\"matched\": list}).reset_index(),\\\n",
    "         on ='docID', how = 'outer', suffixes = (\"_org\", \"_people\")), keywords, on = \"docID\", how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d798a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_keywords_for_project(project, type_of_keyword):\n",
    "    docIDs = projects_df.loc[(projects_df.text == project) & (projects_df.entity)].docID.unique()\n",
    "    return all_merged.loc[all_merged.docID.isin(docIDs)][type_of_keyword].explode().to_list()\n",
    "\n",
    "\n",
    "def normalize_keywords(keywords):\n",
    "    total_keywords = []\n",
    "    for i in keywords:\n",
    "        if isinstance(i, str):\n",
    "            total_keywords.append(i.lower().strip())\n",
    "    return total_keywords\n",
    "\n",
    "\n",
    "# all the words in human genome project folder vs ___ project\n",
    "def term_frequency(project1keywords, project2keywords):\n",
    "    project1keywords = normalize_keywords(project1keywords)\n",
    "    project2keywords = normalize_keywords(project2keywords)\n",
    "    \n",
    "    project1counter = Counter(project1keywords)\n",
    "    project2counter = Counter(project2keywords)\n",
    "\n",
    "    all_words = set(project1keywords).union(set(project2keywords))\n",
    "\n",
    "    for word in all_words:\n",
    "        if word not in project1counter:\n",
    "            project1counter[word] = 0\n",
    "        if word not in project2counter:\n",
    "            project2counter[word] = 0\n",
    "    return project1counter, project2counter\n",
    "\n",
    "def contingency_table(project1, project2, type_of_keyword):\n",
    "    # number of word A in corpus1, number of all other words in corpus1\n",
    "    # number of word A in corpus2, number of all other words in corpus2\n",
    "\n",
    "    project1keywords = get_keywords_for_project(project1, type_of_keyword)\n",
    "    project2keywords = get_keywords_for_project(project2, type_of_keyword)\n",
    "\n",
    "    project1counter, project2counter = term_frequency(project1keywords, project2keywords)\n",
    "    \n",
    "    results = []\n",
    "    for word in tqdm(project1counter.keys(), total =len(project1counter.keys())):\n",
    "        numWordCorpus1 = project1counter[word]\n",
    "        numWordCorpus2 = project2counter[word]\n",
    "\n",
    "        numAllOtherCorpus1 = sum(project1counter.values()) - numWordCorpus1\n",
    "        numAllOtherCorpus2 = sum(project2counter.values()) - numWordCorpus2\n",
    "\n",
    "        \n",
    "\n",
    "        odds_ratio, p_value = fisher_exact([[numWordCorpus1,numWordCorpus2], [numAllOtherCorpus1, numAllOtherCorpus2]])\n",
    "        results.append((word, odds_ratio, p_value, numWordCorpus1 + numWordCorpus2))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "161d17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_table_for_power(project1, project2, type_of_keyword):\n",
    "    project1keywords = get_keywords_for_project(project1, type_of_keyword)\n",
    "    project2keywords = get_keywords_for_project(project2, type_of_keyword)\n",
    "\n",
    "    project1counter, project2counter = term_frequency(project1keywords, project2keywords)\n",
    "    \n",
    "    results = []\n",
    "    for word in tqdm(project1counter.keys(), total =len(project1counter.keys())):\n",
    "        numWordCorpus1 = project1counter[word]\n",
    "        numWordCorpus2 = project2counter[word]\n",
    "\n",
    "        p1 = numWordCorpus1 / sum(project1counter.values())\n",
    "        p2 = numWordCorpus2 / sum(project2counter.values())\n",
    "\n",
    "        numAllOtherCorpus1 = sum(project1counter.values()) - numWordCorpus1\n",
    "        numAllOtherCorpus2 = sum(project2counter.values()) - numWordCorpus2\n",
    "\n",
    "        \n",
    "\n",
    "        odds_ratio, p_value = fisher_exact([[numWordCorpus1,numWordCorpus2], [numAllOtherCorpus1, numAllOtherCorpus2]])\n",
    "\n",
    "        results.append((p1, p2, sum(project1counter.values()), sum(project2counter.values()), p_value))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00653272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_folders = {\n",
    "        \"ENCODE\":[\n",
    "            \"ENCODE/Participants\", \"ENCODE/MS\", \"ENCODE/SAP\", \"ENCODE/OC Information\",\n",
    "            \"ENCODE/PressRelease\", \"ENCODE/ENCODE_2004\", \"ENCODE/publications\", \"ENCODE/Drafts\",\n",
    "        \"ENCODE/Data Standards\", \"ENCODE/encode_align_sop.pdf\", \"ENCODE/ENCODE-PublicationGuidelines 3-29-06.doc\",\n",
    "        \"ENCODE/Minutes\", \"ENCODE/CACR\", \"ENCODE/SAP call minutes 3-15-06.doc\", \"ENCODE/Data release\",\n",
    "        \"ENCODE/Abstracts\", \"ENCODE/Presentations\", \"ENCODE/Scaling\", \"ENCODE/Meeting\", \"ENCODE/MS2\",\n",
    "        \"ENCODE/WorkingGroups\", \"ENCODE/Documents\", \"ENCODE/criteria\", \"ENCODE/Web_site\", \"ENCODE/Hox.doc\", \"ENCODE/Policy\"],\n",
    "        \"modENCODE\": [\"ENCODE/modENCODE\", \"modENCODE\"],\n",
    "        \"HapMap\":[\n",
    " 'Haplotype Map Project'],\n",
    "     \"HGP\": [\n",
    "         \"Large scale sequence/human sequence\", \"Celera\", \"HGP History Summer 2011\", \"sequencingrampupfiles\"],\n",
    "    \"sequence\": [\"Large scale sequence/Box026-010.pdf\", \"Sequence target files\"]\n",
    "}\n",
    "\n",
    "list_of_entities = []\n",
    "\n",
    "\n",
    "for i, row in tqdm(together.iterrows(), total = together.shape[0]):\n",
    "    temp = []\n",
    "    for group, (folder) in enumerate(project_folders):\n",
    "        \n",
    "        \n",
    "        if any([Path(\"/tiramisu/\"+ subfolder) in Path(row['path']).parents for subfolder in project_folders[folder]]):\n",
    "            \n",
    "            list_of_entities.append((True, folder, row['docID'], row['path']))\n",
    "        elif any([Path(\"/tiramisu/\"+ subfolder) == Path(row['path']) for subfolder in project_folders[folder]]):\n",
    "            list_of_entities.append((True, folder, row['docID'], row['path']))\n",
    "        else:\n",
    "            list_of_entities.append((False, folder, row['docID'], row['path']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98109ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = pd.DataFrame(list_of_entities, columns = [\"entity\", \"text\", 'docID', 'path'])\n",
    "projects_df.groupby([\"text\", \"entity\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = projects_df.loc[projects_df.entity].groupby('text').agg({\"docID\": set})\n",
    "\n",
    "all_merged['HGP'] = all_merged['docID'].progress_apply(lambda x: True if x \\\n",
    "    in temp.loc['HGP']['docID'] else False)\n",
    "\n",
    "all_merged['HapMap'] = all_merged['docID'].progress_apply(lambda x: True if x \\\n",
    "    in temp.loc['HapMap']['docID'] else False)\n",
    "\n",
    "all_merged['sequence'] = all_merged['docID'].progress_apply(lambda x: True if x \\\n",
    "    in temp.loc['sequence']['docID'] else False)\n",
    "\n",
    "all_merged['ENCODE'] = all_merged['docID'].progress_apply(lambda x: True if x \\\n",
    "    in temp.loc['ENCODE']['docID'] else False)\n",
    "\n",
    "all_merged['modENCODE'] = all_merged['docID'].progress_apply(lambda x: True if x \\\n",
    "    in temp.loc['modENCODE']['docID'] else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19482a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged['matched_org'] = all_merged['matched_org'].fillna(\"\").apply(list)\n",
    "all_merged['matched_people'] = all_merged['matched_people'].fillna(\"\").apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged['total'] = all_merged.progress_apply(lambda x: x['matched_org'] + x['matched_people'] + x['mesh'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree['starting'] = tree['mn'].str[0]\n",
    "E_labels = tree.loc[(tree.starting.isin([\"E\"]))].ui.unique()\n",
    "\n",
    "G_labels = tree.loc[(tree.starting.isin([\"G\"])) & (~tree.starting.isin([\"E\"]))].ui.unique()\n",
    "\n",
    "\n",
    "categories_dict = {}\n",
    "union_people_org = set(all_merged['matched_org'].explode()).intersection(set(all_merged['matched_people'].explode()))\n",
    "for i in tqdm(all_merged['matched_org'].explode().unique()):\n",
    "\n",
    "    if isinstance(i, str):\n",
    "        categories_dict[i.lower()] = \"org\"\n",
    "\n",
    "for i in tqdm(all_merged['matched_people'].explode().unique()):\n",
    "    if isinstance(i, str):\n",
    "        if i in union_people_org:\n",
    "            continue\n",
    "        else:\n",
    "            categories_dict[i.lower()] = \"people\"\n",
    "\n",
    "for i in tqdm(all_merged['mesh'].explode().unique()):\n",
    "    if isinstance(i, str):\n",
    "        \n",
    "        if i in E_labels:\n",
    "            categories_dict[i.lower()] = \"technique\"\n",
    "\n",
    "        if i in G_labels:\n",
    "            categories_dict[i.lower()] = \"phenomena\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eab433",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_combinations = [(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]\n",
    "exploded = all_merged.explode('total')\n",
    "\n",
    "all_terms ={}\n",
    "for combo in tqdm(project_combinations):\n",
    "    results = contingency_table(combo[0], combo[1], 'total')\n",
    "    pvalues = pd.DataFrame(results, columns = ['word', 'ratio', 'pvalue', 'total'])\n",
    "    all_terms[combo] = pvalues['word'].to_frame()\n",
    "    print(alpha_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30694bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_combinations = [(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]\n",
    "contain_df_power = {}\n",
    "exploded = all_merged.explode('total')\n",
    "alphas = []\n",
    "\n",
    "for combo in tqdm(project_combinations):\n",
    "    results = prepare_table_for_power(combo[0], combo[1], 'total')\n",
    "    pvalues = pd.DataFrame(results, columns = ['p1', 'p2', 'n1', 'n2', 'pvalue'])\n",
    "    rejected, p_adjusted, _, alpha_corrected = multipletests(pvalues['pvalue'], method='bonferroni')\n",
    "    alphas.append(alpha_corrected)\n",
    "    \n",
    "    pvalues['rejected'] = rejected\n",
    "    pvalues['adjusted'] = p_adjusted\n",
    "    \n",
    "    contain_df_power[combo] = pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7648bd",
   "metadata": {},
   "source": [
    "We use the following code for power analysis in R."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaaade",
   "metadata": {},
   "source": [
    "```{r}\n",
    "library(arrow)\n",
    "library(statmod)\n",
    "library(pbapply)\n",
    "library(dplyr)\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "d <- data.frame(projects = c(\"modENCODE\", \"ENCODE\", \"HapMap\", \"sequence\"),\n",
    "                alphas =c(1.5383195397347939e-06,\n",
    " 1.579180089697429e-06,\n",
    " 2.284565475646532e-06,\n",
    " 2.231345947875759e-06))\n",
    "for(i in seq_len(nrow(d))) {\n",
    "  df <- read_parquet(paste(\"../../models/power_fishers/\", d[i,]$projects, \".parquet\", sep = \"\"))\n",
    "  power <- pbapply(df, 1, simulate, alpha =d[i,]$alphas)\n",
    "  \n",
    "  powerData <- data.frame(power=power)\n",
    "  allData <- cbind(df, power = powerData$power)\n",
    "  \n",
    "  write_parquet(allData, paste(\"../../models/power_fishers/\", d[i,]$projects, \"_power.parquet\", sep = \"\"))\n",
    "                            \n",
    "  } \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "\n",
    "\n",
    "simulate <- function(x, alpha) {\n",
    "    return(power.fisher.test(x[1],x[2],x[3],x[4], alpha = alpha))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "power <- pbapply(df, 1, simulate)\n",
    "\n",
    "```\n",
    "\n",
    "```{r}\n",
    "\n",
    "power\n",
    "```\n",
    "```{r}\n",
    "powerData <- data.frame(power=power)\n",
    "allData <- cbind(df, power = powerData$power)\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "write_parquet(allData, \"../../models/power_fishers/modENCODE_power.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29d17854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in contain_df_power:\n",
    "    contain_df_power[key].to_parquet(f\"../models/power_fishers/{key[1]}_new.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925ac30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7435158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_powerful_terms = []\n",
    "\n",
    "for project in [\"modENCODE\", \"ENCODE\", \"HapMap\", \"sequence\"]:\n",
    "    power = pd.read_parquet(f\"../models/power_fishers/{project}_power_new.parquet\")\n",
    "    power['terms'] = all_terms[(\"HGP\", project)]\n",
    "    all_powerful_terms.extend(power.loc[power.power >= 0.8]['terms'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d518045",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_powerful_terms = [i for i in set(all_powerful_terms) if i != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d3f9d",
   "metadata": {},
   "source": [
    "# SI Figure 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percentage_of_docs_with_at_least_one = []\n",
    "for project in [\"HGP\", \"sequence\", \"HapMap\", \"ENCODE\", \"modENCODE\"]:\n",
    "    temp = []\n",
    "    temp.append(all_merged.loc[(all_merged[project]) & (all_merged.matched_people.str.len() > 0)].shape[0] / all_merged.loc[(all_merged[project])].shape[0])\n",
    "    temp.append(all_merged.loc[(all_merged[project]) & (all_merged.matched_org.str.len() > 0)].shape[0] / all_merged.loc[(all_merged[project])].shape[0])\n",
    "    \n",
    "\n",
    "    techniques_doc = 0\n",
    "    phenomena_doc = 0\n",
    "    for i in all_merged.loc[(all_merged[project])]['mesh'].to_list():\n",
    "\n",
    "        techniques = []\n",
    "        phenomena = []\n",
    "        for j in i:\n",
    "            if categories_dict[j.lower()] == 'techniques':\n",
    "                techniques.append(j.lower())\n",
    "            elif categories_dict[j.lower()] == \"phenomena\":\n",
    "                phenomena.append(j.lower())\n",
    "            else:\n",
    "                pass\n",
    "        if len(phenomena) > 0:\n",
    "            phenomena_doc += 1\n",
    "        elif len(techniques) > 0:\n",
    "            techniques_doc += 1\n",
    "    print(techniques_doc, all_merged.loc[all_merged[project]].shape[0], project)\n",
    "    print(phenomena_doc,all_merged.loc[all_merged[project]].shape[0], project)\n",
    "    temp.append(techniques_doc / all_merged.loc[all_merged[project]].shape[0])\n",
    "    temp.append(phenomena_doc / all_merged.loc[all_merged[project]].shape[0])\n",
    "    temp.append(project)\n",
    "    percentage_of_docs_with_at_least_one.append(temp)\n",
    "at_least_one = pd.DataFrame(percentage_of_docs_with_at_least_one, columns = [\"people\", \"org\", \"techniques\", \"phenomena\", \"project\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2), dpi=300)\n",
    "\n",
    "sns.barplot(data = at_least_one.melt(id_vars = \"project\").sort_values(by=['project', \"variable\"], key=lambda x: x.map({\"HGP\": 0, \"sequence\": 1, \"HapMap\": 2, \"ENCODE\": 3, \"modENCODE\": 4, \"phenomena\" : 5, \"techniques\": 6, \"org\": 7, \"people\":8})), x = \"project\", y = \"value\", hue = \"variable\", ax = ax)\n",
    "ax.legend(frameon = False, bbox_to_anchor = (1, 0.5) )\n",
    "\n",
    "def formatter(x, pos):\n",
    "    del pos\n",
    "    return str(int(x*100))\n",
    "\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "ax.xaxis.label.set_color('black')\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.set_ylabel(\"Documents with at least one entity [%]\")\n",
    "ax.set_xlabel(\"\")\n",
    "sns.despine()\n",
    "\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "plt.savefig(\"../cache/documents_with_at_least_one_entity_240904.pdf\", dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599d2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_combinations = [(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]\n",
    "exploded = all_merged.explode('total')\n",
    "\n",
    "contain_df ={}\n",
    "for combo in tqdm(project_combinations):\n",
    "    results = contingency_table(combo[0], combo[1], 'total')\n",
    "    pvalues = pd.DataFrame(results, columns = ['word', 'ratio', 'pvalue', 'total'])\n",
    "    rejected, p_adjusted, _, alpha_corrected = multipletests(pvalues['pvalue'], method='bonferroni')\n",
    "    \n",
    "    pvalues['rejected'] = rejected\n",
    "    pvalues['adjusted'] = p_adjusted\n",
    "    \n",
    "    pvalues['logratio'] = pvalues['ratio'].apply(lambda x: np.log2(x))\n",
    "    pvalues['category'] = pvalues['word'].map(categories_dict)\n",
    "    pvalues['powerful'] = pvalues['word'].apply(lambda x: True if x in all_powerful_terms else False)\n",
    "    contain_df[combo] = pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274111e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7 * 3,\n",
    "\"xtick.labelsize\": 7 * 3,\n",
    "\"ytick.labelsize\": 7 * 3,\n",
    "\"legend.fontsize\": 5,\n",
    "\"font.size\":7 * 3})\n",
    "\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "\n",
    "\n",
    "layout = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5, 6, 7],\n",
    "    [8, 9, 10, 11],\n",
    "    [12, 13, 14, 15]\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(layout, figsize=(35,35))\n",
    "\n",
    "\n",
    "for i, project in enumerate([\"sequence\", \"HapMap\", \"ENCODE\", \"modENCODE\"]):\n",
    "    temp = contain_df[(\"HGP\", project)]\n",
    "\n",
    "    for j, type in enumerate([\"phenomena\", \"technique\", \"org\", \"people\"]):\n",
    "        sns.scatterplot(data = temp.loc[((temp.rejected) & (temp.powerful)) & (temp.category == type)], x = 'total', y = 'logratio', color = 'red', ax = axes[(4 * i) + j])\n",
    "        sns.scatterplot(data = temp.loc[((~temp.rejected) | (~temp.powerful))  & (temp.category == type)], x = 'total', y = 'logratio', color = 'grey', ax = axes[(4 * i) + j])\n",
    "        axes[(4* i) + j].axhline(0, 0, 5500, c='k')\n",
    "        \n",
    "        axes[(4*i) + j].set_xscale(\"log\")\n",
    "        axes[(4 * i) + j].set_title(project + \"-\" + type + \" N=\" + \"{:.2f}\".format(temp.loc[((temp.logratio == np.inf) | (temp.logratio == -1 * np.inf)) & (temp.category == type)].shape[0] / \n",
    "                                                                       temp.loc[temp.category == type].shape[0]), fontsize = 30)\n",
    "        # axes[(4*i) + j].set\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7 * 4,\n",
    "\"xtick.labelsize\": 7 * 4,\n",
    "\"ytick.labelsize\": 7 * 4,\n",
    "\"legend.fontsize\": 5,\n",
    "\"font.size\":7 *4 })\n",
    "\n",
    "for i, project in enumerate([\"sequence\", \"HapMap\", \"ENCODE\", \"modENCODE\"]):\n",
    "    temp = contain_df[(\"HGP\", project)]\n",
    "\n",
    "    for j, type in enumerate([\"phenomena\", \"technique\", \"org\", \"people\"]):\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize = (7,7), sharex=True, gridspec_kw = {\"height_ratios\": [1, 4, 1]}, dpi = 300)\n",
    "        \n",
    "        fig.subplots_adjust(hspace=0)  # adjust space between axes\n",
    "        \n",
    "        \n",
    "        temp['logratio'] = temp['logratio'].apply(lambda x: 10000 if x == np.inf else x)\n",
    "        temp['logratio'] = temp['logratio'].apply(lambda x: -10000 if x == -1 * np.inf else x)\n",
    "\n",
    "        \n",
    "        sns.scatterplot(data = temp.loc[((temp.rejected) & (temp.powerful)) & (temp.category == type)], x = 'total', y = 'logratio', color = 'red', ax = ax1)\n",
    "        sns.scatterplot(data = temp.loc[(~temp.rejected)  & (temp.category == type)], x = 'total', y = 'logratio', color = 'grey', ax = ax1)\n",
    "        sns.scatterplot(data = temp.loc[(temp.rejected & (temp.powerful)) & (temp.category == type)], x = 'total', y = 'logratio', color = 'red', ax = ax2)\n",
    "        sns.scatterplot(data = temp.loc[(~temp.rejected)  & (temp.category == type)], x = 'total', y = 'logratio', color = 'grey', ax = ax2)\n",
    "        sns.scatterplot(data = temp.loc[(temp.rejected & (temp.powerful)) & (temp.category == type)], x = 'total', y = 'logratio', color = 'red', ax = ax3)\n",
    "        sns.scatterplot(data = temp.loc[(~temp.rejected)  & (temp.category == type)], x = 'total', y = 'logratio', color = 'grey', ax = ax3)\n",
    "        ax2.axhline(0, 0, 10000, c='k')\n",
    "        \n",
    "        ax2.set_xscale(\"log\")\n",
    "        # zoom-in / limit the view to different portions of the data\n",
    "        ax1.set_ylim(9998, 10002)  # outliers only\n",
    "        ax2.set_ylim(-12 ,12)  # most of the data\n",
    "        ax3.set_ylim(-10002, -9998)\n",
    "        # hide the spines between ax and ax2\n",
    "        ax1.spines.bottom.set_visible(False)\n",
    "        ax2.spines.top.set_visible(False)\n",
    "        ax2.spines.bottom.set_visible(False)\n",
    "        ax3.spines.top.set_visible(False)\n",
    "        \n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.set_yticks([])\n",
    "        ax3.set_yticks([])\n",
    "        ax3.set_yticklabels([])\n",
    "        \n",
    "        ax1.tick_params(labeltop=False,which = 'both' )  # don't put tick labels at the top\n",
    "        ax2.tick_params(labeltop=False, which = 'both')\n",
    "        ax2.xaxis.tick_bottom()\n",
    "        \n",
    "        percentage = \" inf:\" + \"{:}%\".format(int((temp.loc[((temp.logratio == 10000) | (temp.logratio == -1 * 10000)) & (temp.category == type) & ((temp.rejected) & (temp.powerful))].shape[0] / \n",
    "                                                                               temp.loc[(temp.category == type)].shape[0]) * 100))\n",
    "                                             \n",
    "        ax1.spines['right'].set_linewidth(0)\n",
    "        ax3.spines['right'].set_linewidth(0)\n",
    "        ax1.spines['top'].set_linewidth(0)\n",
    "        ax3.spines['top'].set_linewidth(0)\n",
    "        ax2.spines['right'].set_linewidth(0)\n",
    "        ax2.spines['top'].set_linewidth(0)\n",
    "        ax1.set_ylabel(\"inf\")\n",
    "        ax2.set_ylabel(\"\")\n",
    "        ax3.set_ylabel(\"-inf\")\n",
    "        \n",
    "        print(f\"{project} - {type} - {percentage}\")\n",
    "        ax1.set_title(\"\")\n",
    "#         ax1.set_title(f\"{project} - {type}\" + percentage)\n",
    "        ax1.set_xlabel(\"\")\n",
    "        ax2.set_xlabel(\"\")\n",
    "        ax3.set_xlabel(\"\")\n",
    "        plt.savefig(f\"../cache/{project}-{type}_new.png\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f353e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3695e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms = []\n",
    "\n",
    "for i in [(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]:\n",
    "    temp = contain_df[i]\n",
    "    all_essential_terms.extend(temp.loc[(temp.rejected) & (temp.powerful)]['word'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e577267",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_essential_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf88e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = all_merged.explode('total')\n",
    "matrix = np.zeros((len(set(all_essential_terms)), 4))\n",
    "exploded = exploded[['docID', \"HGP\", \"HapMap\", \"ENCODE\", \"modENCODE\", \"sequence\", \"total\"]].melt(id_vars = [\"docID\",  \"HGP\", \"HapMap\", \"ENCODE\", \"modENCODE\", \"sequence\"])\n",
    "exploded['lower'] = exploded['value'].str.lower().str.strip()\n",
    "for i, term in tqdm(enumerate(list(set(all_essential_terms))), total = len(set(all_essential_terms))):\n",
    "    temp = exploded.loc[(exploded['lower'] == term)]\n",
    "    HGP = temp.loc[temp['HGP']].docID.unique().shape[0]\n",
    "    if HGP == 0:\n",
    "        HGP = .1\n",
    "\n",
    "    for j, project in enumerate([\"sequence\", \"HapMap\", \"ENCODE\", \"modENCODE\"]):\n",
    "        project_found = temp.loc[(temp[project])].docID.unique().shape[0]\n",
    "\n",
    "        if project_found == 0:\n",
    "            project_found = .1\n",
    "        matrix[i][j] = np.log2(project_found/ HGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577af1f",
   "metadata": {},
   "source": [
    "# Figure 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "196cdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "to_concat = []\n",
    "for i, project in enumerate([(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]):\n",
    "    temp = contain_df[project].groupby(['category']).count()['word'].to_frame()\n",
    "    temp.columns = [\"count\"]\n",
    "\n",
    "    temp =temp.reset_index()\n",
    "    \n",
    "    temp['project'] = project[1]\n",
    "    temp['type'] = 'total'\n",
    "    to_concat.append(temp)\n",
    "    temp = contain_df[project].groupby(['category', \"powerful\", \"rejected\"]).count().reset_index()\n",
    "    temp = temp.loc[temp.powerful & temp.rejected][['category', 'word']]\n",
    "    temp.columns = [\"category\", \"count\"]\n",
    "    temp['type'] = 'rejected'\n",
    "    temp['project'] = project[1]\n",
    "\n",
    "    to_concat.append(temp)\n",
    "total_metadata = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for project in [\"sequence\", \"HapMap\", \"ENCODE\", \"modENCODE\"]:\n",
    "    print(project)\n",
    "    for category in [\"org\", \"people\", \"phenomena\", \"technique\"]:\n",
    "        print(category)\n",
    "        total = total_metadata.loc[(total_metadata.category == category) & (total_metadata['project'] == project)]\n",
    "        \n",
    "        print(total.loc[total['type'] == \"total\"]['count'].iloc[0])\n",
    "        print(total.loc[total['type'] == \"rejected\"]['count'].iloc[0])\n",
    "        metadata.append((project, category, total.loc[total['type'] == \"rejected\"]['count'].iloc[0] \\\n",
    "                         / total.loc[total['type'] == \"total\"]['count'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c0f3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(metadata, columns = [\"project\", \"category\", \"percentage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaed3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 7,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2), dpi=300)\n",
    "\n",
    "sns.barplot(data = metadata.sort_values(by=['project'], key=lambda x: x.map({\"HGP\": 0, \"sequence\": 1, \"HapMap\": 2, \"ENCODE\": 3, \"modENCODE\": 4}))\\\n",
    "            .sort_values(by=['category'], key=lambda x: x.map({\"phenomena\": 0, \"technique\": 1, \"org\": 2, \"people\": 3})), x = 'category', y = 'percentage', hue = 'project', palette = {\"HGP\": \"#6EC3E7\",\\\n",
    "                    \"sequence\":\"#51AF4D\",\"HapMap\": \"#E1BE15\", \"ENCODE\": \"#095393\", \"modENCODE\": \"#AD5D95\"}, ax = ax)\n",
    "ax.spines['right'].set_linewidth(0)\n",
    "ax.spines['top'].set_linewidth(0)\n",
    "ax.legend(frameon = False, bbox_to_anchor = (0.5, 0.5))\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_xticklabels([\"Phenomena\", \"Techniques\", \"Organizations\", \"People\"])\n",
    "def formatter(x, pos):\n",
    "    del pos\n",
    "    return str(int(x*100))\n",
    "\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "ax.xaxis.label.set_color('black')\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.set_ylabel(\"Significant entities [%]\")\n",
    "plt.savefig(\"../cache/figure_2_share_of_entities_new.pdf\", transparent = True, dpi = 400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727a146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d3afe277",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms = []\n",
    "all_categories = []\n",
    "all_projects = []\n",
    "for i in [(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]:\n",
    "    \n",
    "    temp = contain_df[i]\n",
    "    all_essential_terms.extend(temp.loc[(temp.rejected) & (temp.powerful)]['word'].str.lower())\n",
    "    all_categories.extend(temp.loc[(temp.rejected) & (temp.powerful)]['category'].str.lower())\n",
    "    all_projects.extend([i[1]] * temp.loc[(temp.rejected) & (temp.powerful)]['category'].str.lower().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c841bf",
   "metadata": {},
   "source": [
    "# SI Figure 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159028c",
   "metadata": {},
   "outputs": [],
   "source": [
    " exploded.lower.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_essential_terms)) / exploded.lower.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2e534898",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms_df = pd.DataFrame(all_essential_terms, columns = ['term'])\n",
    "all_essential_terms_df['category'] = all_categories\n",
    "all_essential_terms_df['projects'] = all_projects\n",
    "all_essential_terms_df = all_essential_terms_df.loc[all_essential_terms_df.term != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b59b6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_essential_terms_df = all_essential_terms_df.drop_duplicates()[['term', 'category']].value_counts().to_frame(\"occ\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 12,\n",
    "\"ytick.labelsize\": 7,\n",
    "\"legend.fontsize\": 7,\n",
    "\"font.size\":7})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2), dpi=300)\n",
    "\n",
    "\n",
    "sns.barplot(data = all_essential_terms_df.groupby([\"occ\", \"category\"]).count().reset_index().sort_values(by=[\"category\"], key=lambda x: x.map({\"HGP\": 0, \"sequence\": 2, \"HapMap\": 1, \"ENCODE\": 3, \"modENCODE\": 4, \"phenomena\" : 5, \"techniques\": 6, \"org\": 7, \"people\":8})),\n",
    "                 x = 'occ', y = 'term', hue = \"category\")\n",
    "# ax.set_xticklabels([])\n",
    "ax.legend(frameon = False, bbox_to_anchor = (0.85, 0.5) )\n",
    "ax.set_ylabel(\"Keyword frequency\")\n",
    "ax.set_xlabel(\"Number of projects\")\n",
    "\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "sns.despine()\n",
    "plt.savefig(\"../cache/number_of_projects_overlap_keywords.pdf\", bbox_inches = \"tight\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc559a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms_df.groupby([\"occ\", \"category\"]).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms_df.groupby([\"occ\", \"category\"]).count().reset_index().term.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf72dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263eb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exploded = all_merged.explode('total')\n",
    "matrix = np.zeros((len(set(all_essential_terms)), 5))\n",
    "exploded = exploded[['docID', \"HGP\", \"HapMap\", \"ENCODE\", \"modENCODE\", \"sequence\", \"total\"]].melt(id_vars = [\"docID\",  \"HGP\", \"HapMap\", \"ENCODE\", \"modENCODE\", \"sequence\"])\n",
    "exploded['lower'] = exploded['value'].str.lower().str.strip()\n",
    "HGP = len(exploded.loc[(exploded['HGP'])]['docID'].unique())\n",
    "sequence = len(exploded.loc[(exploded['sequence'])]['docID'].unique())\n",
    "hapmap = len(exploded.loc[(exploded['HapMap'])]['docID'].unique())\n",
    "encode = len(exploded.loc[(exploded['ENCODE'])]['docID'].unique())\n",
    "modencode = len(exploded.loc[(exploded['modENCODE'])]['docID'].unique())\n",
    "for i, term in tqdm(enumerate(list(set(all_essential_terms))), total = len(set(all_essential_terms))):\n",
    "    temp = exploded.loc[(exploded['lower'] == term)]\n",
    "    matrix[i][0] = temp.loc[(temp['HGP'])].docID.unique().shape[0] / HGP\n",
    "    matrix[i][1] = temp.loc[(temp['sequence'])].docID.unique().shape[0] / sequence\n",
    "    matrix[i][2] = temp.loc[(temp['HapMap'])].docID.unique().shape[0] / hapmap\n",
    "    matrix[i][3] = temp.loc[(temp['ENCODE'])].docID.unique().shape[0] / encode\n",
    "    matrix[i][4] = temp.loc[(temp['modENCODE'])].docID.unique().shape[0] / modencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "26a50e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp, scipy.cluster.hierarchy as hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "13b7ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = pd.DataFrame(matrix, columns = [\"HGP\", \"sequence\", \"HapMap\", \"ENCODE\", \"modENCODE\"], index = list(set(all_essential_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "42bb6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "\treturn (x-np.mean(x)) / np.std(x)\n",
    "\n",
    "def normalize_array(x):\n",
    "    min_val = min(x)\n",
    "    max_val = max(x)\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    normalized_arr = ((x - min_val) / range_val) * 2 - 1\n",
    "    \n",
    "    return normalized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a7fa469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pal = [\"#6EC3E7\", \"#51AF4D\", \"#72c4a0\", \"#E1BE15\", \"#095393\", \"#AD5D95\", \"#5b5895\",\"#2c8171\",]\n",
    "# pal = sns.color_palette(\"Paired\", 10)\n",
    "pal = [\"#E1BE15\", \"#7F8671\", \"#51AF4D\",   \"#2B5B2A\", \"#050606\", \"#AD5D95\", \"#072D4D\",  \"#095393\",  \"#5B5894\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830a969",
   "metadata": {},
   "source": [
    "# Figure 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 10,\n",
    "\"xtick.labelsize\": 12,\n",
    "\"ytick.labelsize\": 14,\n",
    "\"legend.fontsize\": 14,\n",
    "\"font.size\":7})\n",
    "\n",
    "dendrograms = hc.linkage(matrix_df.apply(zscore, axis = 1), method='ward')\n",
    "\n",
    "# clusters = hc.fcluster(dendrograms, t =4, criterion = 'distance')\n",
    "g = sns.clustermap(matrix_df.apply(zscore, axis = 1), row_linkage = dendrograms, row_cluster = True, col_cluster=False, \\\n",
    "                   yticklabels = False, xticklabels = True, cmap = 'coolwarm', \n",
    "                   row_colors =  [pal[get_color(i)] for i in list(set(all_essential_terms))],\n",
    "                   cbar_kws = dict(orientation=\"vertical\", ticks = [-2, -1, 0, 1, 2]), \\\n",
    "                   tree_kws = {\"linewidth\": 0.5, \"color\":'k'}, center = 0, vmin = -2, vmax = 2, colors_ratio=0.015, figsize=(7,15),)\n",
    "# g.ax_heatmap.tick_params(left=False, bottom=False)\n",
    "g.ax_cbar.set_position((0.12 , 0.1, .02, .08))\n",
    "\n",
    "g.ax_row_dendrogram.set_visible(False)\n",
    "g.ax_col_dendrogram.set_visible(False)\n",
    "# g.ax_row_dendrogram.set_xlim([0,0])\n",
    "# g.ax_col_dendrogram.set_ylim([0,0])\n",
    "# hm = g.ax_heatmap.get_position()\n",
    "# g.ax_heatmap.set_position([hm.x0, hm.y0 + 0.703, hm.width, hm.height * 0.1])\n",
    "hm_row = g.ax_col_dendrogram.get_position()\n",
    "g.ax_col_dendrogram.set_position([hm_row.x0, hm_row.y0, hm_row.width * 0.1, hm_row.height * 0.25])\n",
    "hm = g.ax_row_colors.get_position()\n",
    "g.ax_row_colors.set_position([hm.x0, hm.y0, hm.width, hm.height * 0.25])\n",
    "hm = g.ax_heatmap.get_position()\n",
    "g.ax_heatmap.set_position([hm.x0, hm.y0, hm.width * 0.2, hm.height * 0.25])\n",
    "# hm_row = g.ax_row_dendrogram.get_position()\n",
    "# g.ax_row_dendrogram.set_position([hm_row.x0, hm_row.y0 + 0.693, hm_row.width, hm_row.height * 0.1])\n",
    "\n",
    "# reordered_labels = matrix_df.iloc[g.dendrogram_col.reordered_ind].index.to_list()\n",
    "# use_labels = [\"white house\", \"doe\", \"hugo\", \"francis s collins\", \"james d watson\", \"elke jordan\", \"eric d green\", \"robert h waterston\", \"celera\"]\n",
    "# use_ticks = [reordered_labels.index(label.lower()) + .5 for label in use_labels]\n",
    "\n",
    "# g.ax_heatmap.set(xticks=use_ticks, xticklabels=use_labels)\n",
    "# g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=65, ha = 'right')\n",
    "g.ax_heatmap.xaxis.tick_bottom()\n",
    "# ax.invert_yaxis()\n",
    "g.ax_heatmap.xaxis.label.set_color('black')\n",
    "g.ax_heatmap.set_xticklabels([\"HGP\", \"LSAC\", \"HapMap\", \"ENCODE\", \"modENCODE\"])\n",
    "plt.setp(g.ax_heatmap.xaxis.get_majorticklabels(), rotation=45, fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01152eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_essential_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = hc.fcluster(dendrograms, t =10, criterion = 'distance')\n",
    "terms = pd.DataFrame({\"term\": list(set(all_essential_terms)), \"cluster\": clusters})\n",
    "\n",
    "temp = mesh.loc[mesh.qualifier == \"MH\"]\n",
    "temp['UI'] = temp['UI'].str.lower()\n",
    "terms['mesh_term'] = terms['term'].map(temp.set_index(\"UI\")['value'].to_dict())\n",
    "terms['category'] = terms['term'].map(categories_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5d1db717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(term):\n",
    "    return terms.loc[terms.term == term].iloc[0]['cluster']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43578612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 10,\n",
    "\"xtick.labelsize\": 12,\n",
    "\"ytick.labelsize\": 14,\n",
    "\"legend.fontsize\": 14,\n",
    "\"font.size\":7})\n",
    "\n",
    "dendrograms = hc.linkage(matrix_df.apply(zscore, axis = 1), method='ward')\n",
    "\n",
    "# clusters = hc.fcluster(dendrograms, t =4, criterion = 'distance')\n",
    "g = sns.clustermap(matrix_df.apply(zscore, axis = 1), row_linkage = dendrograms, row_cluster = True, col_cluster=False, \\\n",
    "                   yticklabels = False, xticklabels = True, cmap = 'coolwarm', \n",
    "                   row_colors =  [pal[get_color(i)] for i in list(set(all_essential_terms))],\n",
    "                   cbar_kws = dict(orientation=\"vertical\", ticks = [-2, -1, 0, 1, 2]), \\\n",
    "                   tree_kws = {\"linewidth\": 0.5, \"color\":'k'}, center = 0, vmin = -2, vmax = 2, colors_ratio=0.015, figsize=(7,15),)\n",
    "# g.ax_heatmap.tick_params(left=False, bottom=False)\n",
    "g.ax_cbar.set_position((0.48 , 0.08, .02, .08))\n",
    "\n",
    "g.ax_row_dendrogram.set_visible(True)\n",
    "g.ax_col_dendrogram.set_visible(False)\n",
    "# g.ax_row_dendrogram.set_xlim([0,0])\n",
    "# g.ax_col_dendrogram.set_ylim([0,0])\n",
    "# hm = g.ax_heatmap.get_position()\n",
    "# g.ax_heatmap.set_position([hm.x0, hm.y0 + 0.703, hm.width, hm.height * 0.1])\n",
    "hm_row = g.ax_col_dendrogram.get_position()\n",
    "g.ax_col_dendrogram.set_position([hm_row.x0, hm_row.y0, hm_row.width * 0.1, hm_row.height * 0.25])\n",
    "hm = g.ax_row_colors.get_position()\n",
    "g.ax_row_colors.set_position([hm.x0, hm.y0, hm.width, hm.height * 0.25])\n",
    "hm = g.ax_heatmap.get_position()\n",
    "g.ax_heatmap.set_position([hm.x0, hm.y0, hm.width * 0.2, hm.height * 0.25])\n",
    "hm_row = g.ax_row_dendrogram.get_position()\n",
    "g.ax_row_dendrogram.set_position([hm_row.x0, hm_row.y0, hm_row.width, hm_row.height * 0.25])\n",
    "\n",
    "# reordered_labels = matrix_df.iloc[g.dendrogram_col.reordered_ind].index.to_list()\n",
    "# use_labels = [\"white house\", \"doe\", \"hugo\", \"francis s collins\", \"james d watson\", \"elke jordan\", \"eric d green\", \"robert h waterston\", \"celera\"]\n",
    "# use_ticks = [reordered_labels.index(label.lower()) + .5 for label in use_labels]\n",
    "\n",
    "# g.ax_heatmap.set(xticks=use_ticks, xticklabels=use_labels)\n",
    "# g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=65, ha = 'right')\n",
    "g.ax_heatmap.xaxis.tick_bottom()\n",
    "# ax.invert_yaxis()\n",
    "g.ax_heatmap.xaxis.label.set_color('black')\n",
    "g.ax_heatmap.set_xticklabels([\"HGP\", \"LSAC\", \"HapMap\", \"ENCODE\", \"modENCODE\"])\n",
    "plt.setp(g.ax_heatmap.xaxis.get_majorticklabels(), rotation=45, fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 10,\n",
    "\"xtick.labelsize\": 12,\n",
    "\"ytick.labelsize\": 14,\n",
    "\"legend.fontsize\": 14,\n",
    "\"font.size\":7})\n",
    "\n",
    "dendrograms = hc.linkage(matrix_df.apply(zscore, axis = 1), method='ward')\n",
    "\n",
    "# clusters = hc.fcluster(dendrograms, t =4, criterion = 'distance')\n",
    "g = sns.clustermap(matrix_df.apply(zscore, axis = 1), row_linkage = dendrograms, row_cluster = True, col_cluster=False, \\\n",
    "                   yticklabels = False, xticklabels = True, cmap = 'coolwarm', \n",
    "                   row_colors =  [pal[get_color(i)] for i in list(set(all_essential_terms))],\n",
    "                   cbar_kws = dict(orientation=\"vertical\", ticks = [-2, -1, 0, 1, 2]), \\\n",
    "                   tree_kws = {\"linewidth\": 0.5, \"color\":'k'}, center = 0, vmin = -2, vmax = 2, colors_ratio=0.015, figsize=(7,15),)\n",
    "# g.ax_heatmap.tick_params(left=False, bottom=False)\n",
    "g.ax_cbar.set_position((0.42 , 0.08, .02, .08))\n",
    "\n",
    "g.ax_row_dendrogram.set_visible(True)\n",
    "g.ax_col_dendrogram.set_visible(False)\n",
    "# g.ax_row_dendrogram.set_xlim([0,0])\n",
    "# g.ax_col_dendrogram.set_ylim([0,0])\n",
    "# hm = g.ax_heatmap.get_position()\n",
    "# g.ax_heatmap.set_position([hm.x0, hm.y0 + 0.703, hm.width, hm.height * 0.1])\n",
    "hm_row = g.ax_col_dendrogram.get_position()\n",
    "g.ax_col_dendrogram.set_position([hm_row.x0, hm_row.y0, hm_row.width * 0.1, hm_row.height * 0.25])\n",
    "hm = g.ax_row_colors.get_position()\n",
    "g.ax_row_colors.set_position([hm.x0, hm.y0, hm.width, hm.height * 0.25])\n",
    "hm = g.ax_heatmap.get_position()\n",
    "g.ax_heatmap.set_position([hm.x0, hm.y0, hm.width * 0.2, hm.height * 0.25])\n",
    "hm_row = g.ax_row_dendrogram.get_position()\n",
    "g.ax_row_dendrogram.set_position([hm_row.x0, hm_row.y0, hm_row.width, hm_row.height * 0.25])\n",
    "\n",
    "# reordered_labels = matrix_df.iloc[g.dendrogram_col.reordered_ind].index.to_list()\n",
    "# use_labels = [\"white house\", \"doe\", \"hugo\", \"francis s collins\", \"james d watson\", \"elke jordan\", \"eric d green\", \"robert h waterston\", \"celera\"]\n",
    "# use_ticks = [reordered_labels.index(label.lower()) + .5 for label in use_labels]\n",
    "\n",
    "# g.ax_heatmap.set(xticks=use_ticks, xticklabels=use_labels)\n",
    "# g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), rotation=65, ha = 'right')\n",
    "g.ax_heatmap.xaxis.tick_bottom()\n",
    "# ax.invert_yaxis()\n",
    "g.ax_heatmap.xaxis.label.set_color('black')\n",
    "g.ax_heatmap.set_xticklabels([\"HGP\", \"LSAC\", \"HapMap\", \"ENCODE\", \"modENCODE\"])\n",
    "plt.setp(g.ax_heatmap.xaxis.get_majorticklabels(), rotation=45, fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ecf794f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [\"D023421\", \"D004351\", \"D006790\", \"D010802\", \"D040641\", \"D012098\", \"D049750\", \"D049750\", \"D044822\",\\\n",
    "         \"D016324\", \"D004251\", \"D015183\", \"D017403\",\\\n",
    "         \"D002874\", \"D022202\", \"D002877\", \"D015894\", \\\n",
    "         \"D010641\", \"D014644\", \"D020641\", \"D006239\", \"D005838\", \\\n",
    "         \"D055106\", 'd016680',  \\\n",
    "         \"D000073336\", \"D017421\",\\\n",
    "         \"D050436\", \"D011401\", \"D046228\",\\\n",
    "          \"D015870\", \"D059467\", \\\n",
    "         \"D012399\",\"D053263\", \\\n",
    "         \"D016364\", \"D059646\", \"D003433\", \"D000081246\", \"D016384\"]\n",
    "\n",
    "E_labels = tree.loc[(tree.ui.isin(labels)) & (tree.starting.isin([\"E\"]))].ui.unique()\n",
    "\n",
    "G_labels = tree.loc[(tree.ui.isin(labels)) & (tree.starting.isin([\"G\"])) & (~tree.starting.isin([\"E\"]))].ui.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b18b41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "         \"usda\", \"fgi\", \"nsf\", \"nchgr\", \"genbank\", \"embl\", \"bcm\",\\\n",
    "         \"congress\", \"white house\", \"doe\", \"lawrence liver more\", \"hugo\", \"celera\", \\\n",
    "         \"cshl\", \"chi nese academy of sciences\", \"unesco\", \"whitehead\", \"howard u\",\\\n",
    "         \"the snps consortium\",\"high international hapmap consortium\", \\\n",
    "         \"ashg\",  \\\n",
    "         \"uwash\", \"stanford\", \"national cell culture resource center\", \"nimblegen\", \"niaid\", \"the broad institute\", \"sanger\"]\n",
    "use_labels = [\n",
    "             \"USDA\", \"FGI\", \"NSF\", \"NCHGR\", \"GenBank\", \"EMBL\", \"BCM\",\\\n",
    "             \"Congress\", \"White House\", \"DOE\", \"LLNL\", \"HUGO\", \"Celera\",\\\n",
    "             \"CSHL\", \"Chinese Academy of Sciences\", \"UNESCO\", \"Whitehead\", \"Howard Univ.\", \\\n",
    "             \"SNP Consortium\",  \"HapMap Consortium\",\\\n",
    "             \"ASHG\", \\\n",
    "             \"UWash\", \"Stanford\", \"NCCC\", \"Roche Nimblegen\", \"NIAID\", \"Broad\", \"Sanger\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "eab4cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered = matrix_df.iloc[g.dendrogram_row.reordered_ind].reset_index()['index'].to_frame()\n",
    "\n",
    "orgs =  [(i[0], i[1]) for i in \\\n",
    "    sorted(zip(labels, use_labels, [reordered.loc[reordered['index'].str.lower().str.strip() == label.lower()].index[0] for label in labels]), \\\n",
    "           key = lambda x: x[1])]\n",
    "\n",
    "\n",
    "labels = [  \"j craig venter\",  \"james d watson\", \"harold e varmus\",\\\n",
    "         \"elke jordan\", \"eric s lander\", \\\n",
    "         \"francis s collins\",\\\n",
    "          \"eric d green\"\n",
    "        ]\n",
    "use_labels = [ \"Venter\",  \"Watson\",  \"Varmus\",\\\n",
    "              \"Elke Jordan\", \"Lander\", \\\n",
    "             \"Collins\",\\\n",
    "              \"Green\"\n",
    "             ]\n",
    "\n",
    "people =  [(i[0], i[1]) for i in \\\n",
    "    sorted(zip(labels, use_labels, [reordered.loc[reordered['index'].str.lower().str.strip() == label.lower()].index[0] for label in labels]), \\\n",
    "           key = lambda x: x[1])]\n",
    "\n",
    "E_labels = [i[0] for i in \\\n",
    "    sorted(zip(E_labels, [reordered.loc[reordered['index'].str.lower().str.strip() == label.lower()].index[0] for label in E_labels]), \\\n",
    "           key = lambda x: x[1])]\n",
    "\n",
    "\n",
    "\n",
    "G_labels = [i[0] for i in \\\n",
    "    sorted(zip(G_labels, [reordered.loc[reordered['index'].str.lower().str.strip() == label.lower()].index[0] for label in G_labels]), \\\n",
    "           key = lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, space1, ax2, space2, ax3, space3, ax4, space4) = plt.subplots(8, 1, gridspec_kw={'height_ratios': [1,2, 1, 2, 1,2, 1, 2]}, figsize=(24, 10), dpi = 300)\n",
    "\n",
    "lines = sorted(Counter(clusters).items())\n",
    "cumsum = 0\n",
    "for line in lines:\n",
    "    y = np.zeros(line[1])\n",
    "    x = np.arange(cumsum, cumsum + y.shape[0])\n",
    "    cumsum += line[1]\n",
    "    \n",
    "    ax1.plot(x, y, color = pal[line[0]-1], solid_joinstyle = \"miter\", solid_capstyle = 'butt')\n",
    "    # l1.set_join_capstyle('miter')\n",
    "    ax2.plot(x, y, color = pal[line[0]-1], solid_joinstyle = \"miter\", solid_capstyle = 'butt')\n",
    "    ax3.plot(x, y, color = pal[line[0]-1], solid_joinstyle = \"miter\", solid_capstyle = 'butt')\n",
    "    ax4.plot(x, y, color = pal[line[0]-1], solid_joinstyle = \"miter\", solid_capstyle = 'butt')\n",
    "ax1.spines['right'].set_linewidth(0)\n",
    "ax1.spines['top'].set_linewidth(0)\n",
    "ax1.spines['bottom'].set_linewidth(0)\n",
    "ax1.spines['left'].set_linewidth(0)\n",
    "ax1.get_yaxis().set_ticks([])\n",
    "\n",
    "ax1.spines['bottom'].set_position('zero')\n",
    "\n",
    "\n",
    "space1.spines['right'].set_linewidth(0)\n",
    "space1.spines['top'].set_linewidth(0)\n",
    "space1.spines['bottom'].set_linewidth(0)\n",
    "space1.spines['left'].set_linewidth(0)\n",
    "space1.axis('off')\n",
    "\n",
    "space2.spines['right'].set_linewidth(0)\n",
    "space2.spines['top'].set_linewidth(0)\n",
    "space2.spines['bottom'].set_linewidth(0)\n",
    "space2.spines['left'].set_linewidth(0)\n",
    "space2.axis('off')\n",
    "\n",
    "ax2.spines['right'].set_linewidth(0)\n",
    "ax2.spines['top'].set_linewidth(0)\n",
    "ax2.spines['bottom'].set_linewidth(0)\n",
    "ax2.spines['left'].set_linewidth(0)\n",
    "ax2.get_yaxis().set_ticks([])\n",
    "ax2.spines['bottom'].set_position('zero')\n",
    "\n",
    "space3.spines['right'].set_linewidth(0)\n",
    "space3.spines['top'].set_linewidth(0)\n",
    "space3.spines['bottom'].set_linewidth(0)\n",
    "space3.spines['left'].set_linewidth(0)\n",
    "space3.axis('off')\n",
    "    \n",
    "\n",
    "ax3.spines['right'].set_linewidth(0)\n",
    "ax3.spines['top'].set_linewidth(0)\n",
    "ax3.spines['bottom'].set_linewidth(0)\n",
    "ax3.spines['left'].set_linewidth(0)\n",
    "ax3.get_yaxis().set_ticks([])\n",
    "ax3.spines['bottom'].set_position('zero')\n",
    "\n",
    "\n",
    "space4.spines['right'].set_linewidth(0)\n",
    "space4.spines['top'].set_linewidth(0)\n",
    "space4.spines['bottom'].set_linewidth(0)\n",
    "space4.spines['left'].set_linewidth(0)\n",
    "space4.axis('off')\n",
    "    \n",
    "\n",
    "ax4.spines['right'].set_linewidth(0)\n",
    "ax4.spines['top'].set_linewidth(0)\n",
    "ax4.spines['bottom'].set_linewidth(0)\n",
    "ax4.spines['left'].set_linewidth(0)\n",
    "ax4.get_yaxis().set_ticks([])\n",
    "ax4.spines['bottom'].set_position('zero')\n",
    "\n",
    "\n",
    "temp = mesh.loc[mesh.qualifier == \"MH\"]\n",
    "temp['UI'] = temp['UI'].str.lower()\n",
    "translate_mesh = temp.set_index(\"UI\")['value'].to_dict()\n",
    "\n",
    "# Add labels to specific points\n",
    "# labels = [ \"doe\", \"hugo\", \"white house\", \"celera\", \"lawrence liver more\", \"herac\", \\\n",
    "#            \"international aphid genomics consortium\",  \"cge\", \"fgi\",\\\n",
    "#          \"usda\", \"nachgr\", \"niaid\", \"genbank\", \\\n",
    "#           \"high international hapmap consortium\", \"the snp consortium ltd\",\\\n",
    "#          \"encode\", \"geneva\",\"national cell culture resource center\",\\\n",
    "#          \"cit\"]\n",
    "# use_labels = [\"DOE\", \"HUGO\", \"White House\", \"Celera\", \"LLNL\", \"HERAC\",\\\n",
    "#               \"IAGC\", \"CGE\", \"FGI\",\\\n",
    "#              \"USDA\", \"NACHGR\", \"NIAID\", \"GenBank\",  \\\n",
    "#              \"HapMap consortium\", \"SNP consortium\",\\\n",
    "#              \"ENCODE\", \"GENEVA\", \"NCCC\",\\\n",
    "#              \"NIH CIT\"]\n",
    "\n",
    "\n",
    "\n",
    "reordered = matrix_df.iloc[g.dendrogram_row.reordered_ind].reset_index()['index'].to_frame()\n",
    "\n",
    "\n",
    "ax2.tick_params(axis=\"x\", direction='in', length=7)\n",
    "\n",
    "ax2_bottom = ax2.secondary_xaxis(\"bottom\")\n",
    "ax2_bottom.tick_params(axis=\"x\", direction=\"out\", length=7)\n",
    "ticks = []\n",
    "bottom_ticks = []\n",
    "for i, label in enumerate(orgs):\n",
    "    print(label)\n",
    "    if i%2 == 0:\n",
    "        x = reordered.loc[reordered['index'].str.lower().str.strip() == label[0].lower()]\n",
    "        # ax1.axline((reordered.loc[reordered['index'].str.lower().str.strip() == label].index[0], 0.1), (reordered.loc[reordered['index'].str.lower().str.strip() == label].index[0],0.01) )  \n",
    "        \n",
    "        ax2.text(x.index[0], 0.02, label[1], fontsize=12, ha='left',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='left', verticalalignment='top',color = pal[get_color(x['index'].iloc[0])])\n",
    "        ticks.append(x.index[0])    \n",
    "    else:\n",
    "        x = reordered.loc[reordered['index'].str.lower().str.strip() == label[0].lower()]\n",
    "        # ax1.axline((reordered.loc[reordered['index'].str.lower().str.strip() == label].index[0], -0.1), (reordered.loc[reordered['index'].str.lower().str.strip() == label].index[0], 0.01)) \n",
    "        ax2.text(x.index[0], -0.02, label[1], fontsize=12, ha='right',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='right', verticalalignment='bottom',color = pal[get_color(x['index'].iloc[0])] )\n",
    "        bottom_ticks.append(x.index[0])\n",
    "\n",
    "ax2.set_xticks(ticks,  [])\n",
    "ax2_bottom.set_xticks(bottom_ticks, [])\n",
    "# ax2.xaxis.set_tick_params(length=7)\n",
    "\n",
    "\n",
    "use_labels = [f\"{translate_mesh[i.lower()]}\" for i in E_labels]\n",
    "\n",
    "reordered = matrix_df.iloc[g.dendrogram_row.reordered_ind].reset_index()['index'].to_frame()\n",
    "\n",
    "label_up = True\n",
    "ticks = []\n",
    "for i, label in enumerate(E_labels):\n",
    "    x = reordered.loc[reordered['index'].str.lower().str.strip() == label.lower()]\n",
    "\n",
    "    if label == 'd055106':\n",
    "        use_label = 'GWAS'\n",
    "    else:\n",
    "        use_label = use_labels[i]\n",
    "    if label_up:\n",
    "        ax3.text(x.index[0], 0.02, use_label, fontsize=12, ha='left',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='left', verticalalignment='top',color = pal[get_color(x['index'].iloc[0])] )\n",
    "    else:\n",
    "        ax3.text(x.index[0], -0.02, use_label, fontsize=12, ha='right',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='right', verticalalignment='bottom',color = pal[get_color(x['index'].iloc[0])] )\n",
    "    label_up = not label_up\n",
    "    ticks.append(x.index[0])\n",
    "\n",
    "ax3.set_xticks(ticks,  [])\n",
    "ax3.xaxis.set_tick_params(length=7)\n",
    "\n",
    "\n",
    "ticks = []\n",
    "for i, label in enumerate(people):\n",
    "    print(label)\n",
    "    if i%2 == 0:\n",
    "        x = reordered.loc[reordered['index'].str.lower().str.strip() == label[0]]\n",
    "        ax1.axline((reordered.loc[reordered['index'].str.lower().str.strip() == label[0]].index[0], 0.1), (reordered.loc[reordered['index'].str.lower().str.strip() == label[0]].index[0],0.01) )  \n",
    "        \n",
    "        ax1.text(x.index[0], 0.02, label[1], fontsize=12, ha='left',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='left', verticalalignment='top',color = pal[get_color(x['index'].iloc[0])])\n",
    "    else:\n",
    "        x = reordered.loc[reordered['index'].str.lower().str.strip() == label[0]]\n",
    "        ax1.axline((reordered.loc[reordered['index'].str.lower().str.strip() == label[0]].index[0], -0.1), (reordered.loc[reordered['index'].str.lower().str.strip() == label[0]].index[0], 0.01)) \n",
    "        ax1.text(x.index[0], -0.02, label[1], fontsize=12, ha='right',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='right', verticalalignment='bottom',color = pal[get_color(x['index'].iloc[0])] )\n",
    "    ticks.append(x.index[0])\n",
    "\n",
    "ax1.set_xticks(ticks,  [])\n",
    "ax1.xaxis.set_tick_params(length=7)\n",
    "\n",
    "\n",
    "\n",
    "use_labels = [f\"{translate_mesh[i.lower()]}\" for i in G_labels]\n",
    "reordered = matrix_df.iloc[g.dendrogram_row.reordered_ind].reset_index()['index'].to_frame()\n",
    "ticks = []\n",
    "label_up = True\n",
    "for i, label in enumerate(G_labels):\n",
    "    x = reordered.loc[reordered['index'].str.lower().str.strip() == label.lower()]\n",
    "\n",
    "    if label == \"d040641\":\n",
    "        use_label = \"QTLs\"\n",
    "    elif label == \"d022202\":\n",
    "        use_label = \"BACs\"\n",
    "    elif label == \"d018244\":\n",
    "        use_label = \"YACs\"\n",
    "    else:\n",
    "        use_label = use_labels[i]\n",
    "    if label_up:\n",
    "        ax4.text(x.index[0], 0.02, use_label, fontsize=12, ha='left',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='left', verticalalignment='top', color = pal[get_color(x['index'].iloc[0])]  )\n",
    "    else:\n",
    "        ax4.text(x.index[0], -0.02, use_label, fontsize=12, ha='right',\\\n",
    "                rotation=90, rotation_mode = \"anchor\",horizontalalignment='right', verticalalignment='bottom',color = pal[get_color(x['index'].iloc[0])]  )\n",
    "    label_up = not label_up\n",
    "\n",
    "    ticks.append(x.index[0])\n",
    "\n",
    "ax4.set_xticks(ticks,  [])\n",
    "ax4.xaxis.set_tick_params(length=7)\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "be5e330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms['category'] = terms['term'].map(categories_dict).fillna(\"org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf863c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "67b1d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_export(x):\n",
    "\n",
    "    if x['term'] in [\"eric s lander\", \"francis s collins\", \"james d watson\", 'elke jordan', 'harold e varmus', \n",
    "         'bill clinton', 'j craig venter']:\n",
    "        return x['term']\n",
    "    elif x['category'] == \"people\":\n",
    "        return \"PERSON\"\n",
    "    elif x['category'] == \"phenomena\" or x['category'] == \"technique\":\n",
    "        return x['term'].upper() + \" (\" + x['mesh_term'].strip() + \")\"\n",
    "    else:\n",
    "        return x['term']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9006350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms['term'] = terms.apply(lambda x: prepare_for_export(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "79ccc202",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = terms.loc[terms.term != \"\"]\n",
    "terms = terms.sort_values('cluster').groupby([\"category\", \"cluster\"]).agg({\"term\": list}).reset_index().explode('term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "6ecb8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms[[\"cluster\", \"category\", \"term\"]].to_csv(\"../cache/significant_keywords.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms.loc[terms.category == \"technique\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_essential_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a975ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b65cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms = []\n",
    "\n",
    "for i in [(\"HGP\", \"sequence\"), (\"HGP\", \"HapMap\"), (\"HGP\", \"modENCODE\"), (\"HGP\", \"ENCODE\")]:\n",
    "    temp = contain_df[i]\n",
    "    all_essential_terms.append((temp.loc[(temp.rejected) & (temp.powerful)]['word'].str.lower().to_list(), i[1]))\n",
    "all_essential_terms = pd.DataFrame(all_essential_terms, columns = [\"word\", \"project\"]).explode(\"word\")\n",
    "all_essential_terms['category'] = all_essential_terms['word'].map(categories_dict)\n",
    "all_essential_terms = all_essential_terms[all_essential_terms.category == \"technique\"]\n",
    "all_essential_terms.word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af853c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = nlm.mesh('descriptor')\n",
    "description['UI'] = description['UI'].str.lower()\n",
    "dates_of_mesh = description.loc[(description.UI.isin(all_essential_terms['word'].to_list())) & (description.qualifier == \"DA\")]\n",
    "dates_of_mesh['value'] = dates_of_mesh['value'].apply(lambda x: datetime.strptime(x, \"%Y%m%d\")).dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_essential_terms = all_merged.explode(\"mesh\")\n",
    "documents_with_essential_terms['mesh'] = documents_with_essential_terms['mesh'].str.lower()\n",
    "documents_with_essential_terms = pd.merge(documents_with_essential_terms, all_essential_terms, left_on = 'mesh', right_on = 'word')\n",
    "documents_with_essential_terms[['mesh', 'docID', 'project']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79df729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dates = pd.read_parquet(\"../cache/pdfs_word_excel_powerpoint_031924_with_dates_for_all.parquet\")\n",
    "dates_merged = pd.merge(documents_with_essential_terms[['mesh', 'docID', 'project']].drop_duplicates(), dates, left_on = \"docID\", right_on = \"nodeID\")\n",
    "dates_merged['year'] = dates_merged['date'].dt.year\n",
    "dates_merged.mesh.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "0034f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_to_earliest_doc_year = dates_merged.sort_values(['mesh', 'year']).groupby('mesh').head(1).set_index('mesh')['year'].to_dict()\n",
    "mesh_to_entry_year = dates_of_mesh.set_index(\"UI\")['value'].to_dict()\n",
    "all_essential_terms['doc_year'] = all_essential_terms['word'].map(mesh_to_earliest_doc_year)\n",
    "all_essential_terms['mesh_year'] = all_essential_terms['word'].map(mesh_to_entry_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "f82304f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms['diff'] = all_essential_terms['doc_year'] - all_essential_terms['mesh_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms.loc[all_essential_terms['diff'] >= 0 ].word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481884aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essential_terms.loc[all_essential_terms['diff'] < 0 ].word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "7d561da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_of_each_project = {\n",
    "\"ENCODE\": 2003,\n",
    "\"modENCODE\": 2003,\n",
    "\"sequence\": 2002,\n",
    "\"HapMap\": 2002\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a265ae",
   "metadata": {},
   "source": [
    "`../cache/separate_genomic_techniques_determined.csv` is provided in the repository!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "24151706",
   "metadata": {},
   "outputs": [],
   "source": [
    "determined_genetic_techniques = pd.read_csv(\"../cache/separate_genomic_techniques_determined.csv\")[['name', 'mesh_year', 'doc_year', 'count', 'genomic']]\n",
    "determined_genetic_techniques['diff'] = determined_genetic_techniques['doc_year'] - determined_genetic_techniques['mesh_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "48174a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_techniques = all_essential_terms[\n",
    "    all_essential_terms['category'] == \"technique\"\n",
    "]\n",
    "\n",
    "just_techniques['name'] = just_techniques['word'].apply(lambda x: translate_mesh[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64eeefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "0f03bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_techniques['genomic'] = just_techniques['name'].map(determined_genetic_techniques.set_index(\"name\")['genomic'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272c410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "56be16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_techniques['project_year'] = just_techniques['project'].map(start_of_each_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_techniques.word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "05510bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = just_techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "f8a04563",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['doc_year'] = temp.apply(lambda x: x['doc_year'] if x['word'] != \"d000081246\" else 2009, axis = 1)\n",
    "temp['diff_project'] = temp.apply(lambda x: x['doc_year'] - x['project_year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176452e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp.genomic].word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d49ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[(temp.genomic) & (temp.diff_project < 0)].word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[(temp.genomic) & (temp.diff_project >= 0)].word.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e39f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec7cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c82544a",
   "metadata": {},
   "source": [
    "# Figure 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modENCODE: \"#AD5D95\"\n",
    "# ENCODE: \"#095393\"\n",
    "# LSAC: \"#51AF4D\"\n",
    "# HapMap: \"#E1BE15\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white', rc={\n",
    "    'xtick.bottom': True,\n",
    "    'ytick.left': True,\n",
    "})\n",
    "\n",
    "sns.color_palette(\"Set1\")\n",
    "\n",
    "matplotlib.rc('font', family='Helvetica') \n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "matplotlib.rc('text', usetex='false') \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 2\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.5\n",
    "matplotlib.rcParams['xtick.minor.size'] = 2\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.5\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.5\n",
    "matplotlib.rcParams['ytick.minor.size'] = 2\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.5\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({\"axes.labelsize\": 7,\n",
    "\"xtick.labelsize\": 10,\n",
    "\"ytick.labelsize\": 10,\n",
    "\"legend.fontsize\": 10,\n",
    "\"font.size\":7})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 2), dpi=300)\n",
    "\n",
    "\n",
    "# sns.violinplot(temp.sort_values(by=['project'],\n",
    "#                 key=lambda x: x.map({\"HGP\": 0, \"sequence\": 2, \"HapMap\": 1, \"ENCODE\": 3, \"modENCODE\": 4})), \n",
    "#                x = 'diff_project', y = 'project', hue = 'project', ax = ax, dodge=False, linecolor ='k', linewidth = 0.2, alpha = 0.1,\n",
    "#               palette = [\"#E1BE15\", \"#51AF4D\", \"#095393\", \"#AD5D95\"] )\n",
    "\n",
    "# for violin in ax.collections[::2]:\n",
    "#     violin.set_alpha(0.5)\n",
    "\n",
    "sns.swarmplot(data = temp.loc[(temp.word == \"d055106\") & (temp.genomic)].sort_values(by=['project'],\n",
    "                key=lambda x: x.map({\"HGP\": 0, \"sequence\": 2, \"HapMap\": 1, \"ENCODE\": 3, \"modENCODE\": 4})), x = 'diff_project', y = 'project', ax = ax, palette = [\"red\", \"red\", \"red\", \"red\"], size = 2, )\n",
    "\n",
    "sns.swarmplot(data = temp.loc[(temp.word != \"d055106\") & (temp.genomic)].sort_values(by=['project'],\n",
    "                key=lambda x: x.map({\"HGP\": 0, \"sequence\": 2, \"HapMap\": 1, \"ENCODE\": 3, \"modENCODE\": 4})), x = 'diff_project', y = 'project', ax = ax, palette = ['k', 'k','k','k'], size = 2, )\n",
    "\n",
    "\n",
    "# sns.swarmplot(temp.loc[temp.diff_project <= 0], x = 'diff_project', y = 'project',ax = ax, palette = ['black'], size = 2, )\n",
    "ax.legend().remove()\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.yaxis.label.set_color('black')\n",
    "ax.xaxis.label.set_color('black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "sns.despine()\n",
    "ax.set_ylabel(\"\")\n",
    "ax.axvline(0, 0, 10, c=  'k', linewidth = 0.5, linestyle = '--')\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# ax.set_yticklabels([\"HapMap\", \"LSAC\", \"ENCODE\", \"modENCODE\"])\n",
    "\n",
    "ax.set_xlim(-40, 30)\n",
    "ax.set_xlabel(\"Year of appearance relative to project\", fontsize = 10)\n",
    "ax.set_xticks(np.arange(-40, 30, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig('../cache/genomic_techniques_project_start.pdf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820b9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
